[
  {
    "objectID": "WGAN.html",
    "href": "WGAN.html",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "",
    "text": "The Wasserstein generative adversarial network (WGAN) (Arjovsky, Chintala, and Bottou 2017) is a recently proposed state-of-the-art generative model that leverages ideas from optimal transport to address key limitations of the traditional generative adversarial network (GAN) (Goodfellow et al. 2014).\nTo provide a comprehensive introduction to WGAN, we begin by clarifying the problem of generative modeling. Among various approaches, we focus specifically on GANs, outlining their main ideas and performing a straightforward theoretical analysis. Motivated by the challenges encountered in the training of GANs, WGANs are naturally introduced, highlighting how ideas from optimal transport improve the performance and stability of machine learning models."
  },
  {
    "objectID": "WGAN.html#general-overview",
    "href": "WGAN.html#general-overview",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "General Overview",
    "text": "General Overview\nIn recent years, it has been a hot topic to train machine learning models that exhibit human capabilities, one of which is creativity — the ability to create novel objects within a given category after observing a set of examples. Just like a human child, who can draw numerous distinct cats after seeing just a few real-life cats, one example of generative modeling is to train a machine learning model based on a set of cat images, which can produce an unlimited variety of new cat images that do not resemble those in the training set.\nIn brief, the problem of generative modeling has the following requirements:\n\nThe training set consists of a finite number of samples.\nThe generated outputs belong to the same category as the training samples.\nThe model is capable of producing an infinite variety of outputs.\n\nGenerative models have numerous connections and applications in various fields, e.g., synthetic data generation (Eigenschink et al. 2023), image generation (Oussidi and Elhassouny 2018), game theory (Cao, Guo, and Laurière 2020) and reinforcement learning (Franceschelli and Musolesi 2024), etc. We refer interested readers to (Harshvardhan et al. 2020) for a comprehensive survey on the analysis and applications of generative models."
  },
  {
    "objectID": "WGAN.html#problem-formulation",
    "href": "WGAN.html#problem-formulation",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Problem Formulation",
    "text": "Problem Formulation\nLet \\(\\mathcal{X}\\subset \\mathbb{R}^D\\) denote the sample space where training samples take values. The key assumption of generative modeling is the existence of an unknown probability distribution \\(\\mathbb{P}_r\\) on \\(\\mathcal{X}\\), from which the training samples are drawn. Therefore, generative modeling typically consists of two parts:\n\nApproximation: Approximate the distribution \\(\\mathbb{P}_r\\) with a parameterized model \\(\\mathbb{P}_\\theta\\in\\mathscr{P}(\\mathcal{X})\\), where \\(\\theta\\) represents the parameters, based on a finite number of samples from \\(\\mathbb{P}_r\\).\nSampling: Generate samples from the approximated distribution \\(\\mathbb{P}_\\theta\\)."
  },
  {
    "objectID": "WGAN.html#geometric-structure",
    "href": "WGAN.html#geometric-structure",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Geometric Structure",
    "text": "Geometric Structure\nBefore diving deeper into state-of-the-art methods of generative modeling, it is crucial to understand why classical density estimation methods in Statistics are not performing well. One of the reasons lies in the geometric structure of the problem that the distribution \\(\\mathbb{P}_r\\), in most cases, is supported only on a low-dimensional manifold within \\(\\mathbb{R}^D\\).\nConsider the task of image generation, where each image consists of approximately \\(10^6\\) pixels with three color channels, i.e., \\(D \\approx 3\\times 10^6\\). While each one of the images can be viewed as a single point in \\(\\mathbb{R}^D\\), the true distribution \\(\\mathbb{P}_r\\) is typically concentrated on a manifold with an intrinsic dimension \\(d\\), where \\(d\\leq 50\\) for most image datasets (Pope et al. 2021). This significant gap between the ambient dimension \\(D\\) and the intrinsic dimension \\(d\\) arises from implicit constraints that define the structure of \\(\\mathbb{P}_r\\). For instance, human face images require symmetry and the nose and ears have to adhere to specific shape characteristics.\nUnder this specific structure, perturbation-based sampling methods, which refer to the generation of a randomly perturbed version of one of the training samples, no longer work. As a simple example illustrating such failure, consider \\(\\mathbb{P}_r\\) as a uniform distribution on a unit circle \\(C^1\\subset\\mathbb{R}^2\\). Adding Gaussian noises to a point \\((x,y)\\in C^1\\) almost surely yields a point outside the manifold \\(C^1\\). In other words, general perturbations would destroy the manifold structure, if not carefully designed.\nSimilarly, classical density estimation methods face at least two major challenges:\n\nThe density approximation on the manifold. The density of \\(\\mathbb{P}_r\\), if exists on the manifold, is almost everywhere zero under the Lebesgue measure on \\(\\mathbb{R}^D\\).\nThe density-based sampling scheme. Sampling from a distribution on a high-dimensional space with a known density suffers from the curse of dimensionality.\n\nThose challenges motivate the development of state-of-the-art generative modeling approaches based on different ideas, as introduced in the following section."
  },
  {
    "objectID": "WGAN.html#sota-approaches",
    "href": "WGAN.html#sota-approaches",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "SOTA Approaches",
    "text": "SOTA Approaches\nPopular methods of generative modeling can be roughly categorized as:\n\nVariational Autoencoders (VAEs): VAEs are likelihood-based generative models that train an encoder to map samples to a latent space and a decoder to reconstruct samples from latent vectors.\nGenerative Adversarial Networks (GANs): GANs involve a generator and a discriminator network trained in an adversarial way, where the discriminator learns to distinguish between real and generated data, while the generator aims to create samples indistinguishable from real data.\nDiffusion models: Diffusion models progressively corrupt samples by adding random noise and reverses this process, reconstructing new samples from pure random noise.\n\nWhile all approaches share the same objective, their methodologies differ significantly. In the following discussion, we briefly introduce GANs to prepare the readers for the discussion on WGANs."
  },
  {
    "objectID": "WGAN.html#main-idea",
    "href": "WGAN.html#main-idea",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Main Idea",
    "text": "Main Idea\nAs mentioned in the previous context, a generative model shall have the “power of infinity”, i.e., the ability to produce an infinite variety of outputs. However, when it comes to numerical implementations of most algorithms, we are always trying to discretize continuous objects. For example, when numerical integration is performed, the integration domain is discretized into a large (but finite) number of small areas. Attaining infinity seems impossible at the first glance, but modern computers do attain infinity in one specific task — generating random samples from a given distribution. We draw distinct samples from a multivariate standard Gaussian \\(N(0,I)\\) each time the random number generator is called, which is actually the source of the “power of infinity”. Inspired by such observations, generative models shall learn how to map random samples from a known distribution, e.g., \\(N(0,I)\\), to outputs that follow \\(\\mathbb{P}_r\\), i.e., the models should be delivering randomness, rather than creating new randomness.\nMathematically speaking, we call \\(\\mathcal{Z}\\subset \\mathbb{R}^l\\) the latent space, where the latent random variable \\(Z\\) takes values. The random variable \\(Z\\) follows a given probability distribution \\(\\mathbb{P}_Z\\), which is often taken as a multivariate Gaussian. A GAN aims to learn a function parameterized by \\(\\theta\\): \\[\nG_\\theta:\\mathcal{Z}\\to\\mathcal{X},\n\\] such that the law of \\(G_\\theta(Z)\\) is approximately \\(\\mathbb{P}_r\\). In practice, \\(G_\\theta\\) is typically taken as a neural network, while \\(\\theta\\) denotes the collection of all the network paramaters. For the purpose of notation, we denote \\(\\mathbb{P}_\\theta\\) as the law of \\(G_\\theta(Z)\\) so that the problem turns into: \\[\n\\inf_\\theta d(\\mathbb{P}_\\theta,\\mathbb{P}_r),\n\\] where \\(d\\) is some information divergence that measures the difference between two probability distributions. Such a formulation naturally poses two questions:\n\nHow to supervise the model with the finite number of training samples from \\(\\mathbb{P}_r\\)?\nHow to select the information divergence \\(d\\)?\n\nThe wisdom of GANs lies in using another neural network to approximate \\(d\\) without specifying it explicitly. The network that approximates \\(d\\) is parameterized by \\(w\\): \\[\nD_w:\\mathcal{X}\\to[0,1].\n\\]\nThe fundamental framework of GANs consists of a generator \\(G_\\theta\\) and a discriminator \\(D_w\\). The generator receives inputs as samples \\(z\\) from the known distribution \\(\\mathbb{P}_Z\\) and outputs the generated samples \\(G_\\theta(z)\\in\\mathcal{X}\\). The discriminator receives inputs as elements in \\(\\mathcal{X}\\) (could be samples in the training set or outputs of the generator \\(G_\\theta(z)\\)), and assigns a score as the probability that the input received is from the true distribution \\(\\mathbb{P}_r\\). In principle, the discriminator hopes to distinguish the training samples from \\(\\mathbb{P}_r\\) and the fake samples from \\(\\mathbb{P}_\\theta\\) that are produced by the generator, while the generator hopes to fool the discriminator on the top of that. Since the GAN architecture creates a competition between the generator and the discriminator, ideally, both networks perform well enough in fulfilling their respective tasks after training. The trained generator is exactly the generative model we have been referring to, while the auxiliary discriminator is simply disregarded.\nWith the GAN architecture in mind, it suffices to propose loss functions for both networks, as the only missing components. The constructions of loss functions are rather intuitive: if the input of the discriminator comes from \\(\\mathbb{P}_r\\), its output shall be close to one; if the input of the discriminator comes from \\(\\mathbb{P}_\\theta\\), its output shall be close to zero. With the logarithms introduced, the discriminator maximizes \\(\\mathbb{E}_{X\\sim\\mathbb{P}_r}\\log D_\\omega(X) + \\mathbb{E}_{Z\\sim\\mathbb{P}_Z}\\log (1-D_\\omega(G_\\theta(Z)))\\) w.r.t. \\(w\\). When the discriminator reaches its optimum, the generator aims to minimize the same loss in order to fool the discriminator, resulting in the optimization problem: \\[\n\\inf_\\theta\\sup_\\omega \\mathbb{E}_{X\\sim\\mathbb{P}_r}\\log D_\\omega(X) + \\mathbb{E}_{Z\\sim\\mathbb{P}_Z}\\log (1-D_\\omega(G_\\theta(Z))).\n\\]\nWe leave the following remarks on the training of GANs:\n\nThe generator and the discriminator share exactly the same loss function, but are optimizing it in opposite directions. In this sense, GAN has essential difference from the well-known actor-critic algorithm in reinforcement learning, where two networks have their respective loss functions to optimize.\nThe expectations in the loss functions are approximated by Monte Carlo. For numerical implementations, one only needs to compute the loss function, call backpropagation for gradient computations (w.r.t. \\(w\\) and \\(\\theta\\)), and conduct gradient descent/ascent for the parameters of the generator/discriminator network.\nDue to the inf-sup structure of the problem, one typically trains the discriminator for several epochs before training the generator for a single epoch. GANs work in the way that the training samples supervise the discriminator, and the discriminator supervises the generator."
  },
  {
    "objectID": "WGAN.html#theoretical-analysis",
    "href": "WGAN.html#theoretical-analysis",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Theoretical Analysis",
    "text": "Theoretical Analysis\nWe demonstrate a simple theoretical analysis for the GAN optimization problem (Goodfellow et al. 2014). Due to technical difficulties modeling the parameter optimization procedure within neural networks, we omit the dependence on the parameters \\(\\theta\\) and \\(w\\), and rewrite the optimization problem in terms of the generator \\(G\\) and the discriminator \\(D\\). For simplicity, we adopt a change of variable \\(Y := G(Z)\\) to absorb all dependencies on the generator \\(G\\). Since \\(Y\\) is the output of the generator, it follows the distribution \\(\\mathbb{P}_\\theta\\). The inner layer optimization of the inf-sup problem turns out to be explicitly solvable: \\[\n\\sup_D f(D):=\\mathbb{E}_{X\\sim\\mathbb{P}_r}\\log D(X) + \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}\\log (1-D(Y)).\n\\] Compute the first variation of \\(f\\) in \\(D\\) w.r.t. the perturbation \\(\\psi\\): \\[\n\\delta f(D)(\\psi) := \\lim_{\\varepsilon\\to 0}\\frac{f(D + \\varepsilon \\psi) - f(D)}{\\varepsilon}\n= \\mathbb{E}_{X\\sim\\mathbb{P}_r}\\frac{\\psi(X)}{D(X)} - \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}\\frac{\\psi(Y)}{1-D(Y)}.\n\\] Assume \\(\\mathbb{P}_r\\) and \\(\\mathbb{P}_\\theta\\) have respective densities \\(p_r\\) and \\(p_\\theta\\) w.r.t. the Lebesgue measure. The optimality criterion \\(\\delta f(D^*)(\\psi) = 0\\), \\(\\forall \\psi\\) implies that \\[\nD^* = \\frac{p_r}{p_r + p_\\theta}.\n\\] Plugging back into the objective function yields \\[\nf(D^*) = \\int \\left[p_r(x)\\log\\frac{p_r(x)}{\\frac{p_r(x) + p_\\theta(x)}{2}} + p_\\theta(x)\\log\\frac{p_\\theta(x)}{\\frac{p_r(x) + p_\\theta(x)}{2}}\\right]\\,dx - 2\\log 2.\n\\] This quantity can be identified with the Jensen-Shannon divergence: \\[\nf(D^*) = D_{\\text{KL}} \\left(\\mathbb{P}_r||\\frac{\\mathbb{P}_r + \\mathbb{P}_\\theta}{2}\\right) + D_{\\text{KL}} \\left(\\mathbb{P}_\\theta||\\frac{\\mathbb{P}_r + \\mathbb{P}_\\theta}{2}\\right) - 2\\log 2=: 2\\text{JS}(\\mathbb{P}_r,\\mathbb{P}_\\theta) - 2\\log 2.\n\\] Consequently, if the discriminator has reached its optimum \\(D^*\\), training the generator is equivalent to minimizing the Jensen-Shannon divergence between the real and the approximated distributions."
  },
  {
    "objectID": "WGAN.html#motivation",
    "href": "WGAN.html#motivation",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Motivation",
    "text": "Motivation\nDespite great success in practical applications, the training of GANs is notoriously unstable. One of the main issues is called mode collapse, the phenomenon that a trained GAN generates samples of one single variety while ignoring other possible modes. It is widely believed that the main cause of mode collapse is the lack of control on the discriminator (Kushwaha, Nandi, et al. 2020). Philosophically, if the discriminator learns much faster than the generator, it criticizes all the samples produced by the generator, so that the generator does not know how to proceed. Conversely, if the discriminator learns much slower than the generator, it accepts all the results generated by the generator, which once again causes the similar issue. Hence, normal GAN training requires a careful adjustment in the relative learning speed of the generator and the discriminator. Numerically, we shall not train the discriminator till optimality before training the generator, despite the actual inf-sup structure of the optimization problem.\nThere are two main streams of ideas in the literature to augment the stability of GANs:\n\nMinimize an information divergence other than the Jensen-Shannon divergence.\nUse multiple generators to explicitly enforce GANs to capture diverse modes.\n\nWasserstein GANs take the first approach and use the Wasserstein distance (with \\(p=1\\)) instead of the Jensen-Shannon divergence. To understand what motivates WGANs, there are two main questions to answer:\n\nWhat is the advantage of the Wasserstein distance over the Jensen-Shannon divergence?\nHow to encode the minimization of the Wasserstein distance in the loss functions?"
  },
  {
    "objectID": "WGAN.html#advantage-of-wasserstein-distance",
    "href": "WGAN.html#advantage-of-wasserstein-distance",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Advantage of Wasserstein Distance",
    "text": "Advantage of Wasserstein Distance\nWe refer to a simple working example from (Arjovsky, Chintala, and Bottou 2017) that illustrates the advantage of using the Wasserstein distance \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\).\nConsider the setting where \\(\\mathcal{X}= \\mathbb{R}^2\\). Let \\(S_\\theta := \\{\\theta\\}\\times [0,1]\\) denote a family of unit line segments parameterized by \\(\\theta\\). Clearly, each \\(S_\\theta\\) is a one-dimensional manifold within \\(\\mathcal{X}\\). Let \\(\\mathbb{P}_\\theta\\) be the uniform distribution on \\(S_\\theta\\) and \\(\\mathbb{P}_r = \\mathbb{P}_{\\theta = 0}\\). The optimal value of \\(\\theta\\) that guarantees \\(\\mathbb{P}_r = \\mathbb{P}_\\theta\\) is thus \\(0\\). We compare four differently defined information divergences between \\(\\mathbb{P}_r\\) and \\(\\mathbb{P}_\\theta\\):\n\nTotal variation: \\[\n\\text{TV}(\\mathbb{P}_0,\\mathbb{P}_\\theta) := \\sup_{A\\in\\mathscr{B}_{\\mathcal{X}}}|\\mathbb{P}_0(A) - \\mathbb{P}_\\theta(A)|,\n\\] where \\(\\mathscr{B}_{\\mathcal{X}}\\) denotes the Borel sigma field on \\(\\mathcal{X}\\). Since the supports \\(S_0\\) and \\(S_\\theta\\) are disjoint when \\(\\theta \\neq 0\\), the total variation attains its maximum value \\(1\\). Therefore, \\[\n\\text{TV}(\\mathbb{P}_0,\\mathbb{P}_\\theta) = \\begin{cases} 0 &\\text{if}\\ \\theta = 0\\\\ 1 & \\text{else}\\end{cases}.\n\\]\nKullback-Leibler divergence: \\[\n\\text{KL}(\\mathbb{P}_0||\\mathbb{P}_\\theta) := \\begin{cases} \\mathbb{E}_{\\mathbb{P}_\\theta} \\left(\\frac{\\text{d}\\mathbb{P}_0}{\\text{d}\\mathbb{P}_\\theta}\\log \\frac{\\text{d}\\mathbb{P}_0}{\\text{d}\\mathbb{P}_\\theta}\\right)& \\text{if}\\ \\mathbb{P}_0 &lt;&lt;\\mathbb{P}_\\theta\\\\ \\infty & \\text{else}\\end{cases},\n\\] where \\(&lt;&lt;\\) denotes the absolute continuity between measures. Since the supports \\(S_0\\) and \\(S_\\theta\\) are disjoint when \\(\\theta \\neq 0\\), absolute continuity fails and the KL divergence is infinite. Therefore, \\[\n\\text{KL}(\\mathbb{P}_0||\\mathbb{P}_\\theta) = \\begin{cases} 0 &\\text{if}\\ \\theta = 0\\\\ \\infty & \\text{else}\\end{cases}.\n\\]\nJensen-Shannon divergence: \\[\n\\text{JS}(\\mathbb{P}_0,\\mathbb{P}_\\theta) := \\frac{1}{2}\\text{KL}(\\mathbb{P}_0||\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}) + \\frac{1}{2}\\text{KL}(\\mathbb{P}_\\theta||\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}).\n\\] Note that \\(\\mathbb{P}_0&lt;&lt;\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}\\), hence the Jensen-Shannon divergence is always finite. When \\(\\theta\\neq 0\\), we calculate the two Radon-Nikodym derivatives \\[\n\\frac{\\text{d}\\mathbb{P}_0}{\\text{d}\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}} = 2\\mathbb{I}_{S_0},\\quad\n\\frac{\\text{d}\\mathbb{P}_\\theta}{\\text{d}\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}} = 2\\mathbb{I}_{S_\\theta},\n\\] where \\(\\mathbb{I}_A\\) denotes the indicator of a set \\(A\\). Those conclusions could be verified from definitions \\[\n\\int_A 2\\mathbb{I}_{S_0}(x)\\,\\text{d}\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}(x) = \\mathbb{P}_0(A\\cap S_0) + \\mathbb{P}_\\theta(A\\cap S_0) = \\mathbb{P}_0(A),\\ \\forall A\\in\\mathscr{B}_{\\mathcal{X}}.\n\\] Therefore, \\[\n\\text{JS}(\\mathbb{P}_0,\\mathbb{P}_\\theta) = \\begin{cases} 0 &\\text{if}\\ \\theta = 0\\\\ \\log 2 & \\text{else}\\end{cases}.\n\\]\nWasserstein distance with \\(p=1\\): \\[\nW(\\mathbb{P}_0,\\mathbb{P}_\\theta) := \\inf_{X\\sim \\mathbb{P}_0,Y\\sim\\mathbb{P}_\\theta} \\mathbb{E}\\|X - Y\\|_2.\n\\] From geometric intuition, \\(T(x) = x+\\theta\\) is the optimal transport map under the convex cost \\(c(x,y) = \\|x-y\\|_2\\). Therefore, \\[\nW(\\mathbb{P}_0,\\mathbb{P}_\\theta) = |\\theta|.\n\\]\n\nObviously, only the Wasserstein distance exhibits the continuity in parameter \\(\\theta\\), while other divergences have discontinuities at \\(0\\). This observation has been made rigorous in general cases (Arjovsky, Chintala, and Bottou 2017). The authors prove that if the generator \\(G_\\theta\\) is continuous in \\(\\theta\\), then so is \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\). If the generator \\(G_\\theta\\) is locally Lipschitz in \\(\\theta\\), then so is \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\) under mild regularity assumptions, which, by Rademacher’s theorem, implies that \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\) is almost everywhere differentiable in \\(\\theta\\).\nThe continuity and differentiability w.r.t. the parameter is crucial in the training of GANs and is closely related to the issue of vanishing gradients. When the discriminator has reached its optimum and one gradient step of the generator is performed, the gradient actually refers to \\(\\nabla_{\\theta}[d(\\mathbb{P}_r,\\mathbb{P}_\\theta)]\\). In the example above, all divergences except the Wasserstein distance provide trivial (vanishing) gradients for the generator. By contrast, the Wasserstein distance provides a gradient that is \\(1\\) for positive \\(\\theta\\) and \\(-1\\) for negative \\(\\theta\\), which is always effective for the generator. Thanks to the physical interpretation of optimal transport, the Wasserstein distance, as a specific optimal-transport-based information divergence, greatly mitigates the issue of vanishing gradients for the generator."
  },
  {
    "objectID": "WGAN.html#construction-of-wgan",
    "href": "WGAN.html#construction-of-wgan",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Construction of WGAN",
    "text": "Construction of WGAN\nIn terms of numerical implementation, an inf-sup formulation of the optimization problem is required, as it clearly specifies the loss functions. Luckily, the Kantorovich-Rubinstein duality provides the representation: \\[\nW(\\mathbb{P}_r,\\mathbb{P}_\\theta) = \\sup_{\\|f\\|_L\\leq 1} \\mathbb{E}_{X\\sim \\mathbb{P}_r} f(X) - \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}  f(Y),\n\\] where the supremum is taken over all Lipschitz functions \\(f\\) with Lipschitz constants no larger than \\(1\\). Substituting \\(f = \\frac{g}{K}\\) allows the relaxation in the Lipschitz constant: \\[\nK\\cdot W(\\mathbb{P}_r,\\mathbb{P}_\\theta) = \\sup_{\\|g\\|_L\\leq K} \\mathbb{E}_{X\\sim \\mathbb{P}_r} g(X) - \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}  g(Y).\n\\] Therefore, the complete optimization problem of WGANs w.r.t. the parameterized generator \\(G_\\theta\\) and discriminator \\(D_w\\) is given by \\[\n\\inf_\\theta\\sup_{w:\\|D_w\\|_L\\leq K} \\mathbb{E}_{X\\sim \\mathbb{P}_r} D_w(X) - \\mathbb{E}_{Z\\sim\\mathbb{P}_Z}  D_w(G_\\theta(Z)).\n\\] The objective of this optimization problem serves as the loss functions in WGANs, shared by both the generator and the discriminator. One last piece of detail for the implementation of WGANs lies in the way to numerically impose the constraint \\(\\|D_w\\|_L\\leq K\\) for parameters \\(w\\). If \\(D_w\\) is parameterized by a feedforward neural network, each layer of the network consists of an affine mapping \\(x\\mapsto Wx + b\\) with weight \\(W\\), bias \\(b\\), and a mapping of the nonlinear activation function \\(\\sigma\\). Clearly, the affine mapping has Lipschitz constant \\(\\|W\\|\\), while the common activation functions, e.g., sigmoid, hyperbolic tangent, ReLU, etc, have Lipschitz constants \\(L_\\sigma&lt;\\infty\\). Therefore, the composition \\(x\\mapsto \\sigma(Wx+b)\\) has a Lipschitz constant \\(L_\\sigma\\|W\\|\\), which is uniform w.r.t. the trainable network parameters if \\(\\|W\\|\\) has a uniform upper bound, i.e., when \\(w\\) is restricted to a compact domain. As a result, WGANs impose the Lipschitz constraint by parameter clipping, e.g., restricting each component of \\(w\\) to take values in \\([-0.01,0.01]\\).\nWe remark that, by imposing the Lipschitz constraint, the discriminator is not allowed to saturate, thus providing effective gradients everywhere. As shown in the plot below, the GAN discriminator is too good at distinguishing two Gaussian distributions to provide effective gradients. In contrary, the WGAN discriminator (critic) with parameter clipping does not perform that well in distinguishing, but does provide effective gradients everywhere. Philosophically, the GAN discriminator is too smart that it demotivates the learning of the generator!\n\n\n\nThe comparisons of the behavior of optimal GAN discriminator and WGAN discriminator (critic) in dintinguishing two Gaussian distributions. The GAN discriminator saturates and provide trivial gradients within the main support of two Gaussian distributions. However, the WGAN discrinimator (critic) has a linear growth and provides effective gradients on the whole space. (Arjovsky, Chintala, and Bottou 2017)"
  },
  {
    "objectID": "WGAN.html#conclusions-and-future-studies",
    "href": "WGAN.html#conclusions-and-future-studies",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Conclusions and Future Studies",
    "text": "Conclusions and Future Studies\nTo conclude, WGANs improve the training stability of GANs by switching the minimization of the Jensen-Shannon divergence to that of the Wasserstein distance, which is continuous in terms of the parameter of the generator. Numerically, we make use of the Kantorovich-Rubinstein duality to keep the inf-sup structure of the optimization problem, and adopt parameter clipping to impose the Lipschitz continuity.\nAs pointed out in (Arjovsky, Chintala, and Bottou 2017), future studies can be conducted in the following directions:\n\nImpose the Lipschitz continuity of the neural network with a different technique, due to the subtleties in choosing the clipping hyperparameter.\nExplain why WGAN training is unstable when momentum based optimizers (like Adam) or high learning rates are used."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Contributors wanted!\nDo you want to help others learn about optimal transport? Do you need a concrete goal to motivate yourself to learn about a new topic? Please consider becoming a contributor to our wiki!\nYour contributions will be gratefully recognized in footers of each page. Please contact Katy Craig to receive an account.\nThis wiki is maintained by Katy Craig with help from the awesome students from Math 260L in Spring 202, Math 201A in Fall 2020, and Math 260J in Winter 2022."
  },
  {
    "objectID": "Kantorovich_Problem.html",
    "href": "Kantorovich_Problem.html",
    "title": "Kantorovich Problem",
    "section": "",
    "text": "The Kantorovich problem (Villani 2003) is one of the two essential minimization problems in optimal transport (the other being the Monge problem). It is named after Russian mathematician and Nobel Laureate Leonid Kantorovich."
  },
  {
    "objectID": "Kantorovich_Problem.html#shipping-problem",
    "href": "Kantorovich_Problem.html#shipping-problem",
    "title": "Kantorovich Problem",
    "section": "Shipping problem",
    "text": "Shipping problem\nThe intuition behind the Kantorovich problem can be given by an explanation of optimizing shipments. Suppose there is a merchant who is attempting to ship items from one place to another. The merchant can hire trucks at some cost \\(c(x, y)\\) for each unit of merchandise which is shipped from point \\(x\\) to point \\(y\\). Now the shipper is approached by a mathematician, who claims that prices can be set such that they align with the shipper’s financial interests (Carlier 2010). This would be achieved by setting the price \\(\\phi(x)\\) and \\(\\phi(y)\\) such that the sum of \\(\\phi(x)\\) and \\(\\phi(y)\\) is always less than the cost \\(c(x, y)\\). This may even involve setting negative prices in certain cases. However, it can be shown that the shipper will spend almost as much as they would have if instead they opted for the original pricing method (Paris 2016)."
  },
  {
    "objectID": "Kantorovich_Problem.html#transport-plans",
    "href": "Kantorovich_Problem.html#transport-plans",
    "title": "Kantorovich Problem",
    "section": "Transport Plans",
    "text": "Transport Plans\nThe Monge problem was about the optimal way to rearrange mass (Craig 2020). Note that in the Monge formulation of the optimal transport problem, the mass cannot be split and thus it is mapped \\(x \\mapsto T(x)\\). When considering discrete cases, this results in problems when trying to establish maps T such that \\(T_{\\#} \\mu=\\nu\\). Kantorovich made the observation that the mass in question could be split, which makes the problem much easier to model  Craig, Katy. The Kantorovich Problem. Math 260L. Univ. of Ca. at Santa Barbara. Spring 2020 . Allowing the mass to be split results in a relaxation of the problem (e.g. half of the mass from \\(x_1\\) can go to \\(y_1\\) and half can go to \\(y_2\\), and so on). To model this consider \\(d \\pi(x, y)\\), which denotes the mass transported from x to y. This allows the mass to be moved to multiple places. Also consider \\(\\mu(A)\\) and \\(\\nu(B)\\): where the total mass taken from measurable set \\(A \\in X\\) must be equal to \\(\\mu(A)\\) and the total mass taken from measurable set \\(B \\in Y\\) must equal \\(\\nu(B)\\).\nThe constraints of the problem can be written in the following manner:\n\\[\n\\pi(A \\times Y)=\\mu(A)\n\\]\n\\[\n\\pi(X \\times B)=\\nu(B)\n\\]\nfor all measurable sets \\(A \\subseteq X, B \\subseteq Y\\). As such, we can interpret \\(\\pi( A\\times B)\\) as representing the amount of mass from \\(\\mu (A)\\) that is directed to \\(\\nu (B)\\)\nIf we have a measure \\(\\pi\\) that satisfies these constraints, then the set of such \\(\\pi\\) is referred to as \\(\\Pi(\\mu, \\nu)\\) – the set of transport plans between \\(\\mu\\) and \\(\\nu\\). Notice again that now we are dealing with transport plans instead of the transport maps that are used in the Monge formulation of the problem  Craig, Katy. The Kantorovich Problem. Math 260L. Univ. of Ca. at Santa Barbara. Spring 2020 ."
  },
  {
    "objectID": "Kantorovich_Problem.html#problem-statement",
    "href": "Kantorovich_Problem.html#problem-statement",
    "title": "Kantorovich Problem",
    "section": "Problem Statement",
    "text": "Problem Statement\nGiven \\(\\mu \\in \\mathcal{P}(X)\\) and \\(\\nu \\in \\mathcal{P}(Y)\\), solve\n\n\\(\\operatorname{min} \\mathbb{K}(\\pi):= \\operatorname{min} \\int_{X \\times Y} c(x, y) \\mathrm{d} \\pi(x, y)\\)\n\nover all such \\(\\pi \\in \\Pi(\\mu, \\nu)\\)\nAssuming there is a transport map \\(T^{\\dagger}: X \\rightarrow Y\\) for the Monge problem, we define \\(\\mathrm{d} \\pi(x, y)=\\mathrm{d} \\mu(x) \\delta_{y=T^{\\dagger}(x)}\\). Using this we can see that:\n\n\\(\\begin{aligned} \\pi(A \\times Y) &=\\int_{A} \\delta_{T^{\\dagger}(x) \\in Y} \\mathrm{d} \\mu(x)=\\mu(A) \\\\ \\pi(X \\times B) &=\\int_{X} \\delta_{T^{\\dagger}(x) \\in B} \\mathrm{d} \\mu(x)=T_{\\#}^{\\dagger} \\mu(B)=\\nu(B) \\end{aligned}\\)\n\nWe can see that \\(\\int_{X \\times Y} c(x, y) \\mathrm{d} \\pi(x, y)=\\int_{X} c\\left(x, T^{\\dagger}(x)\\right) \\mathrm{d} \\mu(x)\\)\nthus \\(\\inf \\mathbb{K}(\\pi) \\leq \\inf \\mathbb{M}(T)\\)."
  },
  {
    "objectID": "Kantorovich_Problem.html#kantorovich-duality",
    "href": "Kantorovich_Problem.html#kantorovich-duality",
    "title": "Kantorovich Problem",
    "section": "Kantorovich Duality",
    "text": "Kantorovich Duality\nSince the Kantorovich problem is a linear minimization problem with convex constraints it admits a dual problem. The astute reader may notice that this is a linear programming problem – Kantorovich is also considered to be the founder of linear programming."
  },
  {
    "objectID": "Kantorovich_Problem.html#calculus-of-variations-approach",
    "href": "Kantorovich_Problem.html#calculus-of-variations-approach",
    "title": "Kantorovich Problem",
    "section": "Calculus of Variations Approach",
    "text": "Calculus of Variations Approach\nUnder the right setting, one can show the Kantovorich problem indeed has a minimizer using the direct method of the calculus of variations. More specifically, if one turns to the narrow topology, then it turns out that we get compactness of the constraint set. Moreover, such a topology ensures us that our objective function is lower semi-continuous."
  },
  {
    "objectID": "Kantorovich_Problem.html#knott-smith-optimality-criterion",
    "href": "Kantorovich_Problem.html#knott-smith-optimality-criterion",
    "title": "Kantorovich Problem",
    "section": "Knott-Smith Optimality Criterion",
    "text": "Knott-Smith Optimality Criterion\nOne useful result we have that allows us to connect both the Monge and Kantorovich problems is the so-called the Knott-Smith Optimality Criterion (see below)."
  },
  {
    "objectID": "ArticlesToRevise.html",
    "href": "ArticlesToRevise.html",
    "title": "Suggestions of Articles to Revise",
    "section": "",
    "text": "Below, you can find a list of suggestions for articles to revise.\nIf you choose to revise one of these articles, let me know, and I will remove it from the list below.\n\nOptimal Transport in One Dimension - This article could benefit from a major reorganization, adding/removing some sections (such as the linear cost example), and stating the main theorem in a higher level of generality, as in Santambrogio Theorem 2.9 p63 of Optimal Transport for Applied Mathematicians\nKantorovich Dual Problem for General Costs - This article could benefit from editing throughout. It would be good to expand the final section on c-convcave functions, stating and explaining Santambrogio Proposition 1.11 p12 of Optimal Transport for Applied Mathematicians\nKantorovich Dual Problem (for c(x,y) = d(x,y)^2 where d is a metric) - This article could benefit from editing throughout, especially regarding the latex formatting. The condition that mu does not give mass to small sets needs to be corrected. This article should more thoroughly reference the other article on the Kantorovich dual probelm. It should prove the relationship between c-concave functions and convex conjugates, as in Santambrogio’s Proposition 1.21, p16 of Optimal Transport for Applied Mathematicians. The relationship between (DP) and (DP-var), as Santambrogio refers to them, needs to be explained. (You do not need to use the notation DP and DP-var.)\nWasserstein barycenters and applications in image processing - This article could benefit from editing throughout, with more precise statements of the main results. It should also be revised to mention recent work on barycenters with positive and negative weights.\nDiscrete Optimal Transport - This article could benefit from editing throughout. Several sections just have a few words as a placeholder and could be expanded. (The dual problem should be stated explicitly and a reference should be added to the wiki article on the dual problem.) The latex formatting could be simplified and made easier to read. Everything could be explained for general costs, rather than quadratic costs. The results of Exercise 25 could be included.\nSinkhorn’s Algorithm - This article could benefit from editing throughout. The wiki article on discrete optimal transport should be referenced. The interpretation of the Sinkhorn algorithm in terms of the corresponding dual probalm and in terms of convex projections should be explained. (See Peyre and Cuturi)\nSliced Wasserstein Distance - This article could benefit from editing throughout. Recent contributions by Kitagawa and Takatsu and Park and Slepcev should be summarized.\nGradient flows on Hilbert Spaces - The organization of this article could be greatly improved. Currently, there is an over emphacsis on the Moreau Yosida regularization. Also, the other wiki article on the MY regularization should be cited. The latex formatting could be improved throughout. The statements in the Examples and Applications section could be made more rigorous and more organized."
  },
  {
    "objectID": "NewArticleIdeas.html",
    "href": "NewArticleIdeas.html",
    "title": "New Article Ideas",
    "section": "",
    "text": "Below, you can find a list of new article ideas and suggested references.\nFeel free to incorporate additional references! Please list all references you use at the bottom of your article.\nIf you choose to write about one of these ideas, let me know, and I will remove it from the list below. Want to write about something that’s not listed here? Great! Let me know, and I will suggest some references.\n\nVariants of Optimal Transport Problems\n\nThe \\(s\\)-Wasserstein metric for \\(s&lt;1\\) Craig and Yu, 2024, Santambrogio 3.3.2\nEntropic optimal transport, Chewi, Niles-Weed, Rigollet Ch.4 and 2\nConnections between entropic optimal transport and the Schrödinger bridge problem 1\nBrenier maps 3\nKnothe maps, Figalli-Glaudo (9-14), Carlier, et. al. ’08\n\n\n\nThe 2-Wasserstein Metric\n\nMulti-marginal optimal transport and density functional theory\nDisplacement convexity; Santambrogio (249-251,271-276); Villani (150-154) (make sure to cite existing wiki article on Geodesics and generalized geodesics)\n\n\n\nNumerical Methods for Optimal Transport\n\nComputing OT via Benamou-Brenier; Santambrogio (220-225); Peyré, Cuturi (102-108)\nWasserstein Barycenters; Santambrogio (215-218); Peyré, Cuturi (138-144)\n\n\n\nWasserstein Gradient Flows\n\nFundamentals of Wasserstein Gradient flows, Chewi, Niles-Weed, Rigollet (135-148) and Santambrogio, ‘Euclidean, Metric, and Wasserstein GFs’\n\n\n\nMathematical Foundations\n\n\nStatistical Foundations\n\nEstimation of Wasserstein distances, Chewi, Niles-Weed, Rigollet Ch.2\n\n\n\nApplications:\n\nOptimal transport methods in economics; see introduction of book by Galichon (I have a copy you can borrow) and 6\nQuantization and Lloyd’s algorithm 7, 8, 9\nTransformers as Wasserstein Gradient Flows; Chewi, Niles-Weed, Rigollet (195-200)\nInferring developmental trajectories of biological cells via optimal transport Waddington-OT, Schiebinger, et. al. 2019"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimal Transport Wiki",
    "section": "",
    "text": "Welcome to the Optimal Transport Wiki!\nHere is a list of new article ideas. Here is a list of suggestions of articles to revise\nContact Katy Craig if you would like to contribute to this wiki.\n\nThe optimal transportation problem\n\nMonge Problem\nKantorovich Problem (has already been updated to quarto)\nOptimal Transport in One Dimension\nKantorovich Dual Problem (for general costs)\nKantorovich Dual Problem (for \\(c(x,y) = d(x,y)^2\\) where \\(d\\) is a metric)\nDynamic formulation of \\(W_p\\) metric - Max Emerick\nRegularity of Optimal Transport Maps and the Monge-Ampére Equation on Riemannian Manifolds\n1-Wasserstein metric and generalizations\nOptimal Transport and Ricci curvature\n\n\n\nThe 2-Wasserstein Metric\n\nGeodesics and generalized geodesics\nFormal Riemannian Structure of the Wasserstein metric\nAsymptotic equivalence of W_2 and H^-1\n\n\n\nVariants of the optimal transport problem\n\nMartingale optimal transport and mathematical finance\n[Martingale optimal transport: dynamic formulation] - Ka Lok Lam\nWasserstein barycenters and applications in image processing\nWasserstein metrics on graphs - Claire\nContinuous time martingale OT and Skorohod Embedding - Qijin Shi\n\n\n\nNumerical methods for optimal transport\n\nDiscrete Optimal Transport\nAuction Algorithm\nSemidiscrete Optimal Transport\nSinkhorn’s Algorithm\nSliced Wasserstein Distance\nBack and Forth Method - Jack Pfaffinger\n\n\n\nOptimal Transport and Statistics\n\nEstimation of transport maps - Evan Tufte, Chewi, Niles-Weed, Rigollet Ch.3\n\n\n\nWasserstein Gradient Flows\n\nProbability flow solution of the Fokker–Planck equation - Simone Betteti - Boffi and Vanden-Eijnden 2023\nBures-Wasserstein Gradient flows - Arie Ogranovich - Chewi, Niles-Weed, Rigollet (149-153)\nHellinger-Kantorvich Gradient flows - Djordje Nikolic\n\n\n\nMathematical foundations\n\nDual space of C_0(x) vs C_b(x)\nConvergence of Measures and Metrizability\nFenchel-Moreau and Primal/Dual Optimization Problems\nFenchel-Rockafellar and Linear Programming\nThe Moreau-Yosida Regularization\nGradient flows in Hilbert spaces\nThe continuity equation and Benamour Brenier formula\nIsoperimetric inequality and OMT\nGaussian Measures - Connor Marrs\nAnalysis on metric spaces - Max Emerick\nGradient flows in metric spaces\n\n\n\nApplications of Optimal Transport\n\nMachine Learning\nShallow neural networks as Wasserstein gradient flows\nWasserstein Generative Adversarial Networks\nKnothe maps and conditional sampling - Charles Kulick - Baptista, Marzouk, Zahm 2023\n\n\n\nOther\n\nDistraction Recommendations\nNonlocal Equations Wiki\nProteopedia\nQuantum Information Wiki"
  },
  {
    "objectID": "GFMetricSpace.html",
    "href": "GFMetricSpace.html",
    "title": "Gradient Flows in Metric Space",
    "section": "",
    "text": "We first briefly reivew gradient flows in \\(\\mathbb{R}^n\\). The motivation comes from the so-called “gradient descent method”: Suppose we would like to find the minimizer of a certain (continuously differentiable) function \\(F:\\mathbb{R}^n\\to \\mathbb{R}\\). A classical way is to start with any point \\(x_0\\), and then go along the direction of negative of gradient of \\(F\\). That is, we are solving\n\\[\\begin{array}{l}\nx^{\\prime}(t)=-\\nabla F(x(t)) \\quad \\text { for } t&gt;0, \\\\\nx(0)=x_0.\n\\end{array}\\]\nA solution to the above initial value problem (IVP) is called a gradient flow of \\(F\\). As the gradient always points at the direction where \\(F\\) increases the most, negative gradient will lead us to a (local) minimizer of \\(F\\). The existence and uniqueness theory of ODE guarantees a satisfactory solution of this gradient flow.\n\n\nNote that along any smooth curve \\(x_t\\) it holds that \\(\\begin{aligned} F(x(s))-F(x(t))=\\int_s^t-\\nabla F(x(r)) \\cdot x^{\\prime}(r) \\mathrm{d} r & \\leq \\int_s^t|\\nabla F(x(r))|\\left|x^{\\prime}(r)\\right| \\mathrm{d} r \\\\ & \\leq \\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r\\end{aligned}\\)\nNote that if \\(x(t)\\) solves the gradient flow IVP, then\n\\[\nF(x(s))-F(x(t))=\\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r, \\quad  \\forall s&lt;t\n\\] which we call Energy Dissipation Enequality (EDE).\nNow if \\(F\\) is \\(\\lambda\\)-convex, the inequality that characterizes the gradient is\n\\[\nF(y) \\geq F(x)+\\frac{\\lambda}{2}|x-y|^2+p \\cdot(y-x) \\quad \\text { for all } y \\in \\mathbb{R}^d .\n\\] We can pick a curve \\(x(t)\\) and a point \\(y\\) and compute\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2=(y-x(t)) \\cdot\\left(-x^{\\prime}(t)\\right)\n\\]\nConsequently, imposing\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2 \\leq F(y)-F(x(t))-\\frac{\\lambda}{2}|x(t)-y|^2\n\\]\nfor all \\(y\\), will be equivalent to \\(-x^{\\prime}(t) \\in-\\partial F(x(t))\\). This will provide a second characterization (called EVI, Evolution Variational Inequality) of gradient flows in a metric environment."
  },
  {
    "objectID": "GFMetricSpace.html#gradient-flows-in-euclidean-space",
    "href": "GFMetricSpace.html#gradient-flows-in-euclidean-space",
    "title": "Gradient Flows in Metric Space",
    "section": "",
    "text": "We first briefly reivew gradient flows in \\(\\mathbb{R}^n\\). The motivation comes from the so-called “gradient descent method”: Suppose we would like to find the minimizer of a certain (continuously differentiable) function \\(F:\\mathbb{R}^n\\to \\mathbb{R}\\). A classical way is to start with any point \\(x_0\\), and then go along the direction of negative of gradient of \\(F\\). That is, we are solving\n\\[\\begin{array}{l}\nx^{\\prime}(t)=-\\nabla F(x(t)) \\quad \\text { for } t&gt;0, \\\\\nx(0)=x_0.\n\\end{array}\\]\nA solution to the above initial value problem (IVP) is called a gradient flow of \\(F\\). As the gradient always points at the direction where \\(F\\) increases the most, negative gradient will lead us to a (local) minimizer of \\(F\\). The existence and uniqueness theory of ODE guarantees a satisfactory solution of this gradient flow.\n\n\nNote that along any smooth curve \\(x_t\\) it holds that \\(\\begin{aligned} F(x(s))-F(x(t))=\\int_s^t-\\nabla F(x(r)) \\cdot x^{\\prime}(r) \\mathrm{d} r & \\leq \\int_s^t|\\nabla F(x(r))|\\left|x^{\\prime}(r)\\right| \\mathrm{d} r \\\\ & \\leq \\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r\\end{aligned}\\)\nNote that if \\(x(t)\\) solves the gradient flow IVP, then\n\\[\nF(x(s))-F(x(t))=\\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r, \\quad  \\forall s&lt;t\n\\] which we call Energy Dissipation Enequality (EDE).\nNow if \\(F\\) is \\(\\lambda\\)-convex, the inequality that characterizes the gradient is\n\\[\nF(y) \\geq F(x)+\\frac{\\lambda}{2}|x-y|^2+p \\cdot(y-x) \\quad \\text { for all } y \\in \\mathbb{R}^d .\n\\] We can pick a curve \\(x(t)\\) and a point \\(y\\) and compute\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2=(y-x(t)) \\cdot\\left(-x^{\\prime}(t)\\right)\n\\]\nConsequently, imposing\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2 \\leq F(y)-F(x(t))-\\frac{\\lambda}{2}|x(t)-y|^2\n\\]\nfor all \\(y\\), will be equivalent to \\(-x^{\\prime}(t) \\in-\\partial F(x(t))\\). This will provide a second characterization (called EVI, Evolution Variational Inequality) of gradient flows in a metric environment."
  },
  {
    "objectID": "GFMetricSpace.html#gradient-flows-in-metric-space",
    "href": "GFMetricSpace.html#gradient-flows-in-metric-space",
    "title": "Gradient Flows in Metric Space",
    "section": "Gradient Flows in Metric Space",
    "text": "Gradient Flows in Metric Space\nTo generalize the above IVP, we need to make sense of derivative and gradient in metric space setting.\nThe first notion is the generalization of “derivative” in metric space, so-called metric derivative defined as follows: Given a curve \\(x:[0, T] \\rightarrow X\\) valued in a metric space, \\[\n\\left|x^{\\prime}\\right|(t):=\\lim _{h \\rightarrow 0} \\frac{d(x(t), x(t+h))}{|h|}.\n\\] Note that there is no vector structure here, so we can only make sense of modulus of the velocity of a curve.\nThe second concept is the generalization of convexity. A function \\(F\\) is called geodeiscally convex if it is convex along any geodesic: \\[F(x(t)) \\leq(1-t) F(x(0))+t F(x(1)).\\] Similarly we define geodeisc \\(\\lambda\\)-convex if \\[\nF(x(t)) \\leq(1-t) F(x(0))+t F(x(1))-\\lambda \\frac{t(1-t)}{2} d^2(x(0), x(1)) .\n\\]\nLastly we define the metric version of \\(|\\nabla F|\\) as slope: Let \\(F: X \\rightarrow \\mathbb{R} \\cup\\{+\\infty\\}\\) and \\(x \\in X\\) be such that \\(F(x)&lt;\\infty\\). Then the slope \\(|\\nabla F|(x)\\) of \\(F\\) at \\(x\\) is:\n\\[\n|\\nabla F|(x):=\\varlimsup_{y \\rightarrow x} \\frac{(F(x)-F(y))^{+}}{d(x, y)}=\\max \\left\\{\\varlimsup_{y \\rightarrow x} \\frac{F(x)-F(y)}{d(x, y)}, 0\\right\\} .\n\\]\nNow we are ready to define gradient flows in metric setting. There are two definitions(Ambrosio et al. 2013):\n(Energy Dissipation Equality definition of GF - EDE) Let \\(E: X \\rightarrow \\mathbb{R} \\cup\\{+\\infty\\}\\) and let \\(\\bar{x} \\in X\\) be such that \\(E(\\bar{x})&lt;\\infty\\). We say that \\([0, \\infty) \\ni t \\mapsto x_t \\in X\\) is a Gradient Flow in the EDE sense starting at \\(\\bar{x}\\) provided it is a locally absolutely continuous curve, \\(x_0=\\bar{x}\\) and\n\\[\nE\\left(x_s\\right)+\\frac{1}{2} \\int_t^s\\left|\\dot{x}_r\\right|^2 d r+\\frac{1}{2} \\int_t^s|\\nabla E|^2\\left(x_r\\right) d r=E\\left(x_t\\right), \\quad \\forall 0 \\leq t \\leq s\n\\]\nThe second definition is the EVI version:\n(Evolution Variation Inequality definition of GF - EVI) Let \\(E: X \\rightarrow \\mathbb{R} \\cup\\{+\\infty\\}\\), \\(\\bar{x} \\in \\overline{\\{E&lt;\\infty\\}}\\) and \\(\\lambda \\in \\mathbb{R}\\). We say that \\((0, \\infty) \\ni t \\mapsto x_t \\in X\\) is a Gradient Flow in the EVI sense (with respect to \\(\\lambda\\) ) starting at \\(\\bar{x}\\) provided it is a locally absolutely continuous curve in \\((0, \\infty), x_t \\rightarrow \\bar{x}\\) as \\(t \\rightarrow 0\\) and\n\\[\nE\\left(x_t\\right)+\\frac{1}{2} \\frac{d}{d t} d^2\\left(x_t, y\\right)+\\frac{\\lambda}{2} d^2\\left(x_t, y\\right) \\leq E(y), \\quad \\forall y \\in X, \\text { a.e. } t&gt;0\n\\]\nIt turns out that the EDE definition is weaker than the EVI definition, i.e. any EVI GF is automatically an EDE GF. The EDE definition is easier to get existence, and the EVI definition is used to get uniqueness results."
  }
]