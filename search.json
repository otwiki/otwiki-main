[
  {
    "objectID": "WGAN.html",
    "href": "WGAN.html",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "",
    "text": "The Wasserstein generative adversarial network (WGAN) (Arjovsky, Chintala, and Bottou 2017) is a recently proposed state-of-the-art generative model that leverages ideas from optimal transport to address key limitations of the traditional generative adversarial network (GAN) (Goodfellow et al. 2014).\nTo provide a comprehensive introduction to WGAN, we begin by clarifying the problem of generative modeling. Among various approaches, we focus specifically on GANs, outlining their main ideas and performing a straightforward theoretical analysis. Motivated by the challenges encountered in the training of GANs, WGANs are naturally introduced, highlighting how ideas from optimal transport improve the performance and stability of machine learning models."
  },
  {
    "objectID": "WGAN.html#general-overview",
    "href": "WGAN.html#general-overview",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "General Overview",
    "text": "General Overview\nIn recent years, it has been a hot topic to train machine learning models that exhibit human capabilities, one of which is creativity — the ability to create novel objects within a given category after observing a set of examples. Just like a human child, who can draw numerous distinct cats after seeing just a few real-life cats, one example of generative modeling is to train a machine learning model based on a set of cat images, which can produce an unlimited variety of new cat images that do not resemble those in the training set.\nIn brief, the problem of generative modeling has the following requirements:\n\nThe training set consists of a finite number of samples.\nThe generated outputs belong to the same category as the training samples.\nThe model is capable of producing an infinite variety of outputs.\n\nGenerative models have numerous connections and applications in various fields, e.g., synthetic data generation (Eigenschink et al. 2023), image generation (Oussidi and Elhassouny 2018), game theory (Cao, Guo, and Laurière 2020) and reinforcement learning (Franceschelli and Musolesi 2024), etc. We refer interested readers to (Harshvardhan et al. 2020) for a comprehensive survey on the analysis and applications of generative models."
  },
  {
    "objectID": "WGAN.html#problem-formulation",
    "href": "WGAN.html#problem-formulation",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Problem Formulation",
    "text": "Problem Formulation\nLet \\(\\mathcal{X}\\subset \\mathbb{R}^D\\) denote the sample space where training samples take values. The key assumption of generative modeling is the existence of an unknown probability distribution \\(\\mathbb{P}_r\\) on \\(\\mathcal{X}\\), from which the training samples are drawn. Therefore, generative modeling typically consists of two parts:\n\nApproximation: Approximate the distribution \\(\\mathbb{P}_r\\) with a parameterized model \\(\\mathbb{P}_\\theta\\in\\mathscr{P}(\\mathcal{X})\\), where \\(\\theta\\) represents the parameters, based on the finite number of training samples from \\(\\mathbb{P}_r\\).\nSampling: Generate samples from the approximated distribution \\(\\mathbb{P}_\\theta\\).\n\nIn the example of generating cat images, the approximation step concludes the features of cats, e.g., with a tail and four legs, while the sampling step creates new cat images based on the features learnt in the previous step. We remark that, the features learnt in the approximation step depend heavily on the quality of the training samples. If all the training samples are only showing black cats, then the color feature learnt by the model might be “all cats are black”, which is not necessarily the ground truth."
  },
  {
    "objectID": "WGAN.html#geometric-structure",
    "href": "WGAN.html#geometric-structure",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Geometric Structure",
    "text": "Geometric Structure\nBefore diving deeper into state-of-the-art methods of generative modeling, it is crucial to understand why classical density estimation methods in Statistics are not performing well. One of the reasons lies in the geometric structure of the problem that the distribution \\(\\mathbb{P}_r\\), in most cases, is supported only on a low-dimensional manifold within \\(\\mathbb{R}^D\\).\nConsider the task of image generation, where each image consists of approximately \\(10^6\\) pixels with three color channels, i.e., \\(D \\approx 3\\times 10^6\\). While each one of the images can be viewed as a single point in \\(\\mathbb{R}^D\\), the true distribution \\(\\mathbb{P}_r\\) is typically concentrated on a manifold with an intrinsic dimension \\(d\\), where \\(d\\leq 50\\) for most image datasets (Pope et al. 2021). This significant gap between the ambient dimension \\(D\\) and the intrinsic dimension \\(d\\) arises from implicit constraints that define the structure of \\(\\mathbb{P}_r\\). For instance, human face images require symmetry and the nose and ears have to adhere to specific shape characteristics.\nUnder this specific structure, perturbation-based sampling methods, which refer to the generation of a randomly perturbed version of one of the training samples, no longer work. As a simple example illustrating such failure, consider \\(\\mathbb{P}_r\\) as a uniform distribution on a unit circle \\(C^1\\subset\\mathbb{R}^2\\). Adding Gaussian noises to a point \\((x,y)\\in C^1\\) almost surely yields a point outside the manifold \\(C^1\\). In other words, general perturbations would destroy the manifold structure, if not carefully designed.\nSimilarly, classical density estimation methods face at least two major challenges:\n\nThe density approximation on the manifold. The density of \\(\\mathbb{P}_r\\), if exists on the manifold, is almost everywhere zero under the Lebesgue measure on \\(\\mathbb{R}^D\\).\nThe density-based sampling scheme. Sampling from a distribution on a high-dimensional space with a known density suffers from the curse of dimensionality.\n\nThose challenges motivate the development of state-of-the-art generative modeling approaches based on different ideas, as introduced in the following section."
  },
  {
    "objectID": "WGAN.html#sota-approaches",
    "href": "WGAN.html#sota-approaches",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "SOTA Approaches",
    "text": "SOTA Approaches\nPopular methods of generative modeling can be roughly categorized as:\n\nVariational Autoencoders (VAEs): VAEs are likelihood-based generative models that train an encoder to map samples to a latent space and a decoder to reconstruct samples from latent vectors.\nGenerative Adversarial Networks (GANs): GANs involve a generator and a discriminator network trained in an adversarial way, where the discriminator learns to distinguish between real and generated data, while the generator aims to create samples indistinguishable from real data.\nDiffusion models: Diffusion models progressively corrupt samples by adding random noise and reverses this process, reconstructing new samples from pure random noise.\n\nWhile all approaches share the same objective, their methodologies differ significantly. In the following discussion, we briefly introduce GANs to prepare the readers for the discussion on WGANs."
  },
  {
    "objectID": "WGAN.html#main-idea",
    "href": "WGAN.html#main-idea",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Main Idea",
    "text": "Main Idea\nAs mentioned above, a generative model shall have the “power of infinity”, i.e., the ability to produce an infinite variety of outputs. However, when it comes to numerical implementations of most algorithms, we are always trying to discretize continuous objects. For example, when numerical integration is performed, the integration domain is discretized into a large (but finite) number of small areas. Attaining infinity seems impossible at the first glance, but modern computers do attain infinity in one specific task — generating random samples from a given distribution. We draw distinct samples from a multivariate standard Gaussian \\(N(0,I)\\) each time the random number generator is called, which is actually the source of the “power of infinity”. Inspired by such observations, generative models shall learn how to map random samples from a known distribution, e.g., \\(N(0,I)\\), to outputs that follow \\(\\mathbb{P}_r\\), i.e., the models should be delivering randomness, rather than creating new randomness.\nMathematically speaking, we call \\(\\mathcal{Z}\\subset \\mathbb{R}^l\\) the latent space, where the latent random variable \\(Z\\) takes values. The random variable \\(Z\\) follows a given probability distribution \\(\\mathbb{P}_Z\\), which is often taken as a multivariate Gaussian. A GAN aims to learn a function parameterized by \\(\\theta\\): \\[\nG_\\theta:\\mathcal{Z}\\to\\mathcal{X},\n\\] such that the law of \\(G_\\theta(Z)\\) is approximately \\(\\mathbb{P}_r\\). In practice, \\(G_\\theta\\) is typically taken as a neural network, while \\(\\theta\\) denotes the collection of all the network paramaters. For the purpose of notation, we denote \\(\\mathbb{P}_\\theta\\) as the law of \\(G_\\theta(Z)\\), i.e., \\(\\mathbb{P}_\\theta = (G_\\theta)_\\#\\mathbb{P}_Z\\), so that the problem turns into: \\[\n\\inf_\\theta d(\\mathbb{P}_\\theta,\\mathbb{P}_r),\n\\] where \\(d\\) is some information divergence that measures the difference between two probability distributions. Such a formulation naturally poses two questions:\n\nHow to supervise the model with the finite number of training samples from \\(\\mathbb{P}_r\\)?\nHow to select the information divergence \\(d\\)?\n\nThe wisdom of GANs lies in using another neural network to approximate \\(d\\) without specifying it explicitly. The network that approximates \\(d\\) is parameterized by \\(w\\): \\[\nD_w:\\mathcal{X}\\to[0,1].\n\\]\nThe fundamental framework of GANs consists of a generator \\(G_\\theta\\) and a discriminator \\(D_w\\). The generator receives inputs as samples \\(z\\) from the known distribution \\(\\mathbb{P}_Z\\) and outputs the generated samples \\(G_\\theta(z)\\in\\mathcal{X}\\). The discriminator receives inputs as elements in \\(\\mathcal{X}\\) (could be samples in the training set or outputs of the generator \\(G_\\theta(z)\\)), and assigns a score as the probability that the input received is from the true distribution \\(\\mathbb{P}_r\\). In principle, the discriminator hopes to distinguish the training samples from \\(\\mathbb{P}_r\\) and the fake samples from \\(\\mathbb{P}_\\theta\\) that are produced by the generator, while the generator hopes to fool the discriminator on the top of that. Since the GAN architecture creates a competition between the generator and the discriminator, ideally, both networks perform well enough in fulfilling their respective tasks after training. The trained generator is exactly the generative model we have been referring to, while the auxiliary discriminator is simply disregarded.\nWith the GAN architecture in mind, it suffices to propose loss functions for both networks, as the only missing components. The constructions of loss functions are rather intuitive: if the input of the discriminator comes from \\(\\mathbb{P}_r\\), its output shall be close to one; if the input of the discriminator comes from \\(\\mathbb{P}_\\theta\\), its output shall be close to zero. With the logarithms introduced, the discriminator maximizes \\(\\mathbb{E}_{X\\sim\\mathbb{P}_r}\\log D_\\omega(X) + \\mathbb{E}_{Z\\sim\\mathbb{P}_Z}\\log (1-D_\\omega(G_\\theta(Z)))\\) w.r.t. \\(w\\). When the discriminator reaches its optimum, the generator aims to minimize the same loss in order to fool the discriminator, resulting in the optimization problem: \\[\n\\inf_\\theta\\sup_\\omega \\mathbb{E}_{X\\sim\\mathbb{P}_r}\\log D_\\omega(X) + \\mathbb{E}_{Z\\sim\\mathbb{P}_Z}\\log (1-D_\\omega(G_\\theta(Z))).\n\\]\nWe leave the following remarks on the training of GANs:\n\nThe generator and the discriminator share exactly the same loss function, but are optimizing it in opposite directions. In this sense, GAN has essential difference from the well-known actor-critic algorithm in reinforcement learning, where two networks have their respective loss functions to optimize.\nThe expectations in the loss functions are approximated by Monte Carlo. For numerical implementations, one only needs to compute the loss function, call backpropagation for gradient computations (w.r.t. \\(w\\) and \\(\\theta\\)), and conduct gradient descent/ascent for the parameters of the generator/discriminator network.\nDue to the inf-sup structure of the problem, one typically trains the discriminator for several epochs before training the generator for a single epoch. GANs work in the way that the training samples supervise the discriminator, and the discriminator supervises the generator."
  },
  {
    "objectID": "WGAN.html#theoretical-analysis",
    "href": "WGAN.html#theoretical-analysis",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Theoretical Analysis",
    "text": "Theoretical Analysis\nWe demonstrate a simple theoretical analysis for the GAN optimization problem (Goodfellow et al. 2014). Due to technical difficulties modeling the parameter optimization procedure within neural networks, we omit the dependence on the parameters \\(\\theta\\) and \\(w\\), and rewrite the optimization problem in terms of the generator \\(G\\) and the discriminator \\(D\\). For simplicity, we adopt a change of variable \\(Y := G(Z)\\) to absorb all dependencies on the generator \\(G\\). Since \\(Y\\) is the output of the generator, it follows the distribution \\(\\mathbb{P}_\\theta\\). The inner layer optimization of the inf-sup problem turns out to be explicitly solvable: \\[\n\\sup_D f(D):=\\mathbb{E}_{X\\sim\\mathbb{P}_r}\\log D(X) + \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}\\log (1-D(Y)).\n\\] Compute the first variation of \\(f\\) in \\(D\\) w.r.t. the perturbation \\(\\psi\\): \\[\n\\delta f(D)(\\psi) := \\lim_{\\varepsilon\\to 0}\\frac{f(D + \\varepsilon \\psi) - f(D)}{\\varepsilon}\n= \\mathbb{E}_{X\\sim\\mathbb{P}_r}\\frac{\\psi(X)}{D(X)} - \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}\\frac{\\psi(Y)}{1-D(Y)}.\n\\] Assume \\(\\mathbb{P}_r\\) and \\(\\mathbb{P}_\\theta\\) have respective densities \\(p_r\\) and \\(p_\\theta\\) w.r.t. the Lebesgue measure. The optimality criterion \\(\\delta f(D^*)(\\psi) = 0\\), \\(\\forall \\psi\\) implies that \\[\nD^* = \\frac{p_r}{p_r + p_\\theta}.\n\\] Plugging back into the objective function yields \\[\nf(D^*) = \\int \\left[p_r(x)\\log\\frac{p_r(x)}{\\frac{p_r(x) + p_\\theta(x)}{2}} + p_\\theta(x)\\log\\frac{p_\\theta(x)}{\\frac{p_r(x) + p_\\theta(x)}{2}}\\right]\\,dx - 2\\log 2.\n\\] This quantity can be identified with the Jensen-Shannon divergence: \\[\nf(D^*) = D_{\\text{KL}} \\left(\\mathbb{P}_r||\\frac{\\mathbb{P}_r + \\mathbb{P}_\\theta}{2}\\right) + D_{\\text{KL}} \\left(\\mathbb{P}_\\theta||\\frac{\\mathbb{P}_r + \\mathbb{P}_\\theta}{2}\\right) - 2\\log 2=: 2\\text{JS}(\\mathbb{P}_r,\\mathbb{P}_\\theta) - 2\\log 2.\n\\] Consequently, if the discriminator has reached its optimum \\(D^*\\), training the generator is equivalent to minimizing the Jensen-Shannon divergence between the real and the approximated distributions."
  },
  {
    "objectID": "WGAN.html#motivation",
    "href": "WGAN.html#motivation",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Motivation",
    "text": "Motivation\nDespite great success in practical applications, the training of GANs is notoriously unstable. One of the main issues is called mode collapse, the phenomenon that a trained GAN generates samples of one single variety while ignoring other possible modes. It is widely believed that the main cause of mode collapse is the lack of control on the discriminator (Kushwaha, Nandi, et al. 2020). Philosophically, if the discriminator learns much faster than the generator, it criticizes all the samples produced by the generator, so that the generator does not know how to proceed. Conversely, if the discriminator learns much slower than the generator, it accepts all the results generated by the generator, which once again causes the similar issue. Hence, normal GAN training requires a careful adjustment in the relative learning speed of the generator and the discriminator. Numerically, we shall not train the discriminator till optimality before training the generator, despite the actual inf-sup structure of the optimization problem.\nThere are two main streams of ideas in the literature to augment the stability of GANs:\n\nMinimize an information divergence other than the Jensen-Shannon divergence.\nUse multiple generators to explicitly enforce GANs to capture diverse modes.\n\nWasserstein GANs take the first approach and use the Wasserstein distance (with \\(p=1\\)) instead of the Jensen-Shannon divergence. To understand what motivates WGANs, there are two main questions to answer:\n\nWhat is the advantage of the Wasserstein distance over the Jensen-Shannon divergence?\nHow to encode the minimization of the Wasserstein distance in the loss functions?"
  },
  {
    "objectID": "WGAN.html#advantage-of-wasserstein-distance",
    "href": "WGAN.html#advantage-of-wasserstein-distance",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Advantage of Wasserstein Distance",
    "text": "Advantage of Wasserstein Distance\nWe refer to a simple working example from (Arjovsky, Chintala, and Bottou 2017) that illustrates the advantage of using the Wasserstein distance \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\).\nConsider the setting where \\(\\mathcal{X}= \\mathbb{R}^2\\). Let \\(S_\\theta := \\{\\theta\\}\\times [0,1]\\) denote a family of unit line segments parameterized by \\(\\theta\\). Clearly, each \\(S_\\theta\\) is a one-dimensional manifold within \\(\\mathcal{X}\\). Let \\(\\mathbb{P}_\\theta\\) be the uniform distribution on \\(S_\\theta\\) and \\(\\mathbb{P}_r = \\mathbb{P}_{\\theta = 0}\\). The optimal value of \\(\\theta\\) that guarantees \\(\\mathbb{P}_r = \\mathbb{P}_\\theta\\) is thus \\(0\\). We compare four differently defined information divergences between \\(\\mathbb{P}_r\\) and \\(\\mathbb{P}_\\theta\\):\n\nTotal variation: \\[\n\\text{TV}(\\mathbb{P}_0,\\mathbb{P}_\\theta) := \\sup_{A\\in\\mathscr{B}_{\\mathcal{X}}}|\\mathbb{P}_0(A) - \\mathbb{P}_\\theta(A)|,\n\\] where \\(\\mathscr{B}_{\\mathcal{X}}\\) denotes the Borel sigma field on \\(\\mathcal{X}\\). Since the supports \\(S_0\\) and \\(S_\\theta\\) are disjoint when \\(\\theta \\neq 0\\), the total variation attains its maximum value \\(1\\). Therefore, \\[\n\\text{TV}(\\mathbb{P}_0,\\mathbb{P}_\\theta) = \\begin{cases} 0 &\\text{if}\\ \\theta = 0\\\\ 1 & \\text{else}\\end{cases}.\n\\]\nKullback-Leibler divergence: \\[\n\\text{KL}(\\mathbb{P}_0||\\mathbb{P}_\\theta) := \\begin{cases} \\mathbb{E}_{\\mathbb{P}_\\theta} \\left(\\frac{\\text{d}\\mathbb{P}_0}{\\text{d}\\mathbb{P}_\\theta}\\log \\frac{\\text{d}\\mathbb{P}_0}{\\text{d}\\mathbb{P}_\\theta}\\right)& \\text{if}\\ \\mathbb{P}_0 &lt;&lt;\\mathbb{P}_\\theta\\\\ \\infty & \\text{else}\\end{cases},\n\\] where \\(&lt;&lt;\\) denotes the absolute continuity between measures. Since the supports \\(S_0\\) and \\(S_\\theta\\) are disjoint when \\(\\theta \\neq 0\\), absolute continuity fails and the KL divergence is infinite. Therefore, \\[\n\\text{KL}(\\mathbb{P}_0||\\mathbb{P}_\\theta) = \\begin{cases} 0 &\\text{if}\\ \\theta = 0\\\\ \\infty & \\text{else}\\end{cases}.\n\\]\nJensen-Shannon divergence: \\[\n\\text{JS}(\\mathbb{P}_0,\\mathbb{P}_\\theta) := \\frac{1}{2}\\text{KL}(\\mathbb{P}_0||\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}) + \\frac{1}{2}\\text{KL}(\\mathbb{P}_\\theta||\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}).\n\\] Note that \\(\\mathbb{P}_0&lt;&lt;\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}\\), hence the Jensen-Shannon divergence is always finite. When \\(\\theta\\neq 0\\), we calculate the two Radon-Nikodym derivatives \\[\n\\frac{\\text{d}\\mathbb{P}_0}{\\text{d}\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}} = 2\\mathbb{I}_{S_0},\\quad\n\\frac{\\text{d}\\mathbb{P}_\\theta}{\\text{d}\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}} = 2\\mathbb{I}_{S_\\theta},\n\\] where \\(\\mathbb{I}_A\\) denotes the indicator of a set \\(A\\). Those conclusions could be verified from definitions \\[\n\\int_A 2\\mathbb{I}_{S_0}(x)\\,\\text{d}\\frac{\\mathbb{P}_0 + \\mathbb{P}_\\theta}{2}(x) = \\mathbb{P}_0(A\\cap S_0) + \\mathbb{P}_\\theta(A\\cap S_0) = \\mathbb{P}_0(A),\\ \\forall A\\in\\mathscr{B}_{\\mathcal{X}}.\n\\] Therefore, \\[\n\\text{JS}(\\mathbb{P}_0,\\mathbb{P}_\\theta) = \\begin{cases} 0 &\\text{if}\\ \\theta = 0\\\\ \\log 2 & \\text{else}\\end{cases}.\n\\]\nWasserstein distance with \\(p=1\\): \\[\nW(\\mathbb{P}_0,\\mathbb{P}_\\theta) := \\inf_{X\\sim \\mathbb{P}_0,Y\\sim\\mathbb{P}_\\theta} \\mathbb{E}\\|X - Y\\|_2.\n\\] From geometric intuition, \\(T(x) = x+\\theta\\) is the optimal transport map under the convex cost \\(c(x,y) = \\|x-y\\|_2\\). Therefore, \\[\nW(\\mathbb{P}_0,\\mathbb{P}_\\theta) = |\\theta|.\n\\]\n\nObviously, only the Wasserstein distance exhibits the continuity in parameter \\(\\theta\\), while other divergences have discontinuities at \\(0\\). This observation has been made rigorous in general cases (Arjovsky, Chintala, and Bottou 2017). The authors prove that if the generator \\(G_\\theta\\) is continuous in \\(\\theta\\), then so is \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\). If the generator \\(G_\\theta\\) is locally Lipschitz in \\(\\theta\\), then so is \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\) under mild regularity assumptions, which, by Rademacher’s theorem, implies that \\(W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\) is almost everywhere differentiable in \\(\\theta\\).\nThe continuity and differentiability w.r.t. the parameter is crucial in the training of GANs and is closely related to the issue of vanishing gradients. When the discriminator has reached its optimum and one gradient step of the generator is performed, the gradient actually refers to \\(\\nabla_{\\theta}[d(\\mathbb{P}_r,\\mathbb{P}_\\theta)]\\). In the example above, all divergences except the Wasserstein distance provide trivial (vanishing) gradients for the generator. By contrast, the Wasserstein distance provides a gradient that is \\(1\\) for positive \\(\\theta\\) and \\(-1\\) for negative \\(\\theta\\), which is always effective for the generator. Thanks to the physical interpretation of optimal transport, the Wasserstein distance, as a specific optimal-transport-based information divergence, greatly mitigates the issue of vanishing gradients for the generator."
  },
  {
    "objectID": "WGAN.html#construction-of-wgan",
    "href": "WGAN.html#construction-of-wgan",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Construction of WGAN",
    "text": "Construction of WGAN\nIn terms of numerical implementation, an inf-sup formulation of the optimization problem is required, as it clearly specifies the loss functions. Luckily, the Kantorovich-Rubinstein duality provides the representation: \\[\nW(\\mathbb{P}_r,\\mathbb{P}_\\theta) = \\sup_{\\|f\\|_L\\leq 1} \\mathbb{E}_{X\\sim \\mathbb{P}_r} f(X) - \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}  f(Y),\n\\] where the supremum is taken over all Lipschitz functions \\(f\\) with Lipschitz constants no larger than \\(1\\). Substituting \\(f = \\frac{g}{K}\\) allows the relaxation in the Lipschitz constant: \\[\nK\\cdot W(\\mathbb{P}_r,\\mathbb{P}_\\theta) = \\sup_{\\|g\\|_L\\leq K} \\mathbb{E}_{X\\sim \\mathbb{P}_r} g(X) - \\mathbb{E}_{Y\\sim\\mathbb{P}_\\theta}  g(Y).\n\\] Therefore, the complete optimization problem of WGANs w.r.t. the parameterized generator \\(G_\\theta\\) and discriminator \\(D_w\\) is given by \\[\n\\inf_\\theta\\sup_{w:\\|D_w\\|_L\\leq K} \\mathbb{E}_{X\\sim \\mathbb{P}_r} D_w(X) - \\mathbb{E}_{Z\\sim\\mathbb{P}_Z}  D_w(G_\\theta(Z)).\n\\] The objective of this optimization problem serves as the loss functions in WGANs, shared by both the generator and the discriminator. One last piece of detail for the implementation of WGANs lies in the way to numerically impose the constraint \\(\\|D_w\\|_L\\leq K\\) for parameters \\(w\\). If \\(D_w\\) is parameterized by a feedforward neural network, each layer of the network consists of an affine mapping \\(x\\mapsto Wx + b\\) with weight \\(W\\), bias \\(b\\), and a mapping of the nonlinear activation function \\(\\sigma\\). Clearly, the affine mapping has Lipschitz constant \\(\\|W\\|\\), while the common activation functions, e.g., sigmoid, hyperbolic tangent, ReLU, etc, have Lipschitz constants \\(L_\\sigma&lt;\\infty\\). Therefore, the composition \\(x\\mapsto \\sigma(Wx+b)\\) has a Lipschitz constant \\(L_\\sigma\\|W\\|\\), which is uniform w.r.t. the trainable network parameters if \\(\\|W\\|\\) has a uniform upper bound, i.e., when \\(w\\) is restricted to a compact domain. As a result, WGANs impose the Lipschitz constraint by parameter clipping, e.g., restricting each component of \\(w\\) to take values in \\([-0.01,0.01]\\).\nWe remark that, by imposing the Lipschitz constraint, the discriminator is not allowed to saturate, thus providing effective gradients everywhere. As shown in the plot below, the GAN discriminator is too good at distinguishing two Gaussian distributions to provide effective gradients. In contrary, the WGAN discriminator (critic) with parameter clipping does not perform that well in distinguishing, but does provide effective gradients everywhere. Philosophically, the GAN discriminator is too smart that it demotivates the learning of the generator!\n\n\n\nThe comparisons of the behavior of optimal GAN discriminator and WGAN discriminator (critic) in dintinguishing two Gaussian distributions. The GAN discriminator saturates and provide trivial gradients within the main support of two Gaussian distributions. However, the WGAN discrinimator (critic) has a linear growth and provides effective gradients on the whole space. (Arjovsky, Chintala, and Bottou 2017)"
  },
  {
    "objectID": "WGAN.html#conclusions-and-future-studies",
    "href": "WGAN.html#conclusions-and-future-studies",
    "title": "Wasserstein Generative Adversarial Network",
    "section": "Conclusions and Future Studies",
    "text": "Conclusions and Future Studies\nTo conclude, WGANs improve the training stability of GANs by switching the minimization of the Jensen-Shannon divergence to that of the Wasserstein distance, which is continuous in terms of the parameter of the generator. Numerically, we make use of the Kantorovich-Rubinstein duality to keep the inf-sup structure of the optimization problem, and adopt parameter clipping to impose the Lipschitz continuity.\nAs pointed out in (Arjovsky, Chintala, and Bottou 2017), future studies can be conducted in the following directions:\n\nImpose the Lipschitz continuity of the neural network with a different technique, due to the subtleties in choosing the clipping hyperparameter.\nExplain why WGAN training is unstable when momentum based optimizers (like Adam) or high learning rates are used."
  },
  {
    "objectID": "GeoGenGeo.html",
    "href": "GeoGenGeo.html",
    "title": "Geodesics and Generalized Geodesics",
    "section": "",
    "text": "There are many ways that we can describe Wasserstein metric. One of them is to characterize absolutely continuos curves (AC)(p.188) and provide a dynamic formulation of the special case \\(W_{2}^{2}\\) Namely, it is possible to see \\(W_{2}^{2}(\\mu, \\nu)\\) as an infimum of the lengts of curves that satisfy Continuity equation."
  },
  {
    "objectID": "GeoGenGeo.html#introduction",
    "href": "GeoGenGeo.html#introduction",
    "title": "Geodesics and Generalized Geodesics",
    "section": "",
    "text": "There are many ways that we can describe Wasserstein metric. One of them is to characterize absolutely continuos curves (AC)(p.188) and provide a dynamic formulation of the special case \\(W_{2}^{2}\\) Namely, it is possible to see \\(W_{2}^{2}(\\mu, \\nu)\\) as an infimum of the lengts of curves that satisfy Continuity equation."
  },
  {
    "objectID": "GeoGenGeo.html#geodesics-in-general-metric-spaces",
    "href": "GeoGenGeo.html#geodesics-in-general-metric-spaces",
    "title": "Geodesics and Generalized Geodesics",
    "section": "Geodesics in general metric spaces",
    "text": "Geodesics in general metric spaces\nFirst, we will introduce definition of the geodesic in general metric space \\((X,d)\\). In the following sections. we are going to follow a presentation from the book by Santambrogio with some digression, here and there.\n\nFor the starting point, we need to introduce length of the curve in our metric space \\((X,d)\\).\n\nDefinition. A length of the curve \\(\\omega:[0,1] \\rightarrow X\\) is defined by \\(L(\\omega)=\\sup\\{ \\sum_{j=0}^{n-1} d(\\omega(t_{j}),\\omega(t_{j+1})) | \\quad n \\geq 2,\\quad 0=t_{0}&lt;t_{1}&lt;...&lt;t_{n-1}=1 \\}\\)\n\nSecondly, we use the definition of length of a curve to introduce a geodesic curve.\n\nDefinition. A curve \\(c:[0,1] \\rightarrow X\\) is said to be geodesic between \\(x\\) and \\(y\\) in \\(X\\) if it minimizes the length \\(L(\\omega)\\) among all the curves \\(\\omega:[0,1] \\rightarrow X\\)  such that \\(x=\\omega(0)\\) and \\(y=\\omega(1)\\).\n\n\nSince we have a definition of a geodesic in the general metric space, it is natural to think of Riemannian structure. It can be formally defined. More about this topic can be seen in the following article Formal Riemannian Structure of the Wasserstein_metric.\n\nNow, we proceed with necessary definitions in order to be able to understand Wasserstein metric in a different way.\n\nDefinition. A metric space \\((X,d)\\) is called a length space if it holds \\(d(x,y)=\\inf \\{ L(\\omega) | \\quad  \\omega \\in AC(X), \\quad \\omega(0)=x \\quad \\omega(1)=y \\}.\\)\n\nA space \\((X,d)\\) is called geodesic space if the distance \\(d(x,y)\\) is attained for some curve \\(\\omega\\).\n\nDefinition. In a length space, a curve \\(\\omega:[0,1]\\rightarrow X\\) is said to be constant speed geodesic between \\(\\omega(0)\\) and \\(\\omega(1)\\) in \\(X\\) if it satisfies\n\n\n\\(d(\\omega(s),\\omega(t))=|t-s|d(\\omega(0),\\omega(1))\\) for all \\(t,s \\in [0,1]\\)\nIt is clear that constant-speed geodesic curve \\(\\omega\\) connecting \\(x\\) and \\(y\\) is a geodesic curve. This is very important definition since we have that every constant-speed geodesic \\(\\omega\\) is also in \\(AC(X)\\) where \\(|\\omega'(t)|=d(\\omega(0),\\omega(1))\\) almost everywhere in \\([0,1]\\).  In addition, minimum of the set \\(\\{ \\int_{0}^{1}|c'(t)|^{p}dt |  c:[0,1]\\rightarrow X, c(0)=x, c(1)=y \\}\\) is attained by our constant-speed geodesic curve \\(\\omega.\\) Last fact is important since it is connected to Wasserstein \\(p\\) metric. For more information, please take a look at Wasserstein metric.\nFor more information on constant-speed geodesics, especially how they depend on uniqueness of the plan that is induced by transport and characterization of a constant-speed geodesic look at the book by L.Ambrosio, N.Gilgi, G.Savaré  or the book by Santambrogio."
  },
  {
    "objectID": "GeoGenGeo.html#dynamic-formulation-of-wasserstein-distance",
    "href": "GeoGenGeo.html#dynamic-formulation-of-wasserstein-distance",
    "title": "Geodesics and Generalized Geodesics",
    "section": "Dynamic formulation of Wasserstein distance",
    "text": "Dynamic formulation of Wasserstein distance\nFinally, we can rephrase Wasserstein metrics in dynamic language as mentioned in the Introduction.\n\nWhenever \\(\\Omega \\subseteq \\mathcal{R}^{d}\\) is convex set, \\(W_{p}(\\Omega)\\) is a geodesic space. Proof can be found in the book by Santambrogio.\n\nTheorem. Let \\(\\mu, \\nu \\in \\mathcal{P}_{2}(R^{d})\\). Then \\(W_{p}^{p}(\\mu, \\nu)=\\inf_{(\\mu(t).\\nu(t))} \\{\\int_{0}^{1} |v(,t)|_{L^{p}(\\mu(t))}^{p}dt \\quad | \\quad \\partial_{t}\\mu+\\nabla\\cdot(v\\mu)=0,\\quad \\mu(0)=\\mu,\\quad \\mu(1)=\\nu \\}.\\)\n\n\nIn special case, when \\(\\Omega\\) is compact, infimum is attained by some constant-speed geodesic."
  },
  {
    "objectID": "GeoGenGeo.html#generalized-geodesics",
    "href": "GeoGenGeo.html#generalized-geodesics",
    "title": "Geodesics and Generalized Geodesics",
    "section": "Generalized geodesics",
    "text": "Generalized geodesics\nThere are many ways to generalize this fact. We will talk about a special case \\(p=2\\) and a displacement convexity. Here we follow again book by Santambrogio.\n\nIn general, the functional \\(\\mu \\rightarrow W_{2}^{2}(\\mu,\\nu)\\) is not a displacement convex. We can fix this by introducing a generalized geodesic.\n\nDefinition. Let \\(\\rho \\in \\mathcal{P}(\\Omega)\\) be an absolutely continuous measure and \\(\\mu_{0}\\) and \\(\\mu_{1}\\) probability measures in \\(\\mathcal{P}(\\Omega)\\). We say that \\(\\mu_{t} = ((1-t)T_{0}+tT_{1})\\#\\rho\\)  is a generalized geodesic in \\(\\mathcal{W}_{2}(\\Omega)\\) with base \\(\\rho\\), where \\(T_{0}\\) is the optimal transport plan from \\(\\rho\\) to \\(\\mu_{0}\\) and \\(T_{1}\\) is the optimal transport plan from \\(\\rho\\) to \\(\\mu_{1}\\).\n\n\nBy calculation, we have the following \\(W_{2}^{2}(\\mu_{t},\\rho) \\leq (1-t)W_{2}^{2}(\\mu_{0},\\rho) + tW_{2}^{2}(\\mu_{1},\\rho).\\)\nTherefore, along the generalized geodesic, the functional \\(t \\rightarrow W_{2}^{2}(\\mu_{t},\\rho)\\) is convex.\nThis fact is very important in establishing uniqueness and existence theorems in the geodesic flows."
  },
  {
    "objectID": "Monge.html",
    "href": "Monge.html",
    "title": "Monge Problem",
    "section": "",
    "text": "The optimal transport problem first came up in the 18th century, brought by Gaspard Monge. The original problem is called the Monge Problem, which wants to find an optimal way to rearrange the dirt dig out from the land into castle walls or other desired shapes. The “optimal way” means the way with the minimal “cost”, or workload. In the following, we will first formulate the Monge problem, then explain why it is a challenging problem. On top of that, we introduce the Kantorovich’s problem which is a generalized version of Monge problem but it perfectly tackles the challenges. Finally, we introduce Brenier’s theorem and discuss the solvability of these two problems."
  },
  {
    "objectID": "Monge.html#formulation-of-monge-problem",
    "href": "Monge.html#formulation-of-monge-problem",
    "title": "Monge Problem",
    "section": "Formulation of Monge Problem",
    "text": "Formulation of Monge Problem\nTo begin with, we use probability measures to represent the piles of dirt. Suppose \\((X,d)\\) is a metric space that is equipped with a natural topology generated by all the open balls. The topology defines the collection of open sets, and we denote \\(\\mathcal{B}(X)\\) as the \\(\\sigma\\)-algebra generated by the open sets, which is called the Borel \\(\\sigma\\)-algebra on the metric space \\((X,d)\\). Denote \\(\\mathcal{M}(X)\\) as the set of all finite measures(a pile has finite amount of dirt!) defined on the measurable space \\((X,\\mathcal{B}(X))\\). For a measure \\(\\mu\\in\\mathcal{M}(X)\\) and \\(B\\in\\mathcal{B}(X)\\), \\(\\mu(B)\\) represents how much dirt is contained in the set \\(B\\), so a measure can precisely characterize the shape of a dirt pile. When we rearrange the dirt pile, we keep the total amount unchanged, so without losing of generality we can assume the total mass \\(\\mu(X)=1\\), meaning we restrict our attention to the set of all probability measures on \\((X,\\mathcal{B}(X))\\), denoted by \\(\\mathcal{P}(X)\\).\nThe next step is to define the behavior of “transport”. A possible way is to use the following notion of transport map.\nDefinition 1. Transport map\nGive two probability measures \\(\\mu,\\nu\\in\\mathcal{P}(X)\\), a measurable function \\(t:X\\rightarrow X\\) transports \\(\\mu\\) onto \\(\\nu\\) if :\\(\\nu(B) = \\mu(t^{-1}(B)),\\ \\forall \\ B\\in\\mathcal{B}(X)\\) where \\(t^{-1} = \\{x\\in X: t(x)\\in B\\}\\) is the preimage. We call \\(\\nu\\) the push-forward measure of \\(\\mu\\) under \\(t\\), denoted by \\(\\nu = t_\\# \\mu\\) or \\(\\nu = \\mu\\circ t^{-1}\\).\nThe intuition of this definition is that a transport map \\(t\\) moves all of the dirt at \\(x\\in X\\) to the location \\(t(x)\\). Now we can formulate the Monge problem mathematically.\nDefinition 2. Monge’s Problem\nGiven \\(\\mu,\\nu\\in\\mathcal{P}(X)\\), Monge problem is the following optimization problem. :\\(\\inf_{t} \\int_{X}|t(x)-x|d\\mu(x), \\ s.t. \\ t:X\\rightarrow X \\ \\text{is measurable and  }t_{\\#}\\mu = \\nu\\)\nTo illustrate, \\(|t(x)-x|\\) is the distance of moving the dirt from \\(x\\) to \\(t(x)\\), and the integral \\(\\int_{X}|t(x)-x|d\\mu(x)\\) is the total cost or effort. The condition \\(t_{\\#}\\mu =\\nu\\) ensures that \\(t\\) is a transport map from \\(\\mu\\) onto \\(\\nu\\), meaning \\(t\\) rearranges the dirt in the shape of \\(\\mu\\) to look like that of \\(\\nu\\). If \\(t\\) solves the Monge problem, meaning it realized the infimum, then we call it an optimal transport map. Here, we use the cost function \\(c(x^1,x^2)=|x^1-x^2|\\), but it can also be replaced by other general cost functions. The problem can easily be generalized to the one regarding two different spaces \\(X,Y\\), with \\(\\mu\\in\\mathcal{P}(X)\\), \\(\\nu\\in\\mathcal{P}(Y)\\) and \\(c:X\\times Y\\rightarrow[0,\\infty)\\) is the cost function, but it is essentially the same as the one regarding the same space.\nThe Monge problem is important because it provides us with spatial insight of how far two probability measures are. If two probability measures are “closed to each other”, there should be some transport map such that the total effort of transport is small. This idea leads to the concept of Wasserstein metric."
  },
  {
    "objectID": "Monge.html#challenges-of-monge-problem",
    "href": "Monge.html#challenges-of-monge-problem",
    "title": "Monge Problem",
    "section": "Challenges of Monge Problem",
    "text": "Challenges of Monge Problem\nThe Monge problem turns out to be a very challenging problem for several reasons as follows.\n\nThe constraint set can be empty. Given \\(\\mu,\\nu\\in\\mathcal{P}(X)\\), it does not necessarily exist a transport map from \\(\\mu\\) onto \\(\\nu\\): \\(t_{\\#}\\mu=\\nu\\). For example, consider \\(\\mu=\\delta_{0}\\), the point mass at \\(0\\), and \\(\\nu\\) is the uniform measure on \\([0,1]\\). For every measurable function \\(t:X\\rightarrow X\\), \\(t_{\\#}\\mu\\) should be a point mass at \\(t(0)\\) so \\(t\\) cannot yield a uniform distribution on \\([0,1]\\). The reason is that a transport map cannot split the mass concentrated at one point\\(---\\)it transports all the mass at point \\(x\\) to the location \\(t(x)\\).\n\n*Solutions may not be unique. Let \\(\\mu\\), \\(\\nu\\) be the uniform distribution on \\((0,1)\\) and \\((1/4,5/4)\\) respectively. Consider the following two transport maps.\n\nTransport all the mass on \\((0,1/4)\\) to \\((1,5/4)\\) and keep the rest unchanged. In this case, \\(t_0(x) = x+1\\) for $ x(0,1/4)$ and \\(t(x) = x\\) for $ x(1/4,1)$ is the transport map and the cost is \\(\\int|t_0(x)-x|d\\mu(x) = \\int_{0}^{1/4}1dx=1/4\\).\nShift all the mass to the right by \\(1/4\\). In this case, \\(t_1(x)=x+1/4\\) is the transport map and the cost is \\(\\int |t_1(x)-x|d\\mu(x) = \\int_{0}^{1}1/4 dx = 1/4\\).\n\nIt is not hard to imagine that other methods of transport will cause larger effort, so this problem has two different optimal solutions.\n\nThe constraint set is not convex. In general, for an optimization problem, we usually take an initial guess of the solution and perturb it to see if the objective function decreases. The convexity of constraint set guarantees that the point after perturbation is still in the constraint set. However, this is not always true for Monge problem. Consider the transport maps \\(t_{0}\\) and \\(t_1\\) defined in the last example. Their convex combination is : \\[t\\_{\\alpha}(x) = (1-\\alpha)t\\_{0}(x) + \\alpha t\\_{1}(x)=\n\\begin{cases}\n(1-\\alpha)(x+1/4) +\\alpha(x+1), &x\\in (0,1/4);\\\\\n(1-\\alpha)(x+1/4) + \\alpha x , & x\\in (1/4,1);\n\\end{cases}\n\\] Take \\(\\alpha = 1/2\\), we have : \\[t\\_{1/2}(x) =\n\\begin{cases}\nx+5/8, &x\\in (0,1/4);\\\\\nx+1/8 , & x\\in (1/4,1);\n\\end{cases}\n\\]\nThen \\(\\nu((5/8,3/4))= 1/8\\), but \\((t_{{1/2}\\#}\\mu)((5/8,3/4)) = \\mu((0,1/8)\\cup(1/2,5/8))=1/4\\), so \\(t_{1/2}\\) does not give a transport map from \\(\\mu\\) onto \\(\\nu\\) any more."
  },
  {
    "objectID": "Monge.html#a-generalized-problem-kantorovichs-problem",
    "href": "Monge.html#a-generalized-problem-kantorovichs-problem",
    "title": "Monge Problem",
    "section": "A generalized problem: Kantorovich’s Problem",
    "text": "A generalized problem: Kantorovich’s Problem\nAn approach to tackle the problems mentioned in Section II is to use a “transport plan” instead of a transport map. This gives a more generalized version of the problem called Kantorovich Problem.\nDefinition 3. Transport plan\nGiven \\(\\mu,\\nu\\in\\mathcal{P}(X)\\), a transport plan is a probability measure \\(\\gamma\\in \\mathcal{P}(X\\times X)\\) on the product space \\((X\\times X,\\mathcal{B}(X\\times X))\\) such that \\((\\pi^{1})_{\\#} \\gamma = \\mu\\), \\((\\pi^{2})_{\\#}\\gamma = \\nu\\), where \\(\\pi^{i}:X\\times X\\rightarrow X\\), \\(\\pi^{i}(x_1,x_2)=x_i\\), \\(i =1,2\\) are projection maps. The set of all transport plans from \\(\\mu\\) to \\(\\nu\\) is denoted by \\(\\Gamma(\\mu,\\nu)\\).\nThe conditions \\((\\pi^{1})_{\\#} \\gamma = \\mu\\), \\((\\pi^{2})_{\\#}\\gamma = \\nu\\) give the right marginals: : (A) = ((^1)# )(A)= ((1){-1}(A))=(AX),\n(B) = ((^2)# )(A)= ((2){-1}(A))=(XB)  Therefore, a transport plan is a probability measure on the product space \\(X\\times X\\) that has marginals distributions \\(\\mu\\) and \\(\\nu\\). In addition, \\(\\gamma(A\\times B)\\) represents the amount of mass taken from set \\(A\\) that is sent to set \\(B\\). Since \\(\\gamma(A\\times B)\\leq \\gamma(A\\times X)=\\mu(A)\\), a transport plan can move only a fraction but not necessarily all of the mass at some place to other locations, and this solves the problem that a transport map cannot split a point mass. Moreover, any transport map can be represented as a transport plan: if \\(t\\) is a transport map with \\(t_{\\#}\\mu=\\nu\\), then \\(\\gamma\\equiv(id\\times t)_{\\#}\\mu\\) is a transport plan in \\(\\Gamma(\\mu,\\nu)\\), where \\(id\\) is the identity map.\nBased on the notion of transport plans, we can state Kantorovich’s problem, which is a problem more general than Monge problem. Instead of optimizing the cost over all transport maps, it minimizes the cost over all transport plans.\nDefinition 4. Kantorovich’s problem\nGiven \\(\\mu,\\nu\\in\\mathcal{P}(X)\\), the following optimization problem is the Kantorovich’s problem: : *{*\\gamma: \\gamma\\in\\Gamma(\\mu,\\nu)}\\int{X\\times X}|x{1}-x{2}|{p}d(x1,x^2),  p \nDenote the cost by \\(\\mathbb{K}_{p}(\\gamma)\\equiv \\int_{X\\times X}|x^1-x^2|d\\gamma(x^1,x^2)\\), which is a functional on \\(\\Gamma(\\mu,\\nu)\\). If \\(\\gamma\\) attains the infimum, we call it an optimal transport plan. Here, we use the cost function \\(c(x^1,x^2)=|x^1-x^2|^p\\), but it can also be replaced by other general cost functions.\nThe Kantorovich’s problem is a better-behaved problem for the following reasons. * The constraint sets is always nonempty. For \\(\\mu,\\nu\\in\\mathcal{P}(X)\\), the product measure \\(\\mu\\times \\nu\\) is always a transport plan in \\(\\Gamma(\\mu,\\nu)\\). * The constraint set is convex. Take \\(\\gamma_{0}\\), \\(\\gamma_{1}\\in\\Gamma(\\mu,\\nu)\\), the convex combination is \\(\\gamma_{\\alpha} \\equiv (1-\\alpha)\\gamma_{0}+ \\alpha \\gamma_{1} ,\\ \\alpha\\in [0,1]\\). Notice that it gives the right marginals: \\((\\pi^{1}_{\\#}\\gamma_{\\alpha})(A)=\\gamma_{\\alpha}(A\\times X)=(1-\\alpha)\\gamma_{0}(A\\times X)+\\alpha\\gamma_{1}(A\\times X)=(1-\\alpha)\\mu(A)+\\alpha\\mu(A)=\\mu(A)\\), and \\((\\pi^{2}_{\\#}\\gamma_{\\alpha})(B)=\\nu(B)\\) holds similarly.\n\nThe objective function is convex as well. Notice that  {p}() = (1-)_{p}(*0)+*(_1) , so objective function is actually linear.\nKantorovich’s problem has a dual problem.\nThe minimizer exists via the direct method of the calculus of variations."
  },
  {
    "objectID": "Monge.html#breniers-theorem-solvability-of-the-problem",
    "href": "Monge.html#breniers-theorem-solvability-of-the-problem",
    "title": "Monge Problem",
    "section": "Brenier’s Theorem, Solvability of the problem",
    "text": "Brenier’s Theorem, Solvability of the problem\nIn this section, we consider a special case where \\(X=\\mathbb{R}^d\\) and the cost function is quadratic: \\(c(x^1,x^2)=|x^1-x^2|^2\\). These assumptions allow us to obtain quite strong results for the solvability of Monge and Kantorovich’s problems, which is known as the following Brenier’s theorem.\nTheorem 1. Brenier’s theorem\nGiven \\(\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^d)\\), \\(\\mu\\ll\\mathcal{L}^d\\), where \\(\\mathcal{P}_{2}(\\mathbb{R}^d)\\equiv\\{\\mu\\in\\mathcal{P}(\\mathbb{R}^{d}):\\int |x|^2 d\\mu(x)&lt;\\infty\\}\\) and \\(\\mathcal{L}^d\\) is the Lebesgue measure on \\(\\mathbb{R}^d\\). Then there exists a unique optimal transport plan \\(\\gamma_{\\star}\\) of the form \\(\\gamma_{\\star} = (id \\times t_{\\star})_{\\#}\\mu\\) for the Kantorovich’s problem, where \\(t_{\\star}:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d\\) is a measurable function. Notice that \\(t_{\\star}\\) is actually a transport map from \\(\\mu\\) onto \\(\\nu\\), so \\(t_{\\star}\\) solves the corresponding Monge problem. In particular, the optimal cost of two problems coincide: :\\(\\inf_{t:t_{\\#}\\mu=\\nu}\\mathbb{M}_{2}(t)=\\inf_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\mathbb{K}_{2}(\\gamma)\\) where \\(\\mathbb{M}_{2}(t)\\equiv \\int |t(x)-x|^2 d\\mu(x)\\).\nThe proof of the result relies heavily on the duality property of the Kantorovich’s problem. By passing to the dual problem, we are able to give the expression of \\(t\\) as the gradient of the Kantorovich potential. The assumption that \\(\\mu\\) is absolute continuous w.r.t Lebesgue measure can be weakened to not giving mass on sets of \\(d-1\\) dimensional Hausdorff measure , and the result still holds for cost functions in the form of \\(c(x^1,x^2)=h(x^1-x^2)\\) for some convex function \\(h\\)."
  },
  {
    "objectID": "Monge.html#reference",
    "href": "Monge.html#reference",
    "title": "Monge Problem",
    "section": "Reference",
    "text": "Reference\n\nF. Santambrogio, Optimal Transport for Applied Mathematicians, p. 54-57\nN.Gigli, On the inverse implication of Brenier-Mccann theorems and the structure of \\((P_2(M),W_2)\\)"
  },
  {
    "objectID": "DiscreteOT.html",
    "href": "DiscreteOT.html",
    "title": "Discrete Optimal Transport",
    "section": "",
    "text": "For a given space \\(X\\), we call a measure \\(\\alpha\\) on \\(X\\) “finitely supported” if \\[ \\alpha = \\sum_{i=1}^{n} a_i \\delta_{x_i} , \\] where \\((a_1, \\ldots, a_n) \\in \\mathbb{R}_+^n\\), \\(\\sum \\limits_{i = 1}^n a_i = 1\\), and each \\(x_i \\in X\\). When the source measure \\(\\mu\\) and target measure \\(\\nu\\) are finitely supported, the Monge problem and the Kantorovich problem may be stated as linear programs (Villani 2003), and solved using classical methods (Peyré and Cuturi 2001). This insight may be used to approximate the Wasserstein distances between general measures, by first discretizing the source and target with a known level of accuracy (a difficult problem), then computing the cost between the discrete measures."
  },
  {
    "objectID": "DiscreteOT.html#existence",
    "href": "DiscreteOT.html#existence",
    "title": "Discrete Optimal Transport",
    "section": "Existence",
    "text": "Existence\nFirst, consider the case \\(n &lt; m\\). In this case, we have more elements \\(\\{ y_j \\}\\) than elements \\(\\{ x_i \\}\\). As a result, the range of any transport map \\(T\\) does not equal \\(\\{ y_1, \\ldots, y_m \\}\\). Let \\(y_k\\) be an element such that \\(y_k \\notin T(x_1, \\ldots, x_n)\\). Notice that \\[ b_k =  \\sum \\limits_{i : T(x_i) = y_k} a_i = 0 , \\] which contradicts our assumption that each \\(b_j\\) is positive. Thus, no transport map exists, so the Monge problem does not have a solution.\nEven in the case \\(n \\geq m\\), existence is not guaranteed. For example, assume \\(n = m = 2\\), \\(X = Y = \\{ 1, 2 \\}\\). Define \\(x_1 = y_1 = 1, x_2 = y_2 = 2\\). Define \\[ \\alpha = \\frac{1}{3} \\delta_{x_1} + \\frac{2}{3} \\delta_{x_2}, \\beta = \\frac{1}{2} \\delta_{y_1} + \\frac{1}{2} \\delta_{y_2} . \\] There are no transport maps from \\(\\alpha\\) to \\(\\beta\\), hence the Monge problem does not have a solution."
  },
  {
    "objectID": "DiscreteOT.html#uniqueness",
    "href": "DiscreteOT.html#uniqueness",
    "title": "Discrete Optimal Transport",
    "section": "Uniqueness",
    "text": "Uniqueness\nThe Monge problem may have multiple minimizers. For example, assume \\(n = m = 2\\), \\(X = Y = \\mathbb{R}^2\\). Define \\[ x_1 = (0, 0), x_2 = (1, 1), y_1 = (1, 0), y_2 = (0, 1) . \\] Notice that \\(x_1\\) and \\(x_2\\) are the opposite corners of the unit square, as are \\(y_1\\) and \\(y_2\\). Define \\[ \\alpha = \\frac{1}{2} \\delta_{x_1} + \\frac{1}{2} \\delta_{x_2}, \\beta = \\frac{1}{2} \\delta_{y_1} + \\frac{1}{2} \\delta_{y_2} . \\] The only two transport maps are \\(T\\) and \\(T'\\), where \\[ T(x_1) = y_1, T(x_2) = y_2 , \\] \\[ T'(x_1) = y_2, T'(x_2) = y_1 . \\]\nWe consider the Monge problem with \\(c(x, y) = |x - y|\\). Notice that \\[ \\sum \\limits_{i = 1}^2 c(x_i, T(x_i)) = 2 , \\] \\[ \\sum \\limits_{i = 1}^2 c(x_i, T'(x_i)) = 2 . \\] Thus both transport maps are optimal, i.e. the Monge problem does not have a unique solution. This example is taken from (Peyré and Cuturi 2001)."
  },
  {
    "objectID": "DiscreteOT.html#existence-1",
    "href": "DiscreteOT.html#existence-1",
    "title": "Discrete Optimal Transport",
    "section": "Existence",
    "text": "Existence\nWe know there always exists an admissible coupling; take \\(P_{ij} = a_i b_j\\)."
  },
  {
    "objectID": "DiscreteOT.html#uniqueness-1",
    "href": "DiscreteOT.html#uniqueness-1",
    "title": "Discrete Optimal Transport",
    "section": "Uniqueness",
    "text": "Uniqueness\nAgain, optimal solutions may not be unique. Consider the same counterexample used to show non-uniqueness of the Monge problem, i.e. the unit square. Notice that for any \\(i, j\\), \\(c(x_i, y_j) = 1\\). Thus, the Kantorovich problem reduces to\n\\[ \\min \\limits_{P \\in U(a, b)} \\sum \\limits_{i, j} P_{ij} . \\] Because \\(P \\in U(a, b)\\), we know that the entries of \\(P_{ij}\\) sum to one. Thus, for any admissible coupling \\(P\\), we have \\(\\sum \\limits_{i, j} c(x_i, y_j) P_{ij} = 1\\).\nConsider \\(P_1 = \\begin{bmatrix}\n\\frac{1}{2} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{bmatrix}\\) and \\(P_2 = \\begin{bmatrix}\n0 & \\frac{1}{2} \\\\\n\\frac{1}{2} & 0\n\\end{bmatrix}\\). Both \\(P_1\\) and \\(P_2\\) are admissible couplings, and both achieve the minimum for the Kantorovich problem."
  },
  {
    "objectID": "DiscreteOT.html#the-dual-problem",
    "href": "DiscreteOT.html#the-dual-problem",
    "title": "Discrete Optimal Transport",
    "section": "The Dual Problem",
    "text": "The Dual Problem\nAs the Kantorovich problem is a linear program, we may consider its dual. In the discrete case, the solution to the dual problem equals the solution to the primal problem, as discussed in the article Kantorovich dual problem. Our dual problem reduces to \\[ \\max \\limits_{(f, g) \\in R(c)} \\left &lt; f, a \\right &gt; + \\left &lt; g, b \\right &gt; , \\] where \\(R(c) := \\left \\{ (f, g) \\in \\mathbb{R}^n \\times \\mathbb{R}^m :  \\text{ for all } 1 \\leq i \\leq n, 1 \\leq j \\leq m, f_i + g_j \\leq c_{ij} \\right \\}.\\) Again, we know that this maximum is achieved; see the article above for the rationale."
  },
  {
    "objectID": "Riemannian_Structure_Wasserstein_Space.html",
    "href": "Riemannian_Structure_Wasserstein_Space.html",
    "title": "Riemannian Structure on 2-Wasserstein Space",
    "section": "",
    "text": "Much of this overview is found in Villani’s text (Villani [2009]). For any Polish space \\(X\\), let \\(P_p(X)\\) be the set of all Borel probability measures on \\(X\\) with finite \\(p\\)-th moment. Recall that \\(p\\in[1,\\infty]\\), the \\(p\\)-Wasserstein metric turns \\(P_p(X)\\) into a Polish space (see e.g. the Wikipedia entry on Wasserstein spaces or (Ambrosio, Gigli, and Savaré [2008]) Chapter 6). Using the general theory of gradient flows in metric spaces (which in general lack any differentiable structure), one can make precise the statement that “a solution to the heat equation is the 2-Wasserstein gradient flow with respect to the Boltzmann Entropy”. This vein of research began with the seminal work of Jordan, Kinderlehrer, and Otto in (Jordan, Kinderlehrer, and Otto 1998) during investigations of the Porous Media Equation, and was molded into a general framework by many authors including Ambrosio, Gigli, and Savaré that is well summarized in (Ambrosio, Gigli, and Savaré [2008]).\nIdentifying a given PDE as a Wasserstein gradient flow with respect to some functional is in general difficult and requires ad hoc methods. In (Otto 2001), Otto developed a formal Riemannian metric on the \\(2\\)-Wasserstein space over a compact Riemannian Manifold \\(M\\). In this framework we can construct analogs of gradients, divergence, parallel transport, and related tools from Riemannian geometry. While this framework is not rigorous enough to prove results about PDE’s, the formal structure does give us valuable heuristics for identifying with respect to which functional a PDE can be interpreted as a Wasserstein 2-Gradient flow."
  },
  {
    "objectID": "Riemannian_Structure_Wasserstein_Space.html#notion-of-the-tangent-space-on-2-wasserstein-space",
    "href": "Riemannian_Structure_Wasserstein_Space.html#notion-of-the-tangent-space-on-2-wasserstein-space",
    "title": "Riemannian Structure on 2-Wasserstein Space",
    "section": "Notion of the Tangent Space on 2-Wasserstein Space",
    "text": "Notion of the Tangent Space on 2-Wasserstein Space\nFrom the Benamou-Brenier Formula , we know that for two measures \\(\\overline{\\mu}_0,\\overline{\\mu}_1\\in P_2(M)\\), \\[\n    W_2(\\overline{\\mu}_0,\\overline{\\mu}_1)^2 = \\inf\\left\\{\\int_0^1\\left(\n        \\int_M|v_t|^2d\\mu_t\n    \\right)dt:\n        \\partial_t\\mu_t+\\operatorname{div}(v_t\\mu_t)=0,  v_t\\cdot\\nu|_{\\partial M}=0,  \\mu_0=\\overline{\\mu}_0, \\mu_1=\\overline{\\mu}_1\n    \\right\\}\n\\] In particular we can understand the \\(W_2\\) distance as being the infimum of the “length” of “curves” in \\(P_2(M)\\) much like the distance function on a Riemannian manifold is the infimum of lengths of paths between two points in the manifold. For our purposes, paths are solutions to the continuity equation. In particular, it seems that the natural inner product on the tangent space should induce the norm \\(\\int_M|v|^2d\\mu\\) where \\(v\\) is the velocity field corresponding to \\(\\mu\\) in the continuity equation.\nTo make the definitions cleaner, we will assume that our measures are absolutely continuous with respect to volume measure on \\(M\\). Thus we may formally define \\[\n    T_\\mu P_2(M)=cl(\\{\\nabla\\varphi: \\varphi \\text{ convex}\\})\n\\] with the closure taken in the \\(L^2(M,\\mu)\\) topology. Given any \\(h_1,h_2:M\\rightarrow \\mathbb{R}\\) such that \\(\\int_Mh_jd\\operatorname{Vol}(M)=0\\), we can define their 2-Wasserstein inner product given by \\[\n    \\langle h_1,h_2\\rangle_{W_2(M)} = \\int_M \\nabla \\psi_1\\cdot \\nabla\\psi_2 d\\operatorname{Vol}(M)\n\\] where \\(\\psi_j\\) solve \\[\n    \\begin{cases}\n        \\operatorname{div}(\\rho_j\\nabla\\psi_j)=-h_j & \\text{ in } M \\\\\n        \\frac{\\partial \\psi_j}{\\partial \\nu}=0 & \\text{ on } \\partial M\n    \\end{cases}\n\\] with \\(\\mu_j=\\rho_j d\\operatorname{Vol}(M)\\). In the next section we will see how this gives rise to the gradient operator on \\(P_2(M)\\). For more exposition on the definitions, we refer the reader to (Ambrosio, Brué, and Semola [2024]), (Ambrosio, Gigli, and Savaré [2008]), and (Figalli and Glaudo [2021])."
  },
  {
    "objectID": "Riemannian_Structure_Wasserstein_Space.html#the-gradient-operator-on-2-wasserstein-space",
    "href": "Riemannian_Structure_Wasserstein_Space.html#the-gradient-operator-on-2-wasserstein-space",
    "title": "Riemannian Structure on 2-Wasserstein Space",
    "section": "The gradient operator on 2-Wasserstein Space",
    "text": "The gradient operator on 2-Wasserstein Space\nSuppose that \\(\\mathcal{F}:P_2(M)\\rightarrow \\mathbb{R}\\) is a functional on the 2-Wasserstein space over \\(M\\). We define its gradient to be the unique function \\(\\operatorname{grad}_{W_2(M)} \\mathcal{F}[\\rho]\\) such that \\[\n    \\left\\langle\n       \\operatorname{grad}_{W_2(M)} \\mathcal{F}[\\rho_0], \\frac{\\partial \\rho}{\\partial\\varepsilon}\\Big|_{\\varepsilon=0}\n    \\right\\rangle_{\\rho_0} = \\frac{d}{d\\varepsilon}\\Big|_{\\varepsilon=0}\\mathcal{F}[\\rho_{\\varepsilon}]\n\\] for any sufficiently nice (e.g. absolutely continuous) curve \\(\\rho_t:(-\\varepsilon,\\varepsilon)\\rightarrow P_2(M)\\)."
  },
  {
    "objectID": "Riemannian_Structure_Wasserstein_Space.html#computations-of-2-wasserstein-gradients",
    "href": "Riemannian_Structure_Wasserstein_Space.html#computations-of-2-wasserstein-gradients",
    "title": "Riemannian Structure on 2-Wasserstein Space",
    "section": "Computations of 2-Wasserstein Gradients",
    "text": "Computations of 2-Wasserstein Gradients\nFor these examples, let’s specialize to the case where our manifold is \\(\\Omega\\subseteq \\mathbb{R}^d\\) with the usual Riemannian metric.\n\nIn the case where we choose the Boltzman entropy, \\(\\mathcal{F}[\\rho]=\\int_\\Omega\\rho\\log\\rho dx\\) where \\(\\rho\\) is the absolute value of some measure, then \\(\\operatorname{grad}_{W_2(M)} \\mathcal{F}[\\rho]=-\\Delta\\rho\\). Thus we can interpret a solution to the heat equation as 2-Wasserstein gradient flow with respect to the Boltzmann entropy.\nIf \\(\\mathcal{F}[\\rho]=\\int_\\Omega\\frac{\\rho^m}{m-1} dx\\) then \\(\\operatorname{grad}_{W_2(M)} \\mathcal{F}[\\rho]=-\\Delta(\\rho^m)\\). Thus a solution to the porous media equation (when \\(m&gt;1\\)) is the 2-Wasserstein gradient flow with respect to the functional \\(\\mathcal{F}\\)."
  },
  {
    "objectID": "Riemannian_Structure_Wasserstein_Space.html#further-reading",
    "href": "Riemannian_Structure_Wasserstein_Space.html#further-reading",
    "title": "Riemannian Structure on 2-Wasserstein Space",
    "section": "Further Reading",
    "text": "Further Reading\nSome of this framework has been formalized and extended, specifically where we consider the subset of \\(P_2(M)\\) of measures absolutely continuous with respect to the volume measure. In this setting Lott (Lott 2008) defined an analog of connection and Poisson structures on \\(P_2(M)\\)."
  },
  {
    "objectID": "GFMetricSpace.html",
    "href": "GFMetricSpace.html",
    "title": "Gradient Flows in Metric Space",
    "section": "",
    "text": "We first briefly reivew gradient flows in \\(\\mathbb{R}^n\\). The motivation comes from the so-called “gradient descent method”: Suppose we would like to find the minimizer of a certain (continuously differentiable) function \\(F:\\mathbb{R}^n\\to \\mathbb{R}\\). A classical way is to start with any point \\(x_0\\), and then go along the direction of negative of gradient of \\(F\\). That is, we are solving\n\\[\\begin{array}{l}\nx^{\\prime}(t)=-\\nabla F(x(t)) \\quad \\text { for } t&gt;0, \\\\\nx(0)=x_0.\n\\end{array}\\]\nA solution to the above initial value problem (IVP) is called a gradient flow of \\(F\\). As the gradient always points at the direction where \\(F\\) increases the most, negative gradient will lead us to a (local) minimizer of \\(F\\). The existence and uniqueness theory of ODE guarantees a satisfactory solution of this gradient flow.\n\n\nNote that along any smooth curve \\(x_t\\) it holds that \\(\\begin{aligned} F(x(s))-F(x(t))=\\int_s^t-\\nabla F(x(r)) \\cdot x^{\\prime}(r) \\mathrm{d} r & \\leq \\int_s^t|\\nabla F(x(r))|\\left|x^{\\prime}(r)\\right| \\mathrm{d} r \\\\ & \\leq \\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r\\end{aligned}\\)\nNote that if \\(x(t)\\) solves the gradient flow IVP, then\n\\[\nF(x(s))-F(x(t))=\\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r, \\quad  \\forall s&lt;t\n\\] which we call Energy Dissipation Enequality (EDE).\nNow if \\(F\\) is \\(\\lambda\\)-convex, the inequality that characterizes the gradient is\n\\[\nF(y) \\geq F(x)+\\frac{\\lambda}{2}|x-y|^2+p \\cdot(y-x) \\quad \\text { for all } y \\in \\mathbb{R}^d .\n\\] We can pick a curve \\(x(t)\\) and a point \\(y\\) and compute\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2=(y-x(t)) \\cdot\\left(-x^{\\prime}(t)\\right)\n\\]\nConsequently, imposing\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2 \\leq F(y)-F(x(t))-\\frac{\\lambda}{2}|x(t)-y|^2\n\\]\nfor all \\(y\\), will be equivalent to \\(-x^{\\prime}(t) \\in-\\partial F(x(t))\\). This will provide a second characterization (called EVI, Evolution Variational Inequality) of gradient flows in a metric environment."
  },
  {
    "objectID": "GFMetricSpace.html#gradient-flows-in-euclidean-space",
    "href": "GFMetricSpace.html#gradient-flows-in-euclidean-space",
    "title": "Gradient Flows in Metric Space",
    "section": "",
    "text": "We first briefly reivew gradient flows in \\(\\mathbb{R}^n\\). The motivation comes from the so-called “gradient descent method”: Suppose we would like to find the minimizer of a certain (continuously differentiable) function \\(F:\\mathbb{R}^n\\to \\mathbb{R}\\). A classical way is to start with any point \\(x_0\\), and then go along the direction of negative of gradient of \\(F\\). That is, we are solving\n\\[\\begin{array}{l}\nx^{\\prime}(t)=-\\nabla F(x(t)) \\quad \\text { for } t&gt;0, \\\\\nx(0)=x_0.\n\\end{array}\\]\nA solution to the above initial value problem (IVP) is called a gradient flow of \\(F\\). As the gradient always points at the direction where \\(F\\) increases the most, negative gradient will lead us to a (local) minimizer of \\(F\\). The existence and uniqueness theory of ODE guarantees a satisfactory solution of this gradient flow.\n\n\nNote that along any smooth curve \\(x_t\\) it holds that \\(\\begin{aligned} F(x(s))-F(x(t))=\\int_s^t-\\nabla F(x(r)) \\cdot x^{\\prime}(r) \\mathrm{d} r & \\leq \\int_s^t|\\nabla F(x(r))|\\left|x^{\\prime}(r)\\right| \\mathrm{d} r \\\\ & \\leq \\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r\\end{aligned}\\)\nNote that if \\(x(t)\\) solves the gradient flow IVP, then\n\\[\nF(x(s))-F(x(t))=\\int_s^t\\left(\\frac{1}{2}\\left|x^{\\prime}(r)\\right|^2+\\frac{1}{2}|\\nabla F(x(r))|^2\\right) \\mathrm{d} r, \\quad  \\forall s&lt;t\n\\] which we call Energy Dissipation Enequality (EDE).\nNow if \\(F\\) is \\(\\lambda\\)-convex, the inequality that characterizes the gradient is\n\\[\nF(y) \\geq F(x)+\\frac{\\lambda}{2}|x-y|^2+p \\cdot(y-x) \\quad \\text { for all } y \\in \\mathbb{R}^d .\n\\] We can pick a curve \\(x(t)\\) and a point \\(y\\) and compute\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2=(y-x(t)) \\cdot\\left(-x^{\\prime}(t)\\right)\n\\]\nConsequently, imposing\n\\[\n\\frac{d}{d t} \\frac{1}{2}|x(t)-y|^2 \\leq F(y)-F(x(t))-\\frac{\\lambda}{2}|x(t)-y|^2\n\\]\nfor all \\(y\\), will be equivalent to \\(-x^{\\prime}(t) \\in-\\partial F(x(t))\\). This will provide a second characterization (called EVI, Evolution Variational Inequality) of gradient flows in a metric environment."
  },
  {
    "objectID": "GFMetricSpace.html#gradient-flows-in-metric-space",
    "href": "GFMetricSpace.html#gradient-flows-in-metric-space",
    "title": "Gradient Flows in Metric Space",
    "section": "Gradient Flows in Metric Space",
    "text": "Gradient Flows in Metric Space\nTo generalize the above IVP, we need to make sense of derivative and gradient in metric space setting.\nThe first notion is the generalization of “derivative” in metric space, so-called metric derivative defined as follows: Given a curve \\(x:[0, T] \\rightarrow X\\) valued in a metric space, \\[\n\\left|x^{\\prime}\\right|(t):=\\lim _{h \\rightarrow 0} \\frac{d(x(t), x(t+h))}{|h|}.\n\\] Note that there is no vector structure here, so we can only make sense of modulus of the velocity of a curve.\nThe second concept is the generalization of convexity. A function \\(F\\) is called geodeiscally convex if it is convex along any geodesic: \\[F(x(t)) \\leq(1-t) F(x(0))+t F(x(1)).\\] Similarly we define geodeisc \\(\\lambda\\)-convex if \\[\nF(x(t)) \\leq(1-t) F(x(0))+t F(x(1))-\\lambda \\frac{t(1-t)}{2} d^2(x(0), x(1)) .\n\\]\nLastly we define the metric version of \\(|\\nabla F|\\) as slope: Let \\(F: X \\rightarrow \\mathbb{R} \\cup\\{+\\infty\\}\\) and \\(x \\in X\\) be such that \\(F(x)&lt;\\infty\\). Then the slope \\(|\\nabla F|(x)\\) of \\(F\\) at \\(x\\) is:\n\\[\n|\\nabla F|(x):=\\varlimsup_{y \\rightarrow x} \\frac{(F(x)-F(y))^{+}}{d(x, y)}=\\max \\left\\{\\varlimsup_{y \\rightarrow x} \\frac{F(x)-F(y)}{d(x, y)}, 0\\right\\} .\n\\]\nNow we are ready to define gradient flows in metric setting. There are two definitions(Ambrosio et al. 2013):\n(Energy Dissipation Equality definition of GF - EDE) Let \\(E: X \\rightarrow \\mathbb{R} \\cup\\{+\\infty\\}\\) and let \\(\\bar{x} \\in X\\) be such that \\(E(\\bar{x})&lt;\\infty\\). We say that \\([0, \\infty) \\ni t \\mapsto x_t \\in X\\) is a Gradient Flow in the EDE sense starting at \\(\\bar{x}\\) provided it is a locally absolutely continuous curve, \\(x_0=\\bar{x}\\) and\n\\[\nE\\left(x_s\\right)+\\frac{1}{2} \\int_t^s\\left|\\dot{x}_r\\right|^2 d r+\\frac{1}{2} \\int_t^s|\\nabla E|^2\\left(x_r\\right) d r=E\\left(x_t\\right), \\quad \\forall 0 \\leq t \\leq s\n\\]\nThe second definition is the EVI version:\n(Evolution Variation Inequality definition of GF - EVI) Let \\(E: X \\rightarrow \\mathbb{R} \\cup\\{+\\infty\\}\\), \\(\\bar{x} \\in \\overline{\\{E&lt;\\infty\\}}\\) and \\(\\lambda \\in \\mathbb{R}\\). We say that \\((0, \\infty) \\ni t \\mapsto x_t \\in X\\) is a Gradient Flow in the EVI sense (with respect to \\(\\lambda\\) ) starting at \\(\\bar{x}\\) provided it is a locally absolutely continuous curve in \\((0, \\infty), x_t \\rightarrow \\bar{x}\\) as \\(t \\rightarrow 0\\) and\n\\[\nE\\left(x_t\\right)+\\frac{1}{2} \\frac{d}{d t} d^2\\left(x_t, y\\right)+\\frac{\\lambda}{2} d^2\\left(x_t, y\\right) \\leq E(y), \\quad \\forall y \\in X, \\text { a.e. } t&gt;0\n\\]\nIt turns out that the EDE definition is weaker than the EVI definition, i.e. any EVI GF is automatically an EDE GF. The EDE definition is easier to get existence, and the EVI definition is used to get uniqueness results."
  },
  {
    "objectID": "W2Hm1.html",
    "href": "W2Hm1.html",
    "title": "Asymptotic equivalence of W2 and H^-1",
    "section": "",
    "text": "The quadratic Wasserstein distance and \\(\\dot{H}^{-1}\\) distance become asymptotically equivalent when the when the measures are absolutely continuous with respect to Lebesgue measure with density close to the value \\(\\varrho = 1\\). This is particularly of interest since the space \\(H^{-1}\\) is a Hilbert space as opposed to \\(W_2\\) being only a metric space. This allows one to extend several well-known results about continuity of various operators in \\(H^{-1}\\) to \\(W_2\\) by asymptotic equivalence. This equivalence is also important numerically, where computing \\(H^{-1}\\) is much easier than computing \\(W_2\\).\nFurthermore, this asymptotic equivalence is relevant for evolution problems with the constraint \\(\\varrho \\leq 1\\), such as crowd motion."
  },
  {
    "objectID": "W2Hm1.html#motivation",
    "href": "W2Hm1.html#motivation",
    "title": "Asymptotic equivalence of W2 and H^-1",
    "section": "",
    "text": "The quadratic Wasserstein distance and \\(\\dot{H}^{-1}\\) distance become asymptotically equivalent when the when the measures are absolutely continuous with respect to Lebesgue measure with density close to the value \\(\\varrho = 1\\). This is particularly of interest since the space \\(H^{-1}\\) is a Hilbert space as opposed to \\(W_2\\) being only a metric space. This allows one to extend several well-known results about continuity of various operators in \\(H^{-1}\\) to \\(W_2\\) by asymptotic equivalence. This equivalence is also important numerically, where computing \\(H^{-1}\\) is much easier than computing \\(W_2\\).\nFurthermore, this asymptotic equivalence is relevant for evolution problems with the constraint \\(\\varrho \\leq 1\\), such as crowd motion."
  },
  {
    "objectID": "W2Hm1.html#formalization",
    "href": "W2Hm1.html#formalization",
    "title": "Asymptotic equivalence of W2 and H^-1",
    "section": "Formalization",
    "text": "Formalization\n\nDefinition of  ^{-1} \nThe negative Sobolev norm \\(\\| \\cdot \\|_\\dot{H}^{-1}\\) is defined   to be\n\\(\\| \\mu - \\nu \\|_{\\dot{H}^{-1} (\\Omega)} := \\sup \\left \\{ \\int_{\\Omega} \\phi \\, \\mathrm{d}( \\mu - \\nu ) : \\phi \\in C^{\\infty}_c(\\Omega), \\,  \\| \\nabla \\phi \\|_{L^2(\\Omega)} \\leq 1  \\right \\} .\\)\n\n\nLemma\nLet \\(\\mu, \\nu\\) be measures that are absolutely continuous with respect to Lebesgue measure on a convex domain \\(\\Omega\\), with densities bounded above by the same constant \\(C &gt; 0\\). Then, for all functions \\(\\phi \\in H^1(\\Omega)\\):\n\\(\\int_\\Omega \\phi \\, \\mathrm{d}( \\mu - \\nu ) \\leq \\sqrt{C} \\| \\nabla \\phi \\|_{L^2(\\Omega)} W_2(\\mu, \\nu)\\)\nProof of the lemma can be found Chapter 5, page 210 of .\n\n\n ^{-1}  as a Dual\nThis material is adapted from .\nAn important property of \\(\\dot{H}^{-1}\\) is its characterization as a dual, which justifies the notation. Let \\(\\Omega \\subseteq \\mathbb{R}^d\\) be an open and connected subset. For \\(\\phi \\in C^1(\\Omega)\\),\n\\(\\| \\phi \\|_{\\dot{H}^1} := \\| \\nabla \\phi \\|_{L^2(\\Omega)} := \\left[ \\int_{\\Omega} | \\nabla \\phi(x) |^2 \\, \\mathrm{d}x \\right]^{\\frac{1}{2}}\\)\ndefines a semi-norm. Then for an absolutely continuous signed measure on \\(\\Omega\\) with zero total mass,\n\\(\\| \\nu \\|_{ \\dot{H}^{-1} } := \\sup \\left \\{ | \\langle \\phi , \\nu \\rangle | : \\phi \\in C^1(\\Omega) , \\, \\| \\phi \\|_{\\dot{H}^1} \\leq 1 \\right \\} = \\sup \\left \\{  \\left| \\int_{\\Omega} \\phi(x) \\, \\mathrm{d}\\nu(x) \\right| : \\phi \\in C^1(\\Omega) , \\, \\| \\phi \\|_{\\dot{H}^1} \\leq 1 \\right \\} .\\)\nThe space \\(\\dot{H}^{-1}\\) is the dual space of zero-mean \\(H^1(\\Omega)\\) functions endowed with the norm \\(L^2\\) norm on the gradient.\n\n\nTheorem\nLet \\(\\mu, \\nu\\) be absolutely continuous measures on a convex domain \\(\\Omega\\), with densities bounded from below and from above by the same constants \\(a, b\\) with \\(0 &lt; a &lt; b &lt; +\\infty\\). Then\n\\(b^{-\\frac{1}{2}} || \\mu - \\nu ||_{\\dot{H}^{-1} (\\Omega)} \\leq W_2( \\mu, \\nu) \\leq a^{-\\frac{1}{2}}|| \\mu - \\nu ||_{\\dot{H}^{-1}(\\Omega)}\\)\nThe proof of the theorem uses the above lemma and can be found Chapter 5, page 211 of ."
  },
  {
    "objectID": "W2Hm1.html#localization",
    "href": "W2Hm1.html#localization",
    "title": "Asymptotic equivalence of W2 and H^-1",
    "section": "Localization",
    "text": "Localization\nThe following material is adapted from .\nThis section deals with the problem of localization of the quadratic Wasserstein distance: if \\(\\mu , \\nu\\) are (signed) measures on \\(\\mathbb{R}^d\\) that are close in the sense of \\(W_2\\), do they remain close to each other when restricted to subsets of \\(\\mathbb{R}^d\\)?\n\nNotation\nHere we are working in Euclidean space \\(\\mathbb{R}^d\\) with the Lebesgue measure \\(\\lambda\\). * Recall that for a subset \\(A \\subseteq\\mathbb{R}^d\\),\n:\\(\\mathrm{dist}(x,A) := \\inf \\{ |x - y| : y \\in A \\}\\)\ndenotes the distance between a point \\(x\\) and the subset \\(A\\). * For a (signed) measure \\(\\mu\\) on \\(\\mathbb{R}^d\\) and \\(\\varphi : \\mathbb{R}^d \\to \\mathbb{R}\\) a nonnegative and measurable function, \\(\\varphi \\cdot \\mu\\) denotes the measure such that \\(\\mathrm{d}(\\varphi \\cdot \\mu) = \\varphi(x) \\, \\mathrm{d}\\mu(x)\\). * The norm\n:\\(\\| \\mu \\|_1 := \\int_{\\mathbb{R}^d} \\, |\\mathrm{d}\\mu(x)|\\)\ndenotes the total variation norm of the signed measure \\(\\mu\\). If \\(\\mu\\) is in fact a measure, then \\(\\| \\mu \\|_1 = \\mu ( \\mathbb{R}^d )\\).\n\nNow we can ask the original question more precisely. If \\(\\varphi : \\mathbb{R}^d \\to \\mathbb{R}\\) is non-negative and compactly supported satisfying further technical assumptions to be specified later, we wish to bound \\(W_2 ( a \\varphi \\cdot \\mu , \\varphi \\cdot \\nu)\\) by \\(W_2(\\mu,\\nu)\\), where \\(a\\) is a constant factor ensuring that \\(a\\varphi \\cdot \\mu\\) and \\(\\varphi \\cdot \\nu\\) have the same mass. The factor of \\(a\\) is necessary, otherwise the \\(W_2\\) distance between \\(\\varphi \\cdot \\mu\\) and \\(\\varphi \\cdot \\nu\\) is in general not well-defined.\n\n\nTheorem\nLet \\(\\mu , \\nu\\) be measures on \\(\\mathbb{R}^d\\) having the same total mass, and let \\(B\\) be a ball in \\(\\mathbb{R}^d\\). Assume that on \\(B\\), the density of \\(\\mu\\) with respect to the Lebesgue measure is bounded above and below, that is\n:\\(\\exists 0 &lt; m_1 \\leq m_2 &lt; \\infty \\quad \\forall x \\in B \\quad m_1 \\mathrm{d}\\lambda(x) \\leq \\mathrm{d}\\mu(x) \\leq m_2 \\mathrm{d}\\lambda(x).\\)\nLet \\(\\varphi : \\mathbb{R}^d \\to (0,+\\infty)\\) be a \\(k\\)-Lipschitz function for some \\(0 \\leq k &lt; \\infty\\) supported in \\(B\\), and suppose that \\(\\varphi\\) is bounded above and below by the map\n:\\(x \\mapsto \\mathrm{dist}(x,B^c)^2\\)\non \\(B\\), that is, there exists constants \\(0 &lt; c_1 \\leq c_2 &lt; \\infty\\) such that for all \\(x \\in B\\),\n:\\(c_1 \\mathrm{dist}(x,B^c)^2 \\leq \\varphi(x) \\leq c_2 \\mathrm{dist}(x,B^c)^2 .\\)\nThen, denoting\n:\\(a := \\| \\varphi \\cdot \\nu \\|_1 / \\| \\varphi \\cdot \\mu \\|_1 = \\frac{ \\int_{\\mathbb{R}^d} |\\varphi(x) \\, \\mathrm{d}\\mu(x)| }{ \\int_{\\mathbb{R}^d} |\\varphi(x) \\, \\mathrm{d}\\nu(x)| } ,\\)\nwe have\n\\(W_2 (a\\varphi \\cdot \\mu , \\varphi \\cdot \\nu) \\leq C(n)^{\\frac{1}{2}} \\left( \\frac{ c_2 m_2 }{ c_1 m_1 } \\right)^{\\frac{3}{2}} k c_1^{-\\frac{1}{2}} W_2(\\mu,\\nu) ,\\)\nfor \\(C(n) &lt; \\infty\\) some absolute constant depending only on \\(n\\). Moreover, taking \\(C(n) := 2^{11} n\\) fits. Furthermore, that \\(\\varphi\\) is supported in a ball is not necessary, as it can be supported in a cube or a simplex.\nThe proof can be found in ."
  },
  {
    "objectID": "W2Hm1.html#connection-with-the-vlasov-poisson-equation",
    "href": "W2Hm1.html#connection-with-the-vlasov-poisson-equation",
    "title": "Asymptotic equivalence of W2 and H^-1",
    "section": "Connection with the Vlasov-Poisson Equation",
    "text": "Connection with the Vlasov-Poisson Equation\nLoeper  contributed an earlier result on a bound between \\(W_2\\) and \\(\\dot{H}^{-1}\\) for bounded densities in studying the existence of solutions to the Vlasov-Poisson equation. Namely, Loeper proved that that if \\(\\rho_1 , \\rho_2\\) be probability measures on \\(\\mathbb{R}^d\\) with \\(L^{\\infty}\\) densities with respect to the Lebesgue measure. Let \\(\\Psi_i\\), \\(i = 1, 2\\) solve\n:\\(-\\Delta \\Psi_i = \\rho_i \\qquad \\text{in } \\mathbb{R}^d ,\\)\n:\\(\\Psi_i(x) \\to 0 \\qquad \\text{as } |x| \\to \\infty ,\\)\nin the integral sense, that is,\n:\\(\\Psi_i(x) = \\frac{1}{4\\pi} \\int_{\\mathbb{R}^d} \\frac{\\rho_i(y)}{|x - y|} \\, \\mathrm{d}y .\\)\nThen\n\\(\\| \\nabla \\Psi_1 - \\nabla \\Psi_2 \\|_{L^2(\\mathbb{R}^d)} \\leq \\left[ \\max \\left \\{ \\| \\rho_1 \\|_{L^{\\infty}} , \\| \\rho_2 \\|_{L^{\\infty}} \\right \\} \\right]^{\\frac{1}{2}} W_2(\\rho_1,\\rho_2) .\\)\nLoeper also extended the result to finite measures with the same total mass."
  },
  {
    "objectID": "W2Hm1.html#references",
    "href": "W2Hm1.html#references",
    "title": "Asymptotic equivalence of W2 and H^-1",
    "section": "References",
    "text": "References\n\n [https://www.sciencedirect.com/science/article/pii/S0021782406000067] Loeper, Grégoire. Uniqueness of the solution to the Vlasov–Poisson system with bounded density. Journal de Mathématiques Pures et Appliquées, Volume 86, Issue 1, 2006, Pages 68-79, ISSN 0021-7824. \n [https://arxiv.org/abs/1104.4631v2] Peyre, Rémi. Comparison between \\(W_2\\) distance and \\(\\dot{H}^{-1}\\) norm, and localisation of Wasserstein distance. \n F. Santambrogio, Optimal Transport for Applied Mathematicians, Chapter 5, pages 209-211"
  },
  {
    "objectID": "KDualProblem.html",
    "href": "KDualProblem.html",
    "title": "Kantorovich Dual problem",
    "section": "",
    "text": "The Kantorovich Dual Problem is one of the minimization problems in Optimal Transport . It is a dual problem of the Kantorovich Problem.\n\nThe Shipper’s Problem\nOne of the ways to understand this problem is stated by Caffarelli. The statement is presented in the book by Villani (Villani 2021). We will provide the modern rephrase of his statement.\nEvery morning, people enjoy a coffee time at home. All in all, it costs Amazon \\(c(x,y)\\) dollars to ship one box of necessary espresso capsules from place \\(x\\) to place \\(y\\), i.e. from warehouses to homes. We want to optimize this expensive habit and consequently to solve appropriate Monge-Kantorovich problem. The mathematicians come to Amazon and propose the new kind of payment. For every box at place \\(x\\) they will charge \\(\\varphi(x)\\) dollars and \\(\\psi(y)\\) dollars to deliver at place \\(y\\). However, mathematicians will not reveal their shipping routes. Of course, in order for Amazon to accept this offer, the price \\(\\varphi(x)+\\psi(y) \\leq c(x,y)\\). The moral is that if the mathematicians are smart enough, they will be capable to make this shipment cheaper. This is provided by Kantorovich duality theorem. Take care that in the same cases, mathematicians will also give negative prices, if it is necessary!\n\n\nStatement of Theorem\nThis is the statement of the Theorem in the book “Topics in Optimal Transportation”, by Cedric Villani (Villani 2021).\nLet \\(X\\) and \\(Y\\) be Polish spaces, let \\(\\mu \\in \\mathcal{P}(X)\\) and \\(\\nu \\in \\mathcal{P}(Y)\\), and let a cost function \\(c:X \\times Y \\rightarrow[0,+\\infty]\\) be lower semi-continuous. Whenever \\(\\pi \\in \\mathcal{P}(X \\times Y)\\) and \\((\\varphi, \\psi) \\in L^{1}(d\\mu) \\times L^{1}(d\\nu)\\), define \\[\nI[\\pi]= \\int_{X\\times Y} c(x,y) d\\pi(x,y), \\quad J(\\varphi,\\psi)=\\int_{X}\\varphi(x)d\\mu(x)+\\int_{Y}\\psi(y) d\\nu(y).\n\\]\nDefine \\(\\Pi(\\mu,\\nu)\\) to be the set of Borel probability measures \\(\\pi\\) on \\(X\\times Y\\) such that for all measurable sets \\(A \\subset X\\) and \\(B \\subset Y\\) , \\(\\pi[A\\times Y]=\\mu(A)\\) , \\(\\pi[X\\times B]=\\nu(B)\\) , and define \\(\\Phi_{c}\\) to be the set of all measurable functions \\((\\varphi, \\psi) \\in L^{1}(d\\mu) \\times L^{1}(d\\nu)\\) satisfying \\(\\varphi(x)+\\psi(y) \\leq c(x,y)\\) for \\(d\\mu\\) almost everywhere in \\(X\\) and \\(d\\nu\\) almost everywhere in \\(Y\\).\nThen \\(\\inf_{\\Pi(\\mu,\\nu)} I[\\pi] = \\sup_{\\Phi_{c}} J(\\varphi,\\psi)\\).\nMoreover, the infimum \\(\\inf_{\\Pi(\\mu,\\nu)} I[\\pi]\\) is attained. In addition it is possible to restrict \\(\\varphi\\) and \\(\\psi\\) to be continuous and bounded.\n\n\nIdeas and the techniques used in the proof\nFirst, we assume that our spaces \\(X\\) and \\(Y\\) are compact and that the cost function \\(c(x,y)\\) is continuous. The general case follows by an approximation argument.\nThe main idea is to use minimax principle, i.e. interchanging inf sup with sup inf in the proof. For this, we need some basic convex analysis techniques, namely Legendre-Fenchel transform and Theorem on Fenchel-Rockafellar Duality (its proof is based on Hahn-Banach theorem consequence on separating convex sets). The required statements can be found in the book by Rockafellar(Rockafellar 1997) and the book by Bauschke and Combettes(Bauschke et al. 2017).\nTake a note that at some point we use Arzela-Ascoli Theorem. In a non-compact space this is not possible. In order to evade compactness property, we have to use Prokhorov’s theorem:\nLet \\(\\mu_{n}\\) be a tight sequence of probability measures on Polish space \\(X\\). Then, there exists \\(\\mu \\in \\mathcal{P}(X)\\) and convergent subsequence \\(\\mu_{n_{k}}\\) such that \\(\\mu_{n_{k}} \\rightharpoonup \\mu\\) in the dual of \\(C_{b}(X)\\). Conversely, every sequence \\(\\mu_{n} \\rightharpoonup \\mu\\) is tight.\nThe proof of the previous Theorem can be found in (Bogachev and Ruas 2007). For more information on \\(C_{b}(X)\\) duality, take a look at Dual space of C_0(x) vs C_b(x).\n\n\nC-concave functions\nThere are a few alternative proofs of the theorem above. First, we will discuss the conclusion of the Theorem.\nIn Kantorovich Duality Theorem, the left-hand side of the last equality, the infimum \\(\\inf_{\\Pi(\\mu,\\nu)} I[\\pi]\\) is attained. For continuous and bounded cost function \\(c(x,y)\\) we can restrict \\(\\sup_{\\Phi_{c}} J(\\varphi,\\psi)\\) to pairs \\((\\varphi^{cc},\\varphi^{c})\\) where \\(\\varphi\\) is bounded and\n\\[ \\varphi^{c}(y)=\\inf_{y \\in Y} [c(x,y) - \\varphi(x)], \\quad \\varphi^{cc}(x)=\\inf_{y \\in Y} [c(x,y) - \\varphi^{c}(y)].\n\\]\nThe pair \\((\\varphi^{cc},\\varphi^{c})\\) is called a pair of conjugate c-concave functions. This way we improve the maximization functions for the dual problem (for motivation, think about shipper’s problem). It is known that under the reasonable assumptions \\(\\varphi^{cc}=\\varphi\\) and that \\(\\varphi^{c}\\) is measurable, see (Santambrogio 2015).\nHence, it is possible to state the Kantorovich Duality theorem using c-concave functions: \\[\n\\text{max}(DP) = \\text{max}_{\\varphi \\in c-conc(X)} \\int_{X} \\varphi(x) d\\mu + \\int_{Y} \\varphi^{c}(y)d\\nu\n\\]\n\n\n\n\n\nReferences\n\nBauschke, Heinz H, Patrick L Combettes, Heinz H Bauschke, and Patrick L Combettes. 2017. Correction to: Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer.\n\n\nBogachev, Vladimir Igorevich, and Maria Aparecida Soares Ruas. 2007. Measure Theory. Vol. 1. 1. Springer.\n\n\nRockafellar, R Tyrrell. 1997. Convex Analysis. Vol. 28. Princeton university press.\n\n\nSantambrogio, Filippo. 2015. Optimal Transport for Applied Mathematicians. Vol. 87. Springer.\n\n\nVillani, Cédric. 2021. Topics in Optimal Transportation. Vol. 58. American Mathematical Soc."
  },
  {
    "objectID": "Bures-Wasserstein Gradient Flows.html",
    "href": "Bures-Wasserstein Gradient Flows.html",
    "title": "Bures-Wasserstein Gradient Flow",
    "section": "",
    "text": "The Bures-Wasserstein gradient flow is a Wasserstein gradient flow in the space of Gaussian measures. Descriptions of Bures-Wasserstein gradient flows take advantage of the simplicity of transport maps between Gaussian distributions in order to provide an alternate, computationally tractable viewpoint on certain types of Wasserstein gradient flows (Chen, Georgiou, and Tannenbaum 2019; Delon and Desolneux 2020)."
  },
  {
    "objectID": "Bures-Wasserstein Gradient Flows.html#variational-characterization",
    "href": "Bures-Wasserstein Gradient Flows.html#variational-characterization",
    "title": "Bures-Wasserstein Gradient Flow",
    "section": "Variational Characterization",
    "text": "Variational Characterization\nLet \\(\\mathcal{F} : \\mathcal{P}_{2,\\text{ac}}(\\mathbb{R}^d)\\rightarrow \\mathbb{R}\\) be a functional with first variation \\(\\delta \\mathcal{F}(\\mu)\\) at \\(\\mu = \\mathcal{N}(m,\\Sigma) \\in \\text{BW}(\\mathbb{R}^d)\\). Then the Bures-Wasserstein gradient of \\(\\mathcal{F}\\) at \\(\\mu\\) is the affine mapping \\[x \\mapsto \\bigg( \\int\\nabla^2  \\delta \\mathcal{F}(\\mu) d\\mu\\bigg)(x-m) + \\int\\nabla  \\delta \\mathcal{F}(\\mu) d\\mu\\] where \\(m\\) is the mean of \\(\\mu\\).\n\nCharacterization in terms of Mean and Variance\nThe Bures-Wasserstein gradient flow of the functional \\(\\mathcal{F}\\) is the curve \\((\\mu_t=\\mathcal{N}(m_t,\\Sigma_t))_{t \\geq 0}\\), where \\[\\begin{align*}\n        \\dot{m}_t &= - \\mathbb E\\left[ \\nabla  \\delta \\mathcal{F}(\\mu_t) (X_t) \\right]\\\\\n        \\dot{\\Sigma}_t &= -\\mathbb E\\left[\\nabla^2  \\delta \\mathcal{F}(\\mu_t) (X_t)\\right]\\Sigma_t - \\Sigma_t \\mathbb E\\left[\\nabla^2  \\delta \\mathcal{F}(\\mu_t) (X_t)\\right]\n\\end{align*}\\] where \\(X_t \\sim \\mu_t\\).\nThe theorem stated above provides a characterization of the Bures-Wasserstein gradient flow based entirely on the mean and covariance of each measure \\(\\mu_t\\) in the gradient flow. This can also be viewed as a Lagrangian or particle-based interpretation of the gradient flow."
  },
  {
    "objectID": "Bures-Wasserstein Gradient Flows.html#wasserstein-gradients-flows-over-gaussian-mixture-models",
    "href": "Bures-Wasserstein Gradient Flows.html#wasserstein-gradients-flows-over-gaussian-mixture-models",
    "title": "Bures-Wasserstein Gradient Flow",
    "section": "Wasserstein Gradients flows over Gaussian Mixture Models",
    "text": "Wasserstein Gradients flows over Gaussian Mixture Models\nBures-Wasserstein gradients can be used to provide an alternative interpretation of gradient flows in \\(\\mathcal{P}_{2,\\text{ac}}(\\mathbb{R}^d)\\) by treating probability measures as mixtures of (possibly degenerate) Gaussians. Traditionally, a Gaussian mixture is a measure whose density is a convex combination of Gaussian densities. This is generalized by defining a Gaussian mixture in terms of what is known as a mixing measure \\(\\nu \\in \\mathcal{P}_2(\\text{BW}(\\mathbb{R}^d)) \\cong \\mathcal{P}_2(\\mathbb{R}^d \\times S^d_{++})\\), so that the corresponding Gaussian mixture \\(G_{\\nu}\\) is defined as \\(G_{\\nu} = \\int \\mathcal{N}(m,\\Sigma) \\nu(dm, d\\Sigma)\\). One may treat \\(\\nu\\) as the distribution of mean and covariance variables \\(m, \\Sigma\\), i.e \\(\\nu = \\text{law}(m,\\Sigma)\\) for some random variables \\(m\\) and \\(\\Sigma\\).\nA Wasserstein metric can be defined on \\(\\mathcal{P}_2(\\text{BW}(\\mathbb{R}^d))\\) in the same fashion as \\(\\mathcal{P}_2(\\mathbb{R}^d)\\) is defined, with the underlying metric space changed but the fundamental concepts unaltered. The mapping \\(\\nu \\mapsto G_{\\nu}\\) provides a surjective mapping from \\(\\mathcal{P}_2(\\text{BW}(\\mathbb{R}^d))\\) to \\(\\mathcal{P}_2(\\mathbb{R}^d)\\), which allows us to reinterpret gradient flows of a functional \\(\\mathcal{F} : \\mathcal{P}_2(\\mathbb{R}^d)\\rightarrow \\mathbb{R}\\) in terms of the functional \\(\\mathcal{G}: \\nu \\mapsto \\mathcal{F}(G_{\\nu})\\) via the proposition below.\n\nGradient Flow Expression in \\(\\mathcal{P}_2(\\text{BW}(\\mathbb{R}^d))\\)\nLet \\(\\nu \\in \\mathcal{P}_2(\\text{BW}(\\mathbb{R}^d))\\) be given and let \\(G_{\\nu} = \\int \\mathcal{N}(m,\\Sigma) \\nu(dm, d\\Sigma)\\) denote the Gaussian mixture corresponding to \\(\\nu\\). Let \\(\\mathcal{F}\\) be a functional over \\(\\mathcal{P}_2(\\mathbb{R}^d)\\) and let \\(\\mathcal{G}\\) be the functional over \\(\\mathcal{P}_2(\\text{BW}(\\mathbb{R}^d))\\) defined by \\(\\mathcal{G}(\\nu) = \\mathcal{F}(G_{\\nu})\\). The Wasserstein gradient flow of \\(\\mathcal{G}\\) in \\(\\mathcal{P}_2(\\text{BW}(\\mathbb{R}^d))\\) is given by the curve \\((\\text{law}(m_t,\\Sigma_t))_{t \\geq 0}\\) for random variables \\(m_t\\) and \\(\\Sigma_t\\) that follow the equations \\[\\begin{align*}\n        \\dot{m}_t &= - \\mathbb E\\left[\\nabla  \\delta \\mathcal{F}(G_t) (X_t)\\right]\\\\\n        \\dot{\\Sigma}_t &= -\\mathbb E\\left[\\nabla^2  \\delta \\mathcal{F}(G_t) (X_t)\\right]\\Sigma_t - \\Sigma_t \\mathbb E\\left[\\nabla^2  \\delta \\mathcal{F}(G_t)(X_t) \\right]\n\\end{align*}\\] where \\(X_t \\sim \\mathcal{N}(m_t,\\Sigma_t)\\)."
  },
  {
    "objectID": "Bures-Wasserstein Gradient Flows.html#gaussian-variational-inference",
    "href": "Bures-Wasserstein Gradient Flows.html#gaussian-variational-inference",
    "title": "Bures-Wasserstein Gradient Flow",
    "section": "Gaussian Variational Inference",
    "text": "Gaussian Variational Inference\nBures-Wasserstein gradient flows can be used to solve variational inference problems when the set of admissible measures is \\(\\text{BW}(\\mathbb{R}^d)\\) (Lambert et al. 2023).\nThe variational inference is an optimization problem which seeks to find a simple distribution in an admissible set \\(\\Omega\\) which closely approximates a given measure \\(\\pi \\in \\mathcal{P}_2(\\mathbb{R}^d)\\) in terms of KL divergence. More precisely, variational inference is the problem of finding a measure \\(q_*\\) such that \\[q_* = \\underset{q \\in \\Omega}{\\arg\\min} \\text{KL}(q || \\pi).\\] Gaussian variational inference is the problem \\[q_* = \\underset{q \\in \\text{BW}(\\mathbb{R}^d)}{\\arg\\min} \\text{KL}(q || \\pi).\\]\nRecent work (Lambert et al. 2023) takes a gradient flow approach to solving Gaussian variational inference. By computing the Bures-Wasserstein gradient flow of the functional \\(\\mathcal{F} = \\text{KL}(\\,\\cdot\\,|| \\pi)\\), one can discretize the gradient flow and obtain a numerical algorithm to approximate a local minimizer of \\(\\mathcal{F}\\).\nIn order to do so, one must derive the Bures-Wasserstein gradient of \\(\\mathcal{F}\\). This is done via the variational characterization of Bures-Wasserstein gradients. For an absolutely continuous measure \\(\\pi\\) with a density \\(\\exp(-V)\\) (\\(V\\) is called the potential), the first variation of \\(\\mathcal{F} = \\text{KL}(\\,\\cdot\\,|| \\pi)\\) is \\[\\nabla\\delta\\mathcal{F}(q) = \\nabla V + \\nabla \\log q\\] and \\[\\nabla^2\\delta\\mathcal{F}(q) = \\nabla^2 V + \\nabla^2 \\log q\\] where \\(\\log q(x)\\) denotes the logarithm of the density of \\(q\\) at \\(x\\).\nBy the formula for Bures-Wasserstein gradients, we see that \\[\\begin{align*}      \\nabla_{\\text{BW}}\\mathcal{F}(q)(x) &= \\bigg( \\int \\big( \\nabla^2 V + \\nabla^2 \\log q \\big) dq \\bigg) (x-m_q) + \\int \\big( \\nabla V + \\nabla \\log q \\big) dq\\\\\n    &= \\bigg( \\int \\big( \\nabla^2 V + \\nabla^2 \\log q \\big) dq \\bigg) (x-m_q) + \\int \\nabla V dq + \\int \\nabla \\log q dq.\n\\end{align*}\\] Because \\(q = \\mathcal{N}(m,\\Sigma)\\) is Gaussian we know \\(\\log q (x) = - \\frac{1}{2}(x-m)\\Sigma^{-1}(x-m) + c\\) where \\(c\\) is some constant. This implies \\(\\log q\\) symmetric about the mean \\(m\\) of \\(q\\), \\(\\nabla \\log q\\) is antisymmetric about \\(m\\), and hence the integral \\(\\int_{\\mathbb{R}} \\nabla \\log q dq = 0\\). Thus, \\[\\begin{align*}                 \n\\nabla_{\\text{BW}}\\mathcal{F}(q)(x) &= \\bigg( \\int \\big( \\nabla^2 V + \\nabla^2 \\log q \\big) dq \\bigg) (x-m_q) + \\int \\nabla V dq.\n\\end{align*}\\] Further, since \\(\\log q (x) = (x-m)\\Sigma^{-1}(x-m) + c\\) we know that \\(\\nabla^2 \\log q (x) = -\\Sigma^{-1}\\), so we conclude that \\[\\begin{align*}                 \n\\nabla_{\\text{BW}}\\mathcal{F}(q)(x) &= \\bigg( \\int \\nabla^2 V dq - \\Sigma^{-1} \\bigg) (x-m_q) + \\int \\nabla V dq.\n\\end{align*}\\]\nIn addition, since \\(\\text{KL}(\\cdot|| \\pi)\\) is convex on \\(\\mathcal{P}_2(\\mathbb{R}^d)\\), its restriction to \\(\\text{BW}(\\mathbb{R}^d)\\) is also a convex functional because \\(\\text{BW}(\\mathbb{R}^d)\\) is geodesically convex. This implies that \\(\\text{KL}(q_t|| \\pi)\\) will converge to \\(\\text{KL}(q_*|| \\pi)\\) exponentially for any gradient flow \\((q_t)_{t\\geq 0}\\) of \\(\\text{KL}(\\cdot|| \\pi)\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimal Transport Wiki",
    "section": "",
    "text": "Welcome to the Optimal Transport Wiki!\nHere is a list of new article ideas. Here is a list of suggestions of articles to revise\nContact Katy Craig if you would like to contribute to this wiki.\n\nThe optimal transportation problem\n\nMonge Problem\nKantorovich Problem\nOptimal Transport in One Dimension\nKantorovich Dual Problem (for general costs)\nKantorivich Dual Problem with Quadratic Cost\nRegularity of Optimal Transport Maps and the Monge-Ampére Equation on Riemannian Manifolds\n1-Wasserstein metric and generalizations\nDynamic Optimal Transport\nOptimal Transport and Ricci curvature\n\n\n\nThe 2-Wasserstein Metric\n\nGeodesics and generalized geodesics\nFormal Riemannian Structure of the Wasserstein metric\nFormal Riemannian Structure and Gradient in the Wasserstein metric\nAsymptotic equivalence of W_2 and H^-1\n\n\n\nVariants of the optimal transport problem\n\nMartingale optimal transport and mathematical finance\nMartingale optimal transport: dynamic formulation\nWasserstein barycenters and applications in image processing\nWasserstein-like Metrics on Graphs\nHellinger-Kantorovich Distance\nContinuous time martingale OT and Skorohod Embedding\nSchrodinger Bridge Problem\n\n\n\nNumerical methods for optimal transport\n\nDiscrete Optimal Transport\nAuction Algorithm\nSemidiscrete Optimal Transport\nEntropic Regularization and Sinkhorn’s Algorithm\nSliced Wasserstein Distance\nBack and Forth Method\n\n\n\nOptimal transport and statistics\n\nEstimation of transport maps\n\n\n\nWasserstein gradient flows\n\nBures-Wasserstein Gradient flows\n\n\n\nMathematical foundations\n\nDual space of C_0(x) vs C_b(x)\nConvergence of Measures and Metrizability\nFenchel-Moreau and Primal/Dual Optimization Problems\nFenchel-Rockafellar and Linear Programming\nThe Moreau-Yosida Regularization\nGradient flows in Hilbert spaces\nThe continuity equation and Benamour Brenier formula\nIsoperimetric inequality and OMT\nGaussian Measures\nAnalysis in metric spaces\nGradient flows in metric spaces\n\n\n\nApplications of optimal transport\n\nMachine Learning\nShallow neural networks as Wasserstein gradient flows\nWasserstein Generative Adversarial Networks\nKnothe Maps and Conditional Sampling\n\n\n\nOther\n\nNonlocal Equations Wiki\nProteopedia\nQuantum Information Wiki"
  },
  {
    "objectID": "backandfortharticle.html",
    "href": "backandfortharticle.html",
    "title": "The Back and Forth Method",
    "section": "",
    "text": "The Back and Forth Method is an iterative method introduced by Jacobs and Léger in (Jacobs and Léger 2020) for solving the optimal transport problem for a class of strictly convex costs, including quadratic and \\(p\\)-power costs. This article closely follows (Jacobs and Léger 2020).\n\nPreliminaries\nLet \\(\\Omega \\subset \\mathbb{R}^{d}\\), be convex and compact. A cost on \\(\\Omega\\) is a continuous function \\(c: \\Omega \\times \\Omega \\rightarrow \\mathbb{R}\\). However, for this approach we will only focus on the case in which \\[ c(x, y) = h(y-x) \\] for some strictly convex and even function \\(h: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\). Let \\(\\mu\\) and \\(\\nu\\) be probability measures. We will concentrate on the special case where \\(\\mu\\) and \\(\\nu\\) are absolutely continuous with respect to Lebesgue measure. Because of this, we may commit a standard abuse of notation and identify a measure with its density function.\n\n\nThe Optimal Transport Problem Background\nGiven measures \\(\\mu\\) and \\(\\nu\\) which are supported in \\(\\Omega\\), we can define the Monge formulation of the optimal transport problem (also known simply as the Monge Problem) as: \\[ C(\\mu, \\nu) = \\inf_{T} \\int_{\\Omega}c(x, T(x))d\\mu(x) \\] where this infimum runs over maps \\(T: \\Omega \\rightarrow \\Omega\\) such that \\(T \\# \\mu = \\nu\\), where \\(T \\# \\mu\\) is the pushforward of \\(\\mu\\) under \\(T\\). The pushforward condition can be characterized by defining the integral of the pushforward measure against continuous test functions \\(f: \\Omega \\rightarrow \\mathbb{R}\\): \\[ \\int_{\\Omega}f(y)d(T \\# \\mu)(y) = \\int_{\\Omega} f(T(x))d\\mu(x) . \\]\nIf \\(\\mu\\) and \\(\\nu\\) are absolutely continuous with respect to Lebesgue, there exists a unique optimal map \\(T_{*}\\) which pushed \\(\\mu\\) to \\(\\nu\\), and its inverse \\(T_{*}^{-1}\\) is the optimal map that pushes \\(\\nu\\) to \\(\\mu\\) (Gangbo 1995).\nThe pushforward constraint \\(T\\#\\mu = \\nu\\) holds if and only if \\[ \\int_{\\Omega}\\phi(T(x))d\\mu(x) = \\int_{\\Omega} \\phi(y) d\\nu(y) \\] for every continuous function \\(\\phi\\).\nIt follows that: \\[ \\inf_{T \\# \\mu = \\nu} \\int_{\\Omega} c(x, T(x))d\\mu(x) = \\inf_{T} \\sup_{\\phi} \\int_{\\Omega} c(x, T(x)) d \\mu(x) - \\phi(T(x))d\\mu(x) + \\int_{\\Omega}\\phi(y)d\\nu(y) . \\]\nIf \\(\\mu\\) is absolutely continuous, then we can interchange the supremum and infimum (Santambrogio 2015) and obtain: \\[ \\sup_{\\phi} \\inf_{T} \\int_{\\Omega}\\left( c(x, T(x)) - \\phi(T(x)) \\right) d \\mu(x) + \\int_{\\Omega}\\phi(y) d\\nu(y) . \\]\nFrom here, we can see that the term \\(\\inf_{T(x)} c(x, T(x)) - \\phi(T(x))\\) is important for the dual problem. To further analyze this, we will introduce a new operation, called the \\(c\\)-transform, which can be seen as a generalization of the Legendre transform.\nDefinition: Given a continuous function \\(\\phi: \\Omega \\rightarrow \\mathbb{R}\\), we define its \\(c\\)-transform \\(\\phi^c : \\Omega \\rightarrow \\mathbb{R}\\) by \\[ \\phi^{c} (x) = \\inf_{y \\in \\Omega} c(x, y) - \\phi(y) . \\]\nWe can now introduce the Kantorovich dual functional \\[ J(\\phi) = \\int_{\\Omega} \\phi d \\nu + \\int_{\\Omega} \\phi^{c} d\\mu . \\] It can be seen from above that \\[ C(\\mu, \\nu)= \\sup_{\\phi}J(\\phi) \\] where the supremum runs over all continuous functions \\(\\phi: \\Omega \\rightarrow \\mathbb{R}\\).\n\n\nGradient Ascent\nLet \\((\\mathcal{H}, || \\cdot||_{\\mathcal{H}})\\) be a separable Hilbert space and suppose that \\(F\\) is a smooth convex functional \\(F: \\mathcal{H} \\rightarrow \\mathbb{R}\\).\nDefinition: Given a point \\(\\phi \\in \\mathcal{H}\\), we say that a bounded linear map \\(\\delta F_{\\phi} : \\mathcal{H} \\rightarrow \\mathbb{R}\\) is the Fréchet derivative of \\(F\\) at \\(\\phi\\) if \\[ \\lim_{||h||_{\\mathcal{H}} \\rightarrow 0} \\frac{||F(\\phi + h) - F(\\phi) - \\delta F_{\\delta}(h)||_{\\mathcal{H}}}{||h||_{\\mathcal{H}}} = 0 . \\]\nDefinition: Let \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal{H}}\\) be the inner product associated with the Hilbert space \\(\\mathcal{H}\\). We say that a map \\(\\nabla_{\\mathcal{H}} F : \\mathcal{H} \\rightarrow \\mathcal{H}\\) is the \\(\\mathcal{H}\\)-gradient of \\(F\\) if \\[ \\langle \\nabla_{\\mathcal{H}} F(\\phi), h \\rangle_{\\mathcal{H}} = \\delta F_{\\phi}(h) \\] for all \\((\\phi, h) \\in \\mathcal{H} \\times \\mathcal{H}\\).\nTo define the back-and-forth method we will need to compute the gradient with respect to \\(\\dot{H^{1}}\\), where \\(\\dot{H^{1}}(\\Omega)\\) is the Hilbert space \\[ \\dot{H^{1}}(\\Omega) = \\{ \\varphi : \\Omega \\rightarrow \\mathbb{R} : \\int_{\\Omega} \\varphi(x)dx = 0 \\text{ and } \\int_{\\Omega} \\|\\nabla \\varphi(x)||^{2}dx &lt; \\infty \\} \\] equipped with the inner product \\[ \\langle \\varphi_{1}, \\varphi_2 \\rangle_{\\dot{H^{1}}(\\Omega)} = \\int_{\\Omega} \\nabla \\varphi_{1}(x) \\cdot \\nabla \\varphi_{2}(x) dx . \\]\n\n\nThe Back and Forth Method\nWe will consider the dual functional in the following equivalent forms \\[ J(\\phi) = \\int_{\\Omega} \\phi d \\nu + \\int_{\\Omega} \\phi^{c} d\\mu \\] and \\[ I(\\psi) = \\int_{\\Omega} \\psi^{c} d \\nu + \\int_{\\Omega} \\psi d\\mu . \\]\nNote that these two functionals are identical, just with the roles of \\(\\mu\\) and \\(\\nu\\) are flipped.\nWe are now ready to define the back-and-forth method.\n\n\nThe Back and Forth Algorithm\nAlgorithm 1: The Back and Forth Method\nGiven probability densities \\(\\mu\\) and \\(\\nu\\), set \\(\\phi_0 = 0, \\psi_0 = 0\\), and iterate:\n\\[\n\\begin{aligned}\n    \\phi_{n+\\frac{1}{2}} &\\leftarrow \\phi_n + \\sigma \\nabla_{\\dot{H}^1} J(\\phi_n), \\\\\n    \\psi_{n+\\frac{1}{2}} &\\leftarrow (\\phi_{n+\\frac{1}{2}})^c, \\\\\n    \\psi_{n+1} &\\leftarrow \\psi_{n+\\frac{1}{2}} + \\sigma \\nabla_{\\dot{H}^1} I(\\psi_{n+\\frac{1}{2}}), \\\\\n    \\phi_{n+1} &\\leftarrow (\\psi_{n+1})^c.\n\\end{aligned}\n\\]\nWe will note that given two probability measures supported on a discrete grid with \\(n\\) points, we compute the optimal map using \\(O(n)\\) storage space and \\(O(n \\log(n))\\) operations per iteration, with an approximately exponential convergence rate. For more information and numerical examples, please see (Jacobs and Léger 2020).\n\n\n\n\n\nReferences\n\nGangbo, Wilfrid. 1995. “Quelques Problèmes d’analyse Non Convexe. Habilitation à Diriger Des Recherches En Mathématiques.” Habilitation. Université de Metz.\n\n\nJacobs, Matt, and Flavien Léger. 2020. “A Fast Approach to Optimal Transport: The Back and Forth Method.” https://arxiv.org/abs/1905.12154.\n\n\nSantambrogio, Filippo. 2015. Optimal Transport for Applied Mathematicians. Vol. 87. Progress in Nonlinear Differential Equations and Their Applications. Cham: Birkhäuser/Springer. https://doi.org/10.1007/978-3-319-20828-2."
  },
  {
    "objectID": "DynamicMOT.html",
    "href": "DynamicMOT.html",
    "title": "Dynamic Formulation of Martingale Optimal Transport",
    "section": "",
    "text": "The following page mainly follows the first two sections of (Backhoff-Veraguas et al. 2020)."
  },
  {
    "objectID": "DynamicMOT.html#a-static-formulation",
    "href": "DynamicMOT.html#a-static-formulation",
    "title": "Dynamic Formulation of Martingale Optimal Transport",
    "section": "A static formulation",
    "text": "A static formulation\nA first observation is to claim the equivalence of \\(MT(\\mu, \\nu)\\) with the static formulation \\[\\begin{align}\n    WOT(\\mu, \\nu):= \\sup \\{\\int \\mu(dx) \\sup_{q\\in \\Pi(\\gamma^d, \\pi_x)}\\int b\\cdot m q(db, dm):\\{\\pi_x\\}_{x\\in \\Omega}, \\overline{\\pi_x} =x, \\nu(dy) = \\int \\pi_x(dy) \\mu(dx)\\}\n\\end{align}\\] where the supremum runs through all kernels (hence conditional distributions) from \\(\\mu\\) to \\(\\nu\\) with the mean property \\(\\overline{\\pi_x}:= \\int y \\pi_x(dy) = x\\). In terms of joint distributions, they corresponds precisely to all martingale couplings between \\(\\mu\\) and \\(\\nu\\).\n\nTheorem 1 (Equivalence between WOT and MT) For all \\(\\mu \\leq_c \\nu \\in P^2(\\Omega)\\), we have \\(MT(\\mu, \\nu) = WOT(\\mu, \\nu)\\).\n\n\nProof. \\((\\leq)\\). Going from dynamic to static is easier. Suppose we have a path \\(X_t = X_0 + \\int_0^t \\sigma_s dB_s\\) with \\(X_0\\sim \\mu\\) and \\(X_1\\in \\nu\\). The key observation is that by Ito’s formula, using super-script to denote the coordinate components, we have \\[\\begin{align}\n    d(X_t^i B_t^i) = X_t^i dB_t^i + B_t^i dX_t^i + d\\langle{X^i, B^i}\\rangle_t\n\\end{align}\\] where \\(d\\langle{X^i, B^i}\\rangle_t = \\sum_j d(\\sigma_{ij}B_t^j) dB_t^i= \\sigma_{ii}dt\\) is the cross variation between \\(X\\) and \\(B\\). Hence \\[\\begin{align}\nX_1\\cdot B_1 =     X_1\\cdot B_1 - X_0\\cdot B_0 = \\sum_i \\int_0^1 X_t^i dB_t^i + \\sum_{ij} \\int_0^1 B_t^i B_t^j \\sigma_{ij} dB_t^j + \\sum_i \\int_0^1 \\sigma_{ii}dt\n\\end{align}\\] Taking expectation gives the following frequently used formula \\[\n    \\mathbb{E}(X_1\\cdot B_1) = \\mathbb{E}\\int_0^1 tr(\\sigma_t)dt\n\\tag{1}\\] Using the tower law gives \\[\\begin{align}\n\\mathbb{E}(X_1\\cdot B_1) = \\mathbb{E}(\\mathbb{E}(X_1\\cdot B_1\\mid X_0)) = \\int \\mu(dx) \\int_{q\\in \\Pi(\\gamma, \\pi_x)} b\\cdot m q(dm, db) \\leq WOT(\\mu, \\nu)\n\\end{align}\\]\nConsidering all paths gives the inequality.\n\\((\\geq)\\). The remaining direction relies on an important construction. Suppose \\(\\pi\\in \\Pi(\\mu, \\nu)\\). Then for all \\(x\\in \\Omega\\), by Brenier’s theorem, there exists a unique convex gradient map \\(\\nabla F^x\\) such that \\(\\nabla F^x\\# \\gamma = \\pi_x\\) since \\(\\gamma\\) is absolutely continuous to \\(\\mathcal{L}eb\\) (recall \\(\\gamma\\) is the d-dim normal). Now we pick any filtration \\(\\mathcal{F}_t\\) and a standard Brownian motion \\(B_t\\) adapted to it and define \\[\\begin{align}\n    M_t^x:= E(\\nabla F^x (B_1) \\mid \\mathcal{F}_t^B) = E(\\nabla F^x (B_1) \\mid B_t)\n\\end{align}\\] where the second equality follows from the Markov property of \\(B_t\\). It is clearly that \\(M_t^x\\) is a martingale with \\(M_1^x = \\nabla F^x(B_1)\\sim \\pi_x\\) and \\(M_0^x = x\\). Now we pick \\(X\\sim \\mu\\) independent to \\(\\mathcal{F}_t\\). Then we define \\[\\begin{align}\n    M_t^X(\\omega) = M_t^{X(\\omega)}\n\\end{align}\\] which could be easily shown to be a martingale adapted to the filtration by \\(X\\) and \\(\\mathcal{F}_t\\) in which \\(X\\) is independent to \\(\\mathcal{F}_t^B\\). Let \\(\\sigma_t\\) be the volatility process of \\(M_t^X\\) (for instance given by the ). Then by Equation 1, we have \\[\\begin{align}\n     \\mathbb{E}\\int_0^1 tr(\\sigma_t)dt &= \\mathbb{E}(M_1^X\\cdot B_1) = \\mathbb{E}(\\mathbb{E}(\\nabla F^x(B_1)\\cdot B_1\\mid X)) = \\int \\mu(dx) \\mathbb{E}(\\nabla F^x(B_1)\\cdot B_1\\mid X = x) \\\\\n    & \\overset{X\\perp B}{=} \\int \\mu(dx) \\mathbb{E}(\\nabla F^x (B_1)\\cdot B_1)=\\int \\mu(dx)  \\sup_{q\\in \\Pi(\\gamma, \\pi_x)}\\int b\\cdot m q(dm, db)\\\\\n    &\\geq WOT(\\mu, \\nu)\n\\end{align}\\] in which the last equality follows from Brenier theorem for the choice of \\(\\nabla F^x\\). This of course implies that \\(MT(\\mu, \\nu)\\geq WOT(\\mu, \\nu)\\).\n\nWell-posedness with unique-in-law solution\nWith the equivalence between the static formulation and dynamic formulation one can then raise the well-established theory of the static case to the dynamic case.\n\nTheorem 2 (Well-posedness of MT and WOT) Let \\(\\mu\\leq_c \\nu\\in P^2(\\Omega)\\). Then we have\n\nThere exists a unique martingale coupling \\(\\pi^*\\) attaining \\(WOT(\\mu, \\nu)\\).\nThere exists a unique-in-law martingale \\(M_t\\) attaining \\(MT(\\mu, \\nu)\\).\n\n\n\nProof. \n\nThis follows from standard arguement so here we only give a sketch of proof. In particular recall that \\[\\begin{align}\nWOT(\\mu, \\nu):= \\sup \\{\\int \\mu(dx) \\sup_{q\\in \\Pi(\\gamma^d, \\pi_x)}\\int b\\cdot m q(db, dm):\\{\\pi_x\\}_{x\\in \\Omega}, \\overline{\\pi_x} =x, \\nu(dy) = \\int \\pi_x(dy) \\mu(dx)\\}\n\\end{align}\\] We can then make use of tightness property of martingale measures as well as strict concavity of the functional \\(H(\\eta):= \\sup_{q\\in \\Pi(\\gamma^d, \\eta)} \\int b\\cdot m d(db, dm)\\), which basically follows from that of the Wasserstein-2 metric, to claim the well-posedness of the static optimization.\nWe first consider the case where \\(\\mu = \\delta_x\\). Suppose \\(N_t\\) attains \\(MT(\\mu, \\nu)\\) with filtration \\(\\mathcal{F}_t\\) and Brownian motion \\(B_t\\). Then we have by Equation 1 \\[\\begin{align}\n\\mathbb{E}(N_1 B_1) =\\mathbb{E}\\int_0^1 tr(\\sigma_s) ds = MT(\\delta_x,\\nu) = WOT(\\delta_x, \\nu) = \\sup_{q\\in \\Pi(\\gamma, \\nu)}\\int b\\cdot n q(db, dn)\n\\end{align}\\] in which the last equality follows simply by definition of WOT. Since \\(N_1\\sim \\nu\\) and \\(B_1\\sim \\gamma\\), it follows that \\(N_t\\) induces the optimal coupling for the last quantity. It follows the by Brenier’s theorem we must have \\(N_1 = \\nabla F^x (B_1)\\). By the martingale property, we then have \\(N_t = E(\\nabla F^x(B_1)\\mid \\mathcal{F}_t)  = \\mathbb{E}(\\nabla F^x(B_1)\\mid B_t)\\) (recall that in the definition of MT, we are restricted to martingale over Brownian filtrations). This determines a unique law. For general \\(\\mu\\), as we are assuming the initial condition of the optimal process has to be independent to the remaining of the process, we already determine the law.\n\n\nProperty of stretched Brownian motions\nBecause of the well-posedness of \\(MT(\\mu, \\nu)\\), it deserves to give a name to the unique-in-law optimizer.\n\nDefinition 1 (Stretched Brownian Motions) Let \\(\\mu\\leq_c \\nu\\). Then we call the unique-in-law optimizer of \\(MT(\\mu, \\nu)\\) the stretched Brownian motion (sBm) from \\(\\mu\\) to \\(\\nu\\), which is a martingale \\(M_t\\) adapted to some Brownian filtration \\(\\mathcal{F}_t\\) and an independent \\(M_0\\sim \\mu\\) with \\(M_1\\sim \\nu\\).\n\n\nMarkov property of sBm\nTo deduce the Markov property of sBm we are going to establish a dynamic programming principle (DPP) for \\(MT(\\mu, \\nu)\\). To this end we define for \\(t\\in [0, 1]\\), \\[\\begin{align}\n    V(t, 1, \\mu, \\nu):= \\sup\\{\\mathbb{E}\\int_t^1 tr(\\sigma_s)ds: (M_t, \\sigma_t, \\mathcal{F}_t, \\mathbb{P}), M_t\\sim \\mu, M_1:=\\int_t^1 \\sigma_s dB_s \\sim \\nu\\}\n\\end{align}\\] with filtration conditions similar to that of \\(MT(\\mu, \\nu)\\) in which \\(M_t\\) has to be independent to the rest of the process. In particular, we have \\(V(0, 1, \\mu, \\nu) = MT(\\mu, \\nu)\\). The DPP is as follows\n\nTheorem 3 (DPP of \\(MT(\\mu, \\nu)\\)) Let \\(\\mu\\leq_c \\nu\\in P^2(\\Omega)\\). Then \\[\\begin{align}\n    V(0, 1, \\mu, \\nu) = \\sup\\{\\mathbb{E}(\\int_0^t tr(\\sigma_s)ds) + V(t, 1, \\mathcal{L}(M_t), \\nu): (M_0, \\sigma_t, \\mathcal{F}_t), M_0\\sim \\mu, M_t = \\int_0^t \\sigma_s dB_s\\}\n\\end{align}\\]\n\n\nProof. The \\(\\leq\\) direction is trivial. We consider the opposite direction. Let \\(M_t\\) be satisfying the condition on the right-hand-side so \\(dM_t = \\sigma_t dB_t\\) with \\(M_0\\sim \\mu\\). Now we want to build an optimal path from time \\(t\\) to \\(1\\). Clearly by similar arguement to sections 3 the value \\(V(t, 1,\\mathcal{L}(M_t), \\nu)\\) is attained by a unique-in-law processes with an independent initial condition. We now choose a Brownian filtration \\(\\mathcal{G}_t = \\sigma(W_t)\\) independent to \\(\\mathcal{F}_t\\). Let \\(\\sigma^{(2)}_t\\) be the volatility process of the dynamics: \\(dM^{(2)}_s = \\sigma^{(2)}_sdW_s\\). We set the initial condition to be \\(M_t^{(2)} = M_t\\). Since this initial condition is independent to the rest of the process, the resulting path gives a valid optimizer that attains the unique law. We then define \\[\\begin{align}\n    M_s^* := \\mathbb{1}_{s\\leq t}M_s + \\mathbb{1}_{s&gt;t} M_s^{(2)}\n\\end{align}\\] We now have to define a Browian motion for which the process is a martingale. Consider \\[\\begin{align}\n    B^*_s:= \\mathbb{1}_{s\\leq t} B_s + \\mathbb{1}_{s&gt;t} (W_s-W_t+ B_{t})\n\\end{align}\\] One can show that this is a Brownian motion! In addition, \\(M_s^*\\) is then a martingale with respect to this Brownian filtration (either by definition or one can show that \\(dM_s^* =  (\\sigma_s \\mathbb{1}_{s\\in [0, t]} + \\sigma_s^{(2)}\\mathbb{1}_{s\\in [t, 1]})dB_s^*\\)). It then follows that \\[\\begin{align}\n    \\mathbb{E}(\\int_0^t \\sigma_s dB_s) + V(t, 1, \\mathcal{L}(M_t), \\nu)=\\mathbb{E}(\\int_0^t \\sigma_s dB_s + \\int_t^1 \\sigma^{(2)}_s dW_s) = \\mathbb{E}(\\int_0^1 \\sigma^*_s dB_s^*)\\leq V(0, 1, \\mu, \\nu)\n\\end{align}\\] which concludes the proof.\n\nCorollary 1 The stretched Brownian motion from \\(\\mu\\) to \\(\\nu\\) is Markov\n\n\nProof. Fix \\(t\\) and a path of sBm \\(M_t\\). Then we can construct another sBm (identified by the same law) by first restricting \\(M_s\\) over \\([0, t]\\) and glue an independent path from \\(M_t\\) (given by the flexibility of filtration in the optimizer of \\(V\\)). It follows that conditioning on the path up to time \\(t\\) is the same as conditioning on the state at \\(t\\)."
  },
  {
    "objectID": "DynamicMOT.html#well-posedness-with-unique-in-law-solution",
    "href": "DynamicMOT.html#well-posedness-with-unique-in-law-solution",
    "title": "Dynamic Formulation of Martingale Optimal Transport",
    "section": "Well-posedness with unique-in-law solution",
    "text": "Well-posedness with unique-in-law solution\nWith the equivalence between the static formulation and dynamic formulation one can then raise the well-established theory of the static case to the dynamic case.\n\nTheorem 2 (Well-posedness of MT and WOT) Let \\(\\mu\\leq_c \\nu\\in P^2(\\Omega)\\). Then we have\n\nThere exists a unique martingale coupling \\(\\pi^*\\) attaining \\(WOT(\\mu, \\nu)\\).\nThere exists a unique-in-law martingale \\(M_t\\) attaining \\(MT(\\mu, \\nu)\\).\n\n\n\nProof. \n\nThis follows from standard arguement so here we only give a sketch of proof. In particular recall that \\[\\begin{align}\nWOT(\\mu, \\nu):= \\sup \\{\\int \\mu(dx) \\sup_{q\\in \\Pi(\\gamma^d, \\pi_x)}\\int b\\cdot m q(db, dm):\\{\\pi_x\\}_{x\\in \\Omega}, \\overline{\\pi_x} =x, \\nu(dy) = \\int \\pi_x(dy) \\mu(dx)\\}\n\\end{align}\\] We can then make use of tightness property of martingale measures as well as strict concavity of the functional \\(H(\\eta):= \\sup_{q\\in \\Pi(\\gamma^d, \\eta)} \\int b\\cdot m d(db, dm)\\), which basically follows from that of the Wasserstein-2 metric, to claim the well-posedness of the static optimization.\nWe first consider the case where \\(\\mu = \\delta_x\\). Suppose \\(N_t\\) attains \\(MT(\\mu, \\nu)\\) with filtration \\(\\mathcal{F}_t\\) and Brownian motion \\(B_t\\). Then we have by Equation 1 \\[\\begin{align}\n\\mathbb{E}(N_1 B_1) =\\mathbb{E}\\int_0^1 tr(\\sigma_s) ds = MT(\\delta_x,\\nu) = WOT(\\delta_x, \\nu) = \\sup_{q\\in \\Pi(\\gamma, \\nu)}\\int b\\cdot n q(db, dn)\n\\end{align}\\] in which the last equality follows simply by definition of WOT. Since \\(N_1\\sim \\nu\\) and \\(B_1\\sim \\gamma\\), it follows that \\(N_t\\) induces the optimal coupling for the last quantity. It follows the by Brenier’s theorem we must have \\(N_1 = \\nabla F^x (B_1)\\). By the martingale property, we then have \\(N_t = E(\\nabla F^x(B_1)\\mid \\mathcal{F}_t)  = \\mathbb{E}(\\nabla F^x(B_1)\\mid B_t)\\) (recall that in the definition of MT, we are restricted to martingale over Brownian filtrations). This determines a unique law. For general \\(\\mu\\), as we are assuming the initial condition of the optimal process has to be independent to the remaining of the process, we already determine the law.\n\n\nProperty of stretched Brownian motions\nBecause of the well-posedness of \\(MT(\\mu, \\nu)\\), it deserves to give a name to the unique-in-law optimizer.\n\nDefinition 1 (Stretched Brownian Motions) Let \\(\\mu\\leq_c \\nu\\). Then we call the unique-in-law optimizer of \\(MT(\\mu, \\nu)\\) the stretched Brownian motion (sBm) from \\(\\mu\\) to \\(\\nu\\), which is a martingale \\(M_t\\) adapted to some Brownian filtration \\(\\mathcal{F}_t\\) and an independent \\(M_0\\sim \\mu\\) with \\(M_1\\sim \\nu\\).\n\n\nMarkov property of sBm\nTo deduce the Markov property of sBm we are going to establish a dynamic programming principle (DPP) for \\(MT(\\mu, \\nu)\\). To this end we define for \\(t\\in [0, 1]\\), \\[\\begin{align}\n    V(t, 1, \\mu, \\nu):= \\sup\\{\\mathbb{E}\\int_t^1 tr(\\sigma_s)ds: (M_t, \\sigma_t, \\mathcal{F}_t, \\mathbb{P}), M_t\\sim \\mu, M_1:=\\int_t^1 \\sigma_s dB_s \\sim \\nu\\}\n\\end{align}\\] with filtration conditions similar to that of \\(MT(\\mu, \\nu)\\) in which \\(M_t\\) has to be independent to the rest of the process. In particular, we have \\(V(0, 1, \\mu, \\nu) = MT(\\mu, \\nu)\\). The DPP is as follows\n\nTheorem 3 (DPP of \\(MT(\\mu, \\nu)\\)) Let \\(\\mu\\leq_c \\nu\\in P^2(\\Omega)\\). Then \\[\\begin{align}\n    V(0, 1, \\mu, \\nu) = \\sup\\{\\mathbb{E}(\\int_0^t tr(\\sigma_s)ds) + V(t, 1, \\mathcal{L}(M_t), \\nu): (M_0, \\sigma_t, \\mathcal{F}_t), M_0\\sim \\mu, M_t = \\int_0^t \\sigma_s dB_s\\}\n\\end{align}\\]\n\n\nProof. The \\(\\leq\\) direction is trivial. We consider the opposite direction. Let \\(M_t\\) be satisfying the condition on the right-hand-side so \\(dM_t = \\sigma_t dB_t\\) with \\(M_0\\sim \\mu\\). Now we want to build an optimal path from time \\(t\\) to \\(1\\). Clearly by similar arguement to sections 3 the value \\(V(t, 1,\\mathcal{L}(M_t), \\nu)\\) is attained by a unique-in-law processes with an independent initial condition. We now choose a Brownian filtration \\(\\mathcal{G}_t = \\sigma(W_t)\\) independent to \\(\\mathcal{F}_t\\). Let \\(\\sigma^{(2)}_t\\) be the volatility process of the dynamics: \\(dM^{(2)}_s = \\sigma^{(2)}_sdW_s\\). We set the initial condition to be \\(M_t^{(2)} = M_t\\). Since this initial condition is independent to the rest of the process, the resulting path gives a valid optimizer that attains the unique law. We then define \\[\\begin{align}\n    M_s^* := \\mathbb{1}_{s\\leq t}M_s + \\mathbb{1}_{s&gt;t} M_s^{(2)}\n\\end{align}\\] We now have to define a Browian motion for which the process is a martingale. Consider \\[\\begin{align}\n    B^*_s:= \\mathbb{1}_{s\\leq t} B_s + \\mathbb{1}_{s&gt;t} (W_s-W_t+ B_{t})\n\\end{align}\\] One can show that this is a Brownian motion! In addition, \\(M_s^*\\) is then a martingale with respect to this Brownian filtration (either by definition or one can show that \\(dM_s^* =  (\\sigma_s \\mathbb{1}_{s\\in [0, t]} + \\sigma_s^{(2)}\\mathbb{1}_{s\\in [t, 1]})dB_s^*\\)). It then follows that \\[\\begin{align}\n    \\mathbb{E}(\\int_0^t \\sigma_s dB_s) + V(t, 1, \\mathcal{L}(M_t), \\nu)=\\mathbb{E}(\\int_0^t \\sigma_s dB_s + \\int_t^1 \\sigma^{(2)}_s dW_s) = \\mathbb{E}(\\int_0^1 \\sigma^*_s dB_s^*)\\leq V(0, 1, \\mu, \\nu)\n\\end{align}\\] which concludes the proof.\n\nCorollary 1 The stretched Brownian motion from \\(\\mu\\) to \\(\\nu\\) is Markov\n\n\nProof. Fix \\(t\\) and a path of sBm \\(M_t\\). Then we can construct another sBm (identified by the same law) by first restricting \\(M_s\\) over \\([0, t]\\) and glue an independent path from \\(M_t\\) (given by the flexibility of filtration in the optimizer of \\(V\\)). It follows that conditioning on the path up to time \\(t\\) is the same as conditioning on the state at \\(t\\)."
  },
  {
    "objectID": "DynamicMOT.html#markov-property-of-sbm",
    "href": "DynamicMOT.html#markov-property-of-sbm",
    "title": "Dynamic Formulation of Martingale Optimal Transport",
    "section": "Markov property of sBm",
    "text": "Markov property of sBm\nTo deduce the Markov property of sBm we are going to establish a dynamic programming principle (DPP) for \\(MT(\\mu, \\nu)\\). To this end we define for \\(t\\in [0, 1]\\), \\[\\begin{align}\n    V(t, 1, \\mu, \\nu):= \\sup\\{\\mathbb{E}\\int_t^1 tr(\\sigma_s)ds: (M_t, \\sigma_t, \\mathcal{F}_t, \\mathbb{P}), M_t\\sim \\mu, M_1:=\\int_t^1 \\sigma_s dB_s \\sim \\nu\\}\n\\end{align}\\] with filtration conditions similar to that of \\(MT(\\mu, \\nu)\\) in which \\(M_t\\) has to be independent to the rest of the process. In particular, we have \\(V(0, 1, \\mu, \\nu) = MT(\\mu, \\nu)\\). The DPP is as follows\n\nTheorem 3 (DPP of \\(MT(\\mu, \\nu)\\)) Let \\(\\mu\\leq_c \\nu\\in P^2(\\Omega)\\). Then \\[\\begin{align}\n    V(0, 1, \\mu, \\nu) = \\sup\\{\\mathbb{E}(\\int_0^t tr(\\sigma_s)ds) + V(t, 1, \\mathcal{L}(M_t), \\nu): (M_0, \\sigma_t, \\mathcal{F}_t), M_0\\sim \\mu, M_t = \\int_0^t \\sigma_s dB_s\\}\n\\end{align}\\]\n\n\nProof. The \\(\\leq\\) direction is trivial. We consider the opposite direction. Let \\(M_t\\) be satisfying the condition on the right-hand-side so \\(dM_t = \\sigma_t dB_t\\) with \\(M_0\\sim \\mu\\). Now we want to build an optimal path from time \\(t\\) to \\(1\\). Clearly by similar arguement to sections 3 the value \\(V(t, 1,\\mathcal{L}(M_t), \\nu)\\) is attained by a unique-in-law processes with an independent initial condition. We now choose a Brownian filtration \\(\\mathcal{G}_t = \\sigma(W_t)\\) independent to \\(\\mathcal{F}_t\\). Let \\(\\sigma^{(2)}_t\\) be the volatility process of the dynamics: \\(dM^{(2)}_s = \\sigma^{(2)}_sdW_s\\). We set the initial condition to be \\(M_t^{(2)} = M_t\\). Since this initial condition is independent to the rest of the process, the resulting path gives a valid optimizer that attains the unique law. We then define \\[\\begin{align}\n    M_s^* := \\mathbb{1}_{s\\leq t}M_s + \\mathbb{1}_{s&gt;t} M_s^{(2)}\n\\end{align}\\] We now have to define a Browian motion for which the process is a martingale. Consider \\[\\begin{align}\n    B^*_s:= \\mathbb{1}_{s\\leq t} B_s + \\mathbb{1}_{s&gt;t} (W_s-W_t+ B_{t})\n\\end{align}\\] One can show that this is a Brownian motion! In addition, \\(M_s^*\\) is then a martingale with respect to this Brownian filtration (either by definition or one can show that \\(dM_s^* =  (\\sigma_s \\mathbb{1}_{s\\in [0, t]} + \\sigma_s^{(2)}\\mathbb{1}_{s\\in [t, 1]})dB_s^*\\)). It then follows that \\[\\begin{align}\n    \\mathbb{E}(\\int_0^t \\sigma_s dB_s) + V(t, 1, \\mathcal{L}(M_t), \\nu)=\\mathbb{E}(\\int_0^t \\sigma_s dB_s + \\int_t^1 \\sigma^{(2)}_s dW_s) = \\mathbb{E}(\\int_0^1 \\sigma^*_s dB_s^*)\\leq V(0, 1, \\mu, \\nu)\n\\end{align}\\] which concludes the proof.\n\nCorollary 1 The stretched Brownian motion from \\(\\mu\\) to \\(\\nu\\) is Markov\n\n\nProof. Fix \\(t\\) and a path of sBm \\(M_t\\). Then we can construct another sBm (identified by the same law) by first restricting \\(M_s\\) over \\([0, t]\\) and glue an independent path from \\(M_t\\) (given by the flexibility of filtration in the optimizer of \\(V\\)). It follows that conditioning on the path up to time \\(t\\) is the same as conditioning on the state at \\(t\\)."
  },
  {
    "objectID": "1Wasserstein.html",
    "href": "1Wasserstein.html",
    "title": "1-Wasserstein Metric and Generalizations",
    "section": "",
    "text": "Let \\(\\mathcal{M}\\) denote the space of all Radon measures on \\(\\mathbb{R}^d\\) with finite mass. Moreover, let \\(\\mathcal{M}^p\\) denote the space of Radon measures with finite \\(p^{th}\\) moment, that is, \\(\\int_{\\mathbb{R}^d} |x|^p d\\mu(x) &lt; \\infty\\). Then, for \\(\\mu,\\nu \\in \\mathcal{M}^p\\) with \\(|\\mu|=|\\nu|\\) the \\(p\\)-Wasserstein distance is defined as :\\(W_p(\\mu,\\nu) = ( |\\mu| \\underset{ \\pi \\in \\Gamma(\\mu,\\nu) }{ \\inf } \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d } |x-y|^p d\\pi(x,y))^{1/p}\\) where \\(\\Gamma(\\mu,\\nu)\\) denotes the set of all transport plans from \\(\\mu\\) to \\(\\nu\\). Note that, if we restrict \\(\\mu\\) and \\(\\nu\\) to be probability measures, then the \\(p\\)-Wasserstein distance can be seen as a special case of the Kantorovich Problem, where \\(c(x_1,x_2) = |x_1-x_2|^p\\). Furthermore, if we let \\(\\mathcal{P}^p\\) denote the space of probability measures on \\(\\mathbb{R}^d\\) with finite \\(p^{th}\\) moment, then \\(W_p\\) defines a metric on \\(\\mathcal{P}^p\\).\n\nMeasures with unequal mass and signed measures\nThe classical Wasserstein distance can be generalized for measures with unequal mass by allowing the addition and removal of mass to \\(\\mu\\) and \\(\\nu\\). In this case, there is unit cost \\(a &gt;0\\) for the addition or removal of mass from both \\(\\mu\\) and \\(\\nu\\), whereas the transport cost of mass between \\(\\mu\\) and \\(\\nu\\) stays the same with the classical Kantorovich Problem; multiplied with some rate \\(b &gt;0\\) .\nDefinition. For some \\(a,b \\in \\mathbb{R}_{++}\\) and \\(p \\geq 1\\), the generalized Wasserstein distance \\({W}_p^{a,b}\\) is given by :\\({W}_p^{a,b}(\\mu,\\nu) = \\underset{\\underset{ |\\bar{\\mu}| = |\\bar{\\nu}|  }{\\bar{\\mu},\\bar{\\nu} \\in \\mathcal{M}^p}}{\\inf} (a(|\\mu-\\bar{\\mu}|) + a(|\\nu-\\bar{\\nu}|) + b {W}_p(\\bar{\\mu},\\bar{\\nu}) )\\) where \\({W}_p(\\bar{\\mu},\\bar{\\nu})\\) is the classical \\(p\\)-Wasserstein distance for measures with equal mass.\nNote that, under this definition \\({W}_p^{a,b}\\) defines a metric on \\(\\mathcal{M}\\), and \\((\\mathcal{M},{W}_p^{a,b})\\) is a complete metric space.. Furthermore, this generalization allows one to extend the \\(1\\)-Wasserstein distance to the signed Radon measures as well. Let us denote the space of all signed Radon measures on \\(\\mathbb{R}^d\\) with finite mass with \\(\\mathcal{M}^s(\\mathbb{R}^d)\\).\nDefinition. For some \\(a,b \\in \\mathbb{R}_{++}\\), the generalized Wasserstein distance for signed measures \\(\\mathbb{W}_1^{a,b}\\) is given by :\\(\\mathbb{W}_1^{a,b}(\\mu,\\nu) = W_1^{a,b}(\\mu_+ + \\nu_- , \\mu_- + \\nu_+)\\) where \\(\\mu_+,\\nu_-,\\mu_-,\\nu_+\\) are any measures in \\(\\mathcal{M}(\\mathbb{R}^d)\\) such that \\(\\mu = \\mu_+ - \\mu_-\\) and \\(\\nu = \\nu_+ - \\nu_-\\).\nMoreover, if we let \\(||\\mu||^{a,b} = \\mathbb{W}_1^{a,b}(\\mu,0) =  W_1^{a,b}(\\mu_+ , \\mu_- )\\), then \\((\\mathcal{M}^s,||\\mu||^{a,b})\\) is a normed vector space. However, as opposed to the completeness of \\((\\mathcal{M},{W}_p^{a,b})\\) , \\((\\mathcal{M}^s,||\\mu||^{a,b})\\) fails to be a Banach space.\n\n\nDuality\nAs a special case of the Kantorovich Dual Problem when \\(c(x_1,x_2) = |x_1-x_2|\\), \\(1\\)-Wasserstein metric has the following dual characterization.\n\nTheorem. Let Lip\\((\\mathbb{R}^d)\\) denote the space of all Lipschitz functions on \\(\\mathbb{R}^d\\), and let \\(||\\varphi||_{\\text{Lip}} = \\underset{x\\neq y}{\\sup} \\frac{\\varphi(x)-\\varphi(y)}{|x-y|}\\). Then,\n\n\\(W_1(\\mu,\\nu) = \\sup \\{ \\int_{\\mathbb{R}^d} \\varphi d(\\mu-\\nu) : \\varphi \\in \\mathcal{C}^0_c, \\text{ } ||\\varphi||_{\\text{Lip}} \\leq 1 \\}\\). In a similar spirit, this duality result can be extended for generalized \\(1\\)-Wasserstein distances \\(W^{1,1}_1\\) and \\(\\mathbb{W}^{1,1}_1\\) as well, where \\(a\\) and \\(b\\) are taken as 1. For measures with unequal mass, when the additional constraint \\(||\\varphi||_\\infty \\leq 1\\) is imposed on the test functions, it holds that\n\n\n:\\(W^{1,1}_1(\\mu,\\nu) = \\sup \\{ \\int_{\\mathbb{R}^d} \\varphi d(\\mu-\\nu) : \\varphi \\in \\mathcal{C}^0_c, \\text{ } ||\\varphi||_\\infty \\leq 1, \\text{ } ||\\varphi||_{\\text{Lip}} \\leq 1 \\}\\).\nIn terms of the generalized 1-Wasserstein metric for signed measures, when we relax the compact support condition on the test functions we obtain the equivalent duality characterization as follows.\n:\\(\\mathbb{W}^{1,1}_1(\\mu,\\nu) = \\sup \\{ \\int_{\\mathbb{R}^d} \\varphi d(\\mu-\\nu) : \\varphi \\in \\mathcal{C}^0_b, \\text{ } ||\\varphi||_\\infty \\leq 1, \\text{ } ||\\varphi||_{\\text{Lip}} \\leq 1 \\}\\).\n\n\nReferences\n  Piccoli, B., Rossi, F. Generalized Wasserstein Distance and its Application to Transport Equations with Source. Arch Rational Mech Anal 211, 335–358 (2014). https://doi.org/10.1007/s00205-013-0669-x \n Piccoli, B., Rossi, F., Tournus M. A Wasserstein norm for signed measures, with application to nonlocal transport equation with source term. arXiv:1910.05105 (2019). \n Piccoli, B., Rossi, F. Piccoli, B., Rossi, F. On Properties of the Generalized Wasserstein Distance. Arch Rational Mech Anal 222, 1339–1365 (2016). https://doi.org/10.1007/s00205-016-1026-7 \nC. Villani, Topics in Optimal Transportation, Chapter 1."
  },
  {
    "objectID": "NewArticleIdeas.html",
    "href": "NewArticleIdeas.html",
    "title": "New Article Ideas",
    "section": "",
    "text": "Below, you can find a list of new article ideas and suggested references.\nFeel free to incorporate additional references! Please list all references you use at the bottom of your article.\nIf you choose to write about one of these ideas, let me know, and I will remove it from the list below. Want to write about something that’s not listed here? Great! Let me know, and I will suggest some references.\n\nVariants of Optimal Transport Problems\n\nThe \\(s\\)-Wasserstein metric for \\(s&lt;1\\) Craig and Yu, 2024, Santambrogio 3.3.2\nEntropic optimal transport, Chewi, Niles-Weed, Rigollet Ch.4 and 2\nConnections between entropic optimal transport and the Schrödinger bridge problem 1 - Ka Lok\nBrenier maps 3\nKnothe maps, Figalli-Glaudo (9-14), Carlier, et. al. ’08\n\n\n\nThe 2-Wasserstein Metric\n\nMulti-marginal optimal transport and density functional theory\nDisplacement convexity; Santambrogio (249-251,271-276); Villani (150-154) (make sure to cite existing wiki article on Geodesics and generalized geodesics)\n\n\n\nNumerical Methods for Optimal Transport\n\nComputing OT via Benamou-Brenier; Santambrogio (220-225); Peyré, Cuturi (102-108)\nWasserstein Barycenters; Santambrogio (215-218); Peyré, Cuturi (138-144)\n\n\n\nWasserstein Gradient Flows\n\nFundamentals of Wasserstein Gradient flows, Chewi, Niles-Weed, Rigollet (135-148) and Santambrogio, ‘Euclidean, Metric, and Wasserstein GFs’\n\n\n\nMathematical Foundations\n\n\nStatistical Foundations\n\nEstimation of Wasserstein distances, Chewi, Niles-Weed, Rigollet Ch.2\n\n\n\nApplications:\n\nOptimal transport methods in economics; see introduction of book by Galichon (I have a copy you can borrow) and 6\nQuantization and Lloyd’s algorithm 7, 8, 9\nTransformers as Wasserstein Gradient Flows; Chewi, Niles-Weed, Rigollet (195-200)\nInferring developmental trajectories of biological cells via optimal transport Waddington-OT, Schiebinger, et. al. 2019"
  },
  {
    "objectID": "RiemannianStructureOriginal.html",
    "href": "RiemannianStructureOriginal.html",
    "title": "Formal Riemannian Structure of the Wasserstein metric",
    "section": "",
    "text": "Given a closed and convex space \\(X \\subseteq R^d\\), two probability measures on the same space, \\(\\mu, \\nu \\in \\mathcal{P}_2(X)\\), the 2-Wasserstein metric is defined as\n:\\(W_2(\\mu, \\nu) := \\min_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\left( \\int |x_1 - x_2|^2 \\, d\\gamma(x_1, x_2) \\right)^{1/2}\\)\nwhere \\(\\Gamma(\\mu, \\nu)\\) is the set of [[Kantorovich Problem|transport plans]] from \\(\\mu\\) to \\(\\nu\\). The Wasserstein metric is indeed a metric in the sense that it satisfies the desired properties of a distance function between probability measures on \\(\\mathcal{P}_2(X)\\). Moreover, the Wasserstein metric can be used to define a formal Riemannian metric on \\(\\mathcal{P}_2(X)\\). Such a formal metric structure allows one to define angles and lengths of vectors at each point in our ambient space."
  },
  {
    "objectID": "RiemannianStructureOriginal.html#tangent-space-induced-by-the-wasserstein-metric",
    "href": "RiemannianStructureOriginal.html#tangent-space-induced-by-the-wasserstein-metric",
    "title": "Formal Riemannian Structure of the Wasserstein metric",
    "section": "Tangent Space Induced by the Wasserstein Metric",
    "text": "Tangent Space Induced by the Wasserstein Metric\nA convenient way to formalize tangent vectors in this setting is to consider time derivatives of curves on the manifold. A tangent vector at a point \\(\\rho\\) would be the time derivative at 0 of a curve, \\(\\rho(t)\\), where \\(\\rho(0) = \\rho\\). Since we are dealing with a space of probability measures, additional restrictions need to be added in order to make our tangent space well-defined. For example, we would like our trajectory to satisfy the continuity equation \\(\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho v) = 0\\). There are many such vector fields that solve the continuity equation, so we can restrict to a vector field that minimizes kinetic energy, which is defined as \\(\\int \\rho|v|^2\\). This choice of tangent vectors is justified by the following lemma\n:Lemma A vector \\(v \\in L^2(\\rho; X)\\) belongs to the tangent cone at \\(\\rho\\) iff :\\(\\lVert v + w \\rVert \\ge \\lVert v \\rVert \\; \\forall w \\in L^2(\\rho; X) \\; \\mbox{such that} \\; \\nabla \\cdot (w\\rho) = 0\\)\nwhere we are taking the \\(L^2(\\rho, X)\\) norm. Divergence condition implies that our tangent vectors are equivalent up to a vector field with zero divergence. This implies that \\(v\\) is in fact a gradient of some function \\(u\\), in which case our continuity equation becomes\n:\\(\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\nabla u) = 0\\)\nThis is an elliptic partial differential equation, so one can apply tools used for that class of PDEs in order to determine existence and uniqueness of the tangent vectors."
  },
  {
    "objectID": "RiemannianStructureOriginal.html#riemannian-metric-induced-by-the-wasserstein-metric",
    "href": "RiemannianStructureOriginal.html#riemannian-metric-induced-by-the-wasserstein-metric",
    "title": "Formal Riemannian Structure of the Wasserstein metric",
    "section": "Riemannian Metric Induced by the Wasserstein Metric",
    "text": "Riemannian Metric Induced by the Wasserstein Metric\nGiven two tangent vectors at a point \\(\\rho\\) in our space, \\(\\mathcal{P}_2(X)\\), we can define the Riemannian metric as follows\n:\\(\\left\\langle \\frac{\\partial \\rho}{\\partial t_1}, \\frac{\\partial \\rho}{\\partial t_2} \\right\\rangle_\\rho = \\int \\rho \\langle \\nabla u_1, \\nabla u_2 \\rangle\\) Here, \\(\\frac{\\partial \\rho}{\\partial t_1}, \\frac{\\partial \\rho}{\\partial t_2}\\) are tangent vectors at \\(\\rho\\), and \\(u_1, u_2\\) are solutions to the modified continuity equation from the previous section. This metric defines an inner product at every point in our space, \\(\\mathcal{P}_2(X)\\). This not only allows one to define [[Geodesics and generalized geodesics|geodesics]] in this space, but the metric can be used to define calculus operators such as gradients and Hessians. These operators can be applied to in a similar manner to the same operators in finite dimensional Riemannian manifolds."
  },
  {
    "objectID": "RiemannianStructureOriginal.html#references",
    "href": "RiemannianStructureOriginal.html#references",
    "title": "Formal Riemannian Structure of the Wasserstein metric",
    "section": "References",
    "text": "References\n L. Ambrosio, N. Gigli,G. Savaré, Gradient Flows in Metric Spaces and in the Space of Probability Measures, p. 189-191 C. Villani, Topics in Optimal Transportation, p. 245-247 C. Villani, Topics in Optimal Transportation, p. 250-251"
  },
  {
    "objectID": "Analysis_in_Metric_Spaces.html",
    "href": "Analysis_in_Metric_Spaces.html",
    "title": "Analysis in Metric Spaces",
    "section": "",
    "text": "This article discusses several ideas which are foundational in the study of curves in metric spaces. In short, curves in a metric space can be given well-defined properties such as ‘length’ or ‘speed’, which generalize the classical notions from Euclidean space. These properties then allow one to define minimum-length curves (or geodesics), which then give a natural way to interpolate between points in a metric space. Beyond being useful in their own right, these ideas are widespread in optimal transport.\n\nCurves in Metric Spaces\nLet \\((X,d)\\) be a metric space. A curve in \\(X\\) is a function \\(\\gamma: [a,b] \\to X\\), where \\([a,b] \\subset \\mathbb{R}\\). The independent variable \\(t \\in [a,b]\\) is typically considered to represent time. Notice that a curve is therefore time-dependent by definition – that is, it is not merely a set of points in \\(X\\).\nA curve \\(\\gamma\\) is said to be continuous is it satisfies the usual definition: for all \\(\\epsilon &gt; 0\\) and for all \\(t,\\tau \\in [a,b]\\), there exists \\(\\delta = \\delta(t,\\epsilon) &gt; 0\\) such that \\(|t - \\tau| &lt; \\delta\\) implies \\(d(\\gamma(t),\\gamma(\\tau)) &lt; \\epsilon\\). Similarly, \\(\\gamma\\) is said to be uniformly continuous if \\(\\delta\\) can be chosen independently of \\(t\\).\nHowever, we will mainly be interested in a stronger notion of continuity in this article: absolute continuity. In the case \\(X = \\mathbb{R}\\), a function \\(f: [a,b] \\to \\mathbb{R}\\) is said to be absolutely continuous if there exists \\(g \\in L^1([a,b])\\) such that \\[\nf(s) - f(r) ~=~ \\int_r^s g(t) \\, dt\n\\] for all \\(r,s \\in [a,b]\\). In other words, \\(f\\) has a derivative almost everywhere (i.e. \\(g\\)) and is equal to the integral of such. This is the class of curves for which notions such as ‘length’ or ‘speed’ are well-behaved.\nIn a general metric space (\\(X \\neq \\mathbb{R}\\)), there is no way to define integrals or derivatives of curves, and so we must relax the definition above. In this setting, a curve \\(\\gamma: [a,b] \\to X\\) is said to be absolutely continuous if there exists \\(g \\in L^1([a,b])\\) such that \\[\nd(\\gamma(r),\\gamma(s)) ~\\leq~ \\int_r^s g(t) \\, dt\n\\] for all \\(a \\leq r \\leq s \\leq b\\). We emphasize again that this is a stronger notion than continuity or uniform continuity, and remark that the original definition can indeed be recovered when \\(X = \\mathbb{R}\\). The space of absolutely continuous curves from \\([a,b]\\) to \\(X\\) will be denoted \\(\\text{AC}([a,b],X)\\).\n\n\nLength and Speed\nWhile there is no way to define the derivative of a curve \\(\\gamma: [a,b] \\to X\\), there is a well-defined notion of speed: the metric derivative. For \\(\\gamma \\in \\text{AC}([a,b],X)\\), its metric derivative is defined as \\[\n|\\gamma'|(t) ~:=~ \\lim_{h \\to 0} \\frac{d(\\gamma(t),\\gamma(t+h))}{|h|} .\n\\] The metric derivative exists for a.e. \\(t \\in (a,b)\\), and is the minimal \\(g\\) that can be chosen in the definition of absolute continuity above [Theorem 9.2, Ambrosio et al. (2021)].\nThe length of \\(\\gamma\\) is then defined as \\[\n\\ell(\\gamma) ~:=~ \\int_a^b |\\gamma'|(t) \\, dt .\n\\] Observe that the above definitions imply that for any curve \\(\\gamma \\in \\text{AC}([a,b],X)\\), it holds that \\(\\ell(\\gamma) \\geq d(\\gamma(a),\\gamma(b))\\), i.e., that the length of \\(\\gamma\\) is greater than the distance between its endpoints. This should be intuitively obvious.\n\n\nReparameterizations\nAs remarked before, curves in a metric space are time-dependent by definition. However, it is often desirable to standardize this time-dependence as much as possible. For example, by reparameterizing the time variable, any curve \\(\\gamma \\in \\text{AC}([a,b],X)\\) can be transformed into a curve \\(\\tilde{\\gamma} \\in \\text{AC}([0,1],X)\\) with constant speed (which is then equal to its length).\nThis is done as follows. Let \\(\\gamma \\in \\text{AC}([a,b],X)\\) be a curve with time variable \\(t\\), metric derivative \\(|\\gamma'|(t)\\), and length \\(\\ell(\\gamma)\\). Define the function \\[\n\\sigma(t) ~:=~ \\frac{1}{\\ell(\\gamma)} \\int_a^t |\\gamma'|(s) \\, ds .\n\\] The function \\(\\sigma(t)\\) represents the fraction of \\(\\gamma\\) which has been traversed by time \\(t\\). Since \\(|\\gamma'|\\) is integrable and nonnegative, \\(\\sigma\\) is an absolutely continuous and monotone nondecreasing function, and thus admits a right inverse \\(\\sigma^{-R}\\). We then define the new time variable \\(\\tau := \\sigma(t)\\) and the reparameterized curve \\(\\tilde{\\gamma}\\) by \\[\n\\tilde{\\gamma}(\\tau) ~:=~ \\gamma(\\sigma^{-R}(\\tau)) ~=~ (\\gamma \\circ \\sigma^{-R})(\\tau) .\n\\] It can then be verified that \\(\\tilde{\\gamma}\\) is an absolutely continuous curve on the interval \\([0,1]\\) with \\(\\tilde{\\gamma}(0) = \\gamma(a)\\), \\(\\tilde{\\gamma}(1) = \\gamma(b)\\), and \\(|\\tilde{\\gamma}'| \\equiv \\ell(\\tilde{\\gamma}) = \\ell(\\gamma) = \\text{constant}\\) [Proposition 9.6, Ambrosio et al. (2021)].\nIt is then often conventient to work in the space of constant-speed absolutely continuous curves on \\([0,1]\\) rather than in the spaces \\(\\text{AC}([a,b],X)\\).\n\n\nGeodesics\nAs remarked before, for any curve \\(\\gamma \\in \\text{AC}([a,b],X)\\), it holds that \\(\\ell(\\gamma) \\geq d(\\gamma(a),\\gamma(b))\\). Any curve which achieves equality here (i.e. for which \\(\\ell(\\gamma) = d(\\gamma(a),\\gamma(b))\\)) is termed a geodesic. Geodesics are thus curves of minimal length between their endpoints, and in this way, generalize the notion of ‘straight line’ from Euclidean space to a general metric space. (In Euclidean space, ‘straight’ typically evokes a notion of angle, however, there is no notion of angle in a general metric space. Therefore, this generealization relies on the variational characterization of straight lines as the shortest paths between their endpoints.)\nJust as straight lines are a natural way to interpolate between points in a Euclidean space, geodesics are a natural way to interpolate between points in a metric space. We are thus led to the following question: Given two points \\(a,b\\) in a metric space \\((X,d)\\), does a geodesic between them always exist, and if so, is it unique? The answer is generically ‘no’, and this can fail in many ways:\n\nIf the space is not path-connected, then curves connecting \\(a\\) and \\(b\\) may not exist at all.\nCurves connecting \\(a\\) and \\(b\\) may exist, but there may not be one of minimum length (e.g. \\(\\mathbb{R}^2 \\backslash \\{ 0 \\}\\) with \\(a = -b\\)).\nA minimal curve connecting \\(a\\) and \\(b\\) may exist, but may have length strictly larger than \\(d(a,b)\\) (e.g. the unit circle \\(\\mathbb{S}^1\\) embedded in \\(\\mathbb{R}^2\\) with the inherited metric).\nEven when geodesics do exist, they are not always unique (e.g. the unit sphere \\(\\mathbb{R}^2\\) with \\(a,b\\) antipodal points).\n\nWe are thus led to make the following definitions. A metric space \\((X,d)\\) is said to be a length space if for all \\(x,y \\in X\\), \\[\nd(x,y) ~=~ \\inf \\{ \\ell(\\gamma) ~:~ \\gamma \\in \\text{AC}([a,b],X) ,~ \\gamma(a) = x ,~ \\gamma(b)=y \\}.\n\\] It is said to be a geodesic space if there exists such a \\(\\gamma\\) achieving the infimum (i.e. a geodesic). It is said to be a uniquely geodesic space if such a \\(\\gamma\\) is unique.\nThere are a large number of results in differential and metric geometry establishing necessary and sufficient conditions for a space to have these properties.\n\n\n\n\n\nReferences\n\nAmbrosio, Luigi, Elia Brué, Daniele Semola, et al. 2021. Lectures on Optimal Transport. Vol. 130. Springer."
  },
  {
    "objectID": "ArticlesToRevise.html",
    "href": "ArticlesToRevise.html",
    "title": "Suggestions of Articles to Revise",
    "section": "",
    "text": "Below, you can find a list of suggestions for articles to revise.\nIf you choose to revise one of these articles, let me know, and I will remove it from the list below.\n\nOptimal Transport in One Dimension - Yusen will revise - This article could benefit from a major reorganization, adding/removing some sections (such as the linear cost example), and stating the main theorem in a higher level of generality, as in Santambrogio Theorem 2.9 p63 of Optimal Transport for Applied Mathematicians\nKantorovich Dual Problem for General Costs - This article could benefit from editing throughout. It would be good to expand the final section on c-convcave functions, stating and explaining Santambrogio Proposition 1.11 p12 of Optimal Transport for Applied Mathematicians\nKantorovich Dual Problem (for c(x,y) = d(x,y)^2 where d is a metric) - Evan Tufte - This article could benefit from editing throughout, especially regarding the latex formatting. The condition that mu does not give mass to small sets needs to be corrected. This article should more thoroughly reference the other article on the Kantorovich dual probelm. It should prove the relationship between c-concave functions and convex conjugates, as in Santambrogio’s Proposition 1.21, p16 of Optimal Transport for Applied Mathematicians. The relationship between (DP) and (DP-var), as Santambrogio refers to them, needs to be explained. (You do not need to use the notation DP and DP-var.)\nWasserstein barycenters and applications in image processing - Charles will revise - This article could benefit from editing throughout, with more precise statements of the main results. It should also be revised to mention recent work on barycenters with positive and negative weights.\nDiscrete Optimal Transport - Charles will revise - This article could benefit from editing throughout. Several sections just have a few words as a placeholder and could be expanded. (The dual problem should be stated explicitly and a reference should be added to the wiki article on the dual problem.) The latex formatting could be simplified and made easier to read. Everything could be explained for general costs, rather than quadratic costs. The results of Exercise 25 could be included.\nSinkhorn’s Algorithm - Jack Pfaffinger will revise - This article could benefit from editing throughout. The wiki article on discrete optimal transport should be referenced. The interpretation of the Sinkhorn algorithm in terms of the corresponding dual probalm and in terms of convex projections should be explained. (See Peyre and Cuturi)\nSliced Wasserstein Distance - Arie - This article could benefit from editing throughout. Recent contributions by Kitagawa and Takatsu and Park and Slepcev should be summarized.\nGradient flows on Hilbert Spaces - The organization of this article could be greatly improved. Currently, there is an over emphacsis on the Moreau Yosida regularization. Also, the other wiki article on the MY regularization should be cited. The latex formatting could be improved throughout. The statements in the Examples and Applications section could be made more rigorous and more organized."
  },
  {
    "objectID": "entropic_regularization_and_sinkhorn.html",
    "href": "entropic_regularization_and_sinkhorn.html",
    "title": "Entropic Regularization and Sinkhorn",
    "section": "",
    "text": "Sinkhorn’s Algorithm is an iterative numerical method used to obtain an optimal transport plan \\(\\pi \\in \\Gamma(\\alpha,\\beta)\\) for the Kantorovich Problem with entropic regularization in the case of finitely supported positive measures \\(\\alpha, \\beta \\in \\mathcal M_{+}(X)\\). For further reading see (Peyré and Cuturi 2018) (pg. 62-73).\n\nContinuous Problem Formulation\nEntropic regularization modifies the Kantorovich problem by adding a Kullback-Leibler divergence term to the optimization goal. Specifically, the general form of the problem is now to determine\n\\[\nL_c^\\epsilon (\\alpha, \\beta) = \\inf_{\\pi \\in \\Gamma(\\alpha, \\beta)} \\underbrace{\\int_{X \\times Y} c(x, y) \\, d\\pi(x, y)}_{\\text{Kantorovich functional } \\mathbb{K}(\\pi)} + \\epsilon \\underbrace{\\text{KL}(\\pi \\mid \\alpha \\otimes \\beta)}_{\\text{entropic term}}\n\\]\nwhere \\(\\alpha\\otimes\\beta\\) is the product measure of \\(\\alpha\\) and \\(\\beta\\), and where\n\\[ \\operatorname{KL}(\\mu\\mid\\nu) = \\int_X \\log\\left(\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\nu}(x) \\right) \\mathrm{d}\\mu(x) + \\int_X (\\mathrm{d}\\nu(x) - \\mathrm{d}\\mu(x)) \\]\nwhenever the Radon-Nikodym derivative \\(\\tfrac{\\mathrm{d}\\mu}{\\mathrm{d}\\nu}\\) exists (i.e. when \\(\\mu\\) is absolutely continuous w.r.t. \\(\\nu\\)) and \\(+\\infty\\) otherwise. This form of the KL divergence is applicable even when \\(\\mu,\\nu \\in \\mathcal{M}_+(X)\\) differ in total mass and it reduces to the standard definition whenever \\(\\mu\\) and \\(\\nu\\) have equal total mass. From this definition it immediately follows that for \\(\\epsilon &gt; 0\\) an optimal coupling \\(\\pi^*\\) must be absolutely continuous w.r.t \\(\\alpha \\otimes \\beta\\). As a result, the optimal plan is in some sense less singular and hence “smoothed out.”\n\n\nDiscrete Problem Formulation\nTo apply Sinkhorn’s algorithm to approximate \\(L^{\\epsilon}_c (\\alpha,\\beta)\\), it will be necessary to assume finite support so let \\(\\alpha = \\textstyle\\sum_{i=1}^{n} a_i \\delta_{x_i}\\) and \\(\\beta = \\textstyle\\sum_{j=1}^m b_j \\delta_{y_j}\\) and denote the corresponding vector of weights by \\(\\mathbf{a} \\in \\mathbb{R}_{+}^n\\) and \\(\\mathbf{b} \\in \\mathbb{R}_{+}^m\\). Additionally let \\(C_{ij} = c(x_i, y_j)\\) and denote the discrete version of \\(\\Gamma(\\alpha,\\beta)\\) by \\(U(a,b)= \\{ P \\in \\mathbb{R}^{n \\times m} \\mid \\textstyle\\sum_{j} P_{ij}=a_i, \\textstyle\\sum_{i} P_{ij} = b_j \\}\\).\nThe discrete entropy of a coupling matrix is defined as:\n\\[\nH(P) := -\\sum_{i,j}P_{i,j}(\\log(P_{i,j}) - 1)\n\\]\nwith an analogous definition for vectors.\nThis lets us write the entropically regularized Kantorovich problem as:\n\\[L^{\\epsilon}_{C}(\\mathbf{a},\\mathbf{b}) = \\inf_{P\\in U(\\mathbf{a},\\mathbf{b})} \\sum_{i,j} C_{i,j} P_{i,j} -\\epsilon H(P).\\]\nAdding the entropy regularization guarantees uniqueness of the solution, and makes the solutions “less sparse” as \\(\\epsilon\\) increases.\nThe unique solution \\(P^{\\epsilon}\\) of the entropically regularized Kantorovich problem converges to the optimal solution of the unregularized Kantorovich problem which has maximal entropy as \\(\\epsilon \\rightarrow 0\\). In other words,\n\\[\nP_{\\epsilon} \\xrightarrow{\\varepsilon \\to 0} \\underset{P \\in U(\\mathbf{a}, \\mathbf{b})}{\\operatorname{argmax}} \\{ H(P) : \\sum_{i, j} P_{i,j}C_{i,j} = L_{C}(\\mathbf{a}, \\mathbf{b}) \\}\n\\]\nwhere \\(L_{C}(\\mathbf{a}, \\mathbf{b})\\) is the minimal cost of the unregularized Kantorovich problem. Thus we can see\n\\[\nL^{\\epsilon}_C(\\mathbf{a}, \\mathbf{b}) \\xrightarrow{\\varepsilon \\to 0} L_{C}(\\mathbf{a}, \\mathbf{b}).\n\\]\nand\n\\[\nP_{\\epsilon} \\xrightarrow{\\varepsilon \\to 0} a \\otimes b = ab^{T}.\n\\]\nSee here for more on Discrete Optimal Transport.\n\n\nCharacterizing the Solution\nThe solution to the discrete entropically regularized problem formulation is unique and has a special form. We can define the Kullback-Leibler divergence between couplings as\n\\[\\operatorname{KL}(P\\mid K) := \\sum_{i,j} P_{ij} \\log\\left(\\frac{P_{i,j}}{K_{i.j}}\\right) + - P_{i,j} + K_{i, j}.\\]\nTheorem (Peyré and Cuturi (2018), Proposition 4.3 on pg. 63):\nThe solution \\(P \\in \\mathbb{R}^{n\\times m}\\) to discrete regularized Kantorovich problem is unique and has the form \\(P_{ij} = u_iK_{ij}v_j\\) for some \\(\\mathbf{u} \\in \\mathbb{R}^n, \\mathbf{v} \\in \\mathbb{R}^m\\) where \\(K_{ij}=e^{-C_{ij}/\\epsilon}\\). Moreover, \\(\\mathbf u\\) and \\(\\mathbf v\\) are unique up to multiplication and division by some scaling factor.\nIn fact, the optimal solution \\(P_{\\epsilon}\\) of \\(L_{c}^{\\epsilon}(\\mathbf{a}, \\mathbf{b})\\) satisfies:\n\\[\nP_{\\epsilon} = \\underset{P \\in U(\\mathbf{a}, \\mathbf{b})}{\\operatorname{argmin}} KL(P| K)\n\\]\ni.e. \\(P^{\\epsilon}\\) is the admissible coupling which has smallest KL-divergence with respect to \\(K\\).\n\n\nSinkhorn’s Algorithm\nSinkhorn’s algorithm takes advantage of the aforementioned characterization result to iteratively approximate the scaling factors \\(\\mathbf u\\) and \\(\\mathbf v\\). The procedure is simple and only involves matrix-vector multiplication and entrywise division as follows\n\\[ u^{(\\ell +1)} = \\frac{\\textbf a}{K\\textbf v^{(\\ell)}} \\quad\\text{and}\\quad v^{(\\ell +1)} = \\frac{\\textbf a}{K^T\\textbf u^{(\\ell+1)}} \\]\nWhere \\(\\mathbf{v}^{(0)}\\) is initialized arbitrarily as a vector of all \\(1's\\). This problem is known known in the numerical analysis community as a matrix scaling problem.\nOnce a sufficient number of iterations \\(L\\) have been taken, we let \\(\\hat P_{ij}=u^{(L)}_i K_{i,j} v^{(L)}_j\\) be our approximation of the optimal plan.\n\n\n\n\n\nReferences\n\nPeyré, Gabriel, and Marco Cuturi. 2018. “Computational Optimal Transport.” arXiv Preprint. https://arxiv.org/abs/1803.00567."
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html",
    "href": "Knothe_Maps_Conditional_Sampling.html",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "",
    "text": "A core problem of modern sampling is the approximation of high complexity distributions on \\(\\mathbb{R}^d\\). If \\(\\mu \\in \\mathcal{P}(\\mathbb{R}^d)\\) is some distribution of interest, and \\(\\nu \\in \\mathcal{P}(\\mathbb{R}^d)\\) is an easily computable reference distribution (typically a Gaussian), a transport map \\(t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\) satisfying \\(\\nu = t \\# \\mu\\) provides a link between distributions that can potentially allow for easier density estimation (Tabak and Turner 2013), inference (El Moselhy and Marzouk 2012), and generative modeling, particularly exhibited in WGANs.\nOptimal transport provides a widely used approach for computing such maps \\(t\\) through minimization of a relevant cost. However, the resulting map may not be amenable to fast computation, which is vitally important in some modern applications. Knothe maps provide a closely connected approach with certain benefits for a class of sampling problems. We focus on the efficient computation of Knothe maps, following the development of a recent algorithmic approach (Baptista, Marzouk, and Zahm 2023)."
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html#definition",
    "href": "Knothe_Maps_Conditional_Sampling.html#definition",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "Definition",
    "text": "Definition\nA Knothe-Rosenblatt map (Davis, Lii, and Politis 2011) is a transport map \\(S : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\) that decomposes component-wise into the form:\n\\[\nS(\\mathbf{x}) = \\begin{bmatrix} S_1(x_1) \\\\ S_2(x_1, x_2) \\\\ \\vdots \\\\ S_d(x_1, \\dots, x_d)\\\\ \\end{bmatrix}\n\\] Each component function \\(S_i\\) may only depend on \\(\\mathbf{x}_{\\leq i} = (x_1, \\dots, x_i)\\) and must be increasing with respect to \\(x_i\\). The triangular structure of Knothe-Rosenblatt maps allows for efficient computation of both the inverse and the Jacobian, improving computational speed, while the increasing condition ensures \\(S_i\\) is an increasing transport map between the marginals \\(\\mu(x_i \\vert \\mathbf{x}_{&lt;i})\\) and \\(\\nu(x_i \\vert \\mathbf{x}_{&lt;i})\\), which allows for direct connection with conditional sampling.\nIn particular, for Jacobian computation, we have the explicit formula:\n\\[\n\\vert \\det \\nabla S(\\mathbf{x}) \\vert = \\prod_{i=1}^d \\partial_i S_i(\\mathbf{x}_{\\leq i})\n\\] Therefore, computation of the determinant of the Jacobian relies upon fast evaluation of partials for the increasing part of each component function, which is relatively quick to compute.\nIf \\(\\mu \\in \\mathcal{P}(\\mathbb{R}^d)\\) is absolutely continuous with respect to \\(\\nu \\in \\mathcal{P}(\\mathbb{R}^d)\\) with \\(\\nu\\) Gaussian, the Knothe-Rosenblatt map exists and is unique almost everywhere. (Bogachev, Kolesnikov, and Medvedev 2005)"
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html#convergence-properties",
    "href": "Knothe_Maps_Conditional_Sampling.html#convergence-properties",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "Convergence Properties",
    "text": "Convergence Properties\nOur eventual goal is to learn a close approximation of the Knothe-Rosenblatt map. Thus it is beneficial to examine convergence of maps \\(S\\) to the unique Knothe-Rosenblatt map \\(S_{KR}\\), assuming \\(\\nu\\) is a standard Gaussian on \\(\\mathbb{R}^d\\).\nKL divergence is an extremely popular statistical measure for quantification of the distance between two probability distributions in generative modeling. For the Knothe-Rosenblatt map, any triangular map \\(S\\) converges to \\(S_{KR}\\) in \\(L^2_{\\mu}\\) when \\(\\mathcal{D}_{KL}(\\mu \\vert \\vert S \\# \\nu) \\rightarrow 0\\).\nDefine the functional:\n\\[\n\\mathcal{J}_i(s) = \\int \\left( \\frac 1 2 s(\\mathbf{x}_{\\leq i})^2 - \\log \\vert \\partial_i s(\\mathbf{x}_{\\leq i}) \\vert  \\right) d\\mu\n\\] By decomposition of the Gaussian, one can write:\n\\[\n\\mathcal{D}_{KL}(\\mu \\vert \\vert S \\# \\nu) = \\sum_{i=1}^d \\mathcal{J}_i(S_i) - \\mathcal{J}_i(S_{KR,i})\n\\] For a given sample of data \\(\\{\\mathbf{X}^k\\}_{k=1}^n\\) from \\(\\mu\\), minimization of the above leads to a maximum likelihood estimator \\(\\hat{S}\\) for \\(S_{KR}\\):\n\\[\n\\hat{S} = \\arg \\max \\sum_{k=1}^n \\log S \\# \\nu (\\mathbf{X}^k)\n\\]\nwhere \\(S\\) are maximized over the space of triangular maps with \\(\\partial_i S_i &gt; 0\\) for all \\(i\\). This generates a convex optimization problem that can be parallelized in each component, as each \\(\\mathcal{J}_i\\) functional is independent."
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html#rectification-operator",
    "href": "Knothe_Maps_Conditional_Sampling.html#rectification-operator",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "Rectification Operator",
    "text": "Rectification Operator\nThe monotone condition on \\(S\\) is necessary for the formulation of triangular maps, but is inconvenient for learning, as most functional bases do not conveniently allow for this constraint to be satisfied. To solve this issue and allow for the use of standard functional bases, a rectifier functional \\(\\mathcal{R}\\) must be introduced, defined component-wise as the following:\n\\[\n\\mathcal{R}_i(f)(\\mathbf{x}_{\\leq i}) = f(\\mathbf{x}_{&lt;i},0) + \\int_0^{x_i} g\\left(\\partial_i f(\\mathbf{x}_{&lt;i}, t)\\right) dt\n\\]\nIf \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}^+\\) is a positive function, this transforms any \\(f\\) of sufficient smoothness into a function satisfying the increasing condition in the last variable component-wise.\n\n\n\nAn example of the action of the rectifier on a class of functions. This utilizes \\(g(x) = \\log(1 + \\exp(x))\\). As the function changes, corresponding changes to rectifier value are evident.\n\n\nThen the core component-wise learning of a Knothe-Rosenblatt map relies on the optimization problem:\n\\[\n\\min_{f\\in V_i} \\mathcal{J}_i(\\mathcal{R}_i(f))\n\\] where \\(V_i\\) is a suitable linear space of sufficiently smooth functions.\nUnder some additional assumptions on the choice of \\(g\\), and assuming the tails of the target distribution \\(\\mu\\) are Gaussian, there exists a unique global minimizer for the optimization problem using this rectifier, and the optimization landscape is relatively favorable. For a full treatment of the theoretical argument, refer to Section 3 of (Baptista, Marzouk, and Zahm 2023)."
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html#functional-basis-expansion",
    "href": "Knothe_Maps_Conditional_Sampling.html#functional-basis-expansion",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "Functional Basis Expansion",
    "text": "Functional Basis Expansion\nFor each \\(i\\), we approximate \\(f\\) by a particular chosen basis for a suitable function space. In particular, we represent:\n\\[\nf(\\mathbf{x}_{\\leq i}) = \\sum_{\\alpha \\in \\Lambda} c_{\\alpha} \\psi_{\\alpha}(\\mathbf{x}_{\\leq i})\n\\] In the above, \\(\\Lambda\\) forms the set of multi-indices \\(\\alpha = (\\alpha_1, \\alpha_2, \\dots, \\alpha_i) \\in \\mathbb{N}^i\\) with \\(\\vert\\Lambda \\vert = m\\), with \\(c_\\alpha \\in \\mathbb{R}\\) coefficients and \\(\\psi_{\\alpha} : \\mathbb{R}^i \\rightarrow \\mathbb{R}\\) basis functions for \\(V_i\\).\nThe basis functions \\(\\psi_{\\alpha}\\) are constructed as products of single variable functions:\n\\[\n\\psi_{\\alpha}(\\mathbf{x}_{\\leq i}) = \\prod_{j=1}^i \\psi^j_{\\alpha_j}(x_j)\n\\]\nSuitable choices for functional spaces include Hermite polynomials and Ricker wavelets, which provide expressive one- or two-parameter families of functions with well-known approximation properties that are suitable to many transport problems."
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html#multi-index-set-construction",
    "href": "Knothe_Maps_Conditional_Sampling.html#multi-index-set-construction",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "Multi-Index Set Construction",
    "text": "Multi-Index Set Construction\nNote that the richness of the functional basis is directly tied to the size \\(m\\) of the multi-index set \\(\\Lambda\\). Thus controlling this size gives a natural method to learn several different hierarchical expansions at varying levels of complexity. However, choosing which components to spend this finite budget of complexity on is a nontrivial problem.\nFortunately, a greedy algorithm is well suited to this task, as we have easily computable heuristics to inform each choice. In particular, we define \\(\\Lambda_0 = \\emptyset\\) and iteratively update:\n\\[\n\\Lambda_{t+1} = \\Lambda_t \\cup \\{\\alpha_t^*\\}\n\\] There are several necessary conditions for \\(\\alpha_t^*\\). First, \\(\\alpha_t^* \\notin \\Lambda_t\\) must hold. Second, we wish for the basis approximation to add complexity in a orderly manner, rather than adding random basis functions of high degree when one of lower degree will suffice. Thus we constrain the set \\(\\Lambda_t\\) to be downward closed:\n\\[\n\\alpha \\in \\Lambda_t \\text{ and } \\alpha' \\leq \\alpha \\implies \\alpha' \\in \\Lambda_t\n\\] We next define the margin of the set \\(\\Lambda_t\\). Let \\(e_j\\) denote the \\(j\\)-th canonical basis vector of \\(\\mathbb{N}^i\\):\n\\[\n\\Lambda_t^M = \\{\\alpha \\notin \\Lambda_t : \\exists j &gt; 0, \\alpha - e_j \\in \\Lambda_t\\}\n\\] Intuitively this means that the margin is any multi-index that is one shift away, in any direction, from the set \\(\\Lambda_t\\). The reduced margin is stronger in that the multi-index must be one shift away not in one, but in every direction:\n\\[\n\\Lambda_t^{RM} = \\{\\alpha \\notin \\Lambda_t : \\forall j &gt; 0, \\alpha - e_j \\in \\Lambda_t\\}\n\\] For our greedy algorithm, we add multi-indices via the following optimization procedure:\n\\[\n\\alpha_t^* \\in \\arg \\max_{\\alpha \\in \\Lambda_t^{RM}} \\vert \\nabla_{\\alpha} \\mathcal{J}_i(\\mathcal{R}_i(f_i)) \\vert\n\\]\nThat is, evaluate the derivative of the loss in the direction of each basis function under consideration, and choose to add the index with the largest derivative.\nBelow we illustrate this entire process for an example multi-index set:\n\n\n\nAn example of the hierarchical building of the set \\(\\Lambda_t\\) over time. At each iteration, the derivative of the loss is evaluated for all reduced margin squares (orange), and the one with highest magnitude is selected (green) and added to the next level of the set \\(\\Lambda_{t+1}\\) (blue)."
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html#adaptive-transport-map-algorithm",
    "href": "Knothe_Maps_Conditional_Sampling.html#adaptive-transport-map-algorithm",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "Adaptive Transport Map Algorithm",
    "text": "Adaptive Transport Map Algorithm\nUsing the above pieces, we can write the full algorithm for learning a map component \\(S_i\\) approximating the Knothe-Rosenblatt map component \\(S_{KR,i}\\):\nAlgorithm 1 (Baptista, Marzouk, and Zahm 2023): Estimate map component \\(S_i\\)\nInput: Training sample \\(\\{X^k_{1:i}\\}_{k=1}^{n}\\), cardinality \\(m\\) for \\(\\Lambda_t\\)\nInitialize: \\(\\Lambda_0 = \\emptyset\\), \\(f_0 = 0\\)\nfor \\(t = 0, \\dots, m-1\\):\n\nConstruct the reduced margin: \\(\\Lambda_t^{RM}\\)\nSelect the new multi-index:\n\n\\[\n\\alpha_t^* \\in \\arg\\max_{\\alpha \\in \\Lambda_t^{RM}} |\\nabla_{\\alpha} \\mathcal{J}_i(\\mathcal{R}_i(f_t))|\n\\]\n\nUpdate the active set: \\(\\Lambda_{t+1} = \\Lambda_t \\cup \\{\\alpha_t^*\\}\\)\nUpdate the approximation:\n\n\\[\nf_{t+1} = \\arg\\min_{f \\in \\text{span}\\{\\psi_{\\alpha} : \\alpha \\in \\Lambda_{t+1}\\}} \\mathcal{J}_i(\\mathcal{R}_i(f))\n\\]\nOutput: \\(\\hat{S}_i = R_i(f_m)\\)\nIn practice, the value of \\(m\\) can be considered a hyperparameter and optimized through cross-validation."
  },
  {
    "objectID": "Knothe_Maps_Conditional_Sampling.html#algorithm-application",
    "href": "Knothe_Maps_Conditional_Sampling.html#algorithm-application",
    "title": "Knothe Maps and Conditional Sampling",
    "section": "Algorithm Application",
    "text": "Algorithm Application\nMatlab code for learning Knothe-Rosenblatt maps is available on GitHub. From this repository, quick replication and extension of experiments is possible. Below we showcase the standard sampling problem: given \\(1000\\) points sampled from a true pdf, and using a reference measure that is standard Gaussian, approximate the true pdf by learning an approximation to the Knothe-Rosenblatt map.\n\n\n A total of \\(1000\\) sampled points from the given true distribution.\n\n\n The approximated distribution using the learned triangular map with Hermite polynomial basis."
  },
  {
    "objectID": "sliced_wasserstein_metric.html",
    "href": "sliced_wasserstein_metric.html",
    "title": "Sliced-Wasserstein Distance",
    "section": "",
    "text": "The sliced Wasserstein distance \\({\\displaystyle SW_{p}}\\) is an alternative distance metric between probability measures that integrates Wasserstein distances over 1-dimensional projections of measures in \\(\\mathbb{R}^d\\). It shares many properties with the Wasserstein distance while being computationally simpler. For further reading see (Santambrogio 2015; Peyré and Cuturi 2020)"
  },
  {
    "objectID": "sliced_wasserstein_metric.html#hilbertian-property-of-displaystyle-sw_p",
    "href": "sliced_wasserstein_metric.html#hilbertian-property-of-displaystyle-sw_p",
    "title": "Sliced-Wasserstein Distance",
    "section": "Hilbertian Property of \\({\\displaystyle SW_{p}}\\)",
    "text": "Hilbertian Property of \\({\\displaystyle SW_{p}}\\)\nThere is an isometric embedding of \\((\\mathcal{P}_p(\\mathbb{R}^d),SW_p)\\) into a Hilbert space which demonstrates that \\(SW_p\\) is a Hilbertian metric. This is a special property of \\(SW_p\\) which is not satisfied by \\(\\mathcal{W}_p\\), which is generally not Hilbertian on \\(\\mathbb{R}^d\\) with \\(d &gt; 1\\) (Peyré and Cuturi 2020).\nWe now demonstrate the Hilbertian nature of \\(SW_p\\) via an isometric embedding of \\((\\mathcal{P}_p(\\mathbb{R}^d),SW_p)\\) into an \\(L^p\\) space. Recall the pseudoinverse-CDF formulation of Wasserstein distance in \\(\\mathbb{R}\\), which tells us that \\[\\mathcal{W}_p(P_{\\theta \\#}\\mu, P_{\\theta \\#}\\nu)^p = \\int_{0}^1 |\\mathcal{C}_{P_{\\theta \\#}\\mu}^{\\dagger}(r) -\\mathcal{C}_{P_{\\theta \\#}\\nu}^\\dagger(r)|^pdr\\] which implies that \\[SW_p(P_{\\theta \\#}\\mu, P_{\\theta \\#}\\nu)^p = \\int_{\\mathbb{S}^{d-1}}\\int_{0}^1 |\\mathcal{C}_{P_{\\theta \\#}\\mu}^{\\dagger}(r) -\\mathcal{C}_{P_{\\theta \\#}\\nu}^\\dagger(r)|^pdrd\\theta.\\] This tells us that the map \\(F: \\mathcal{P}_p(\\mathbb{R}^d)\\rightarrow L^p(\\mathbb{S}^{d-1} \\times [0,1])\\) defined by \\(F(\\mu): (\\theta,r) \\mapsto \\mathcal{C}_{P_{\\theta \\#}\\mu}^{\\dagger}(r)\\) is an isometric embedding for \\(\\mathcal{P}_p(\\mathbb{R}^d)\\) into \\(L^p(\\mathbb{S}^{d-1} \\times [0,1])\\), which means that \\(SW_p\\) is a Hilbertian metric.\nThis embedding allows one to construct an analogue of displacement interpolation between measures \\(\\mu\\) and \\(\\nu\\). This is done via a gradient flow on the function \\(\\sigma \\mapsto SW_p(\\nu,\\sigma)^p\\) with initial condition \\(\\mu_0 = \\mu\\), as long as such a flow converges to \\(\\nu\\). In the case of \\(SW_2\\), numerical simulations have shown that the flow indeed converges to \\(\\nu\\) (Peyré and Cuturi 2020)."
  },
  {
    "objectID": "sliced_wasserstein_metric.html#topological-equivalence-of-displaystyle-sw_p-and-displaystyle-w_p-on-compact-domains",
    "href": "sliced_wasserstein_metric.html#topological-equivalence-of-displaystyle-sw_p-and-displaystyle-w_p-on-compact-domains",
    "title": "Sliced-Wasserstein Distance",
    "section": "Topological Equivalence of \\({\\displaystyle SW_p}\\) and \\({\\displaystyle W_p}\\) on compact domains",
    "text": "Topological Equivalence of \\({\\displaystyle SW_p}\\) and \\({\\displaystyle W_p}\\) on compact domains\nOn compact domains \\(\\Omega \\subset \\mathbb{R}\\), the topologies on \\((\\mathcal{P}_p(\\Omega), SW_p)\\) and \\((\\mathcal{P}_p(\\Omega), W_p)\\) are the same. To see this, first notice that \\({\\displaystyle SW_p(\\mu ,\\nu )\\leq W_p(\\mu ,\\nu )}\\) from the property that \\(\\displaystyle W_p(P_{\\theta \\#}\\mu ,P_{\\theta \\#}\\nu )\\leq W_p(\\mu ,\\nu ).\\) This implies that convergence of a sequence in \\({\\displaystyle W_p}\\) implies convergence in \\({\\displaystyle SW_p}\\), i.e the identity map on \\({\\displaystyle {\\mathcal {P}}_p(\\mathbb {R} ^{d})}\\) is \\({\\displaystyle W_p}\\)-to-\\({\\displaystyle SW_p}\\)-continuous. Moreover, since \\(\\Omega\\) is compact it follows that \\({\\displaystyle ({\\mathcal {P}}_p(\\Omega ),W_p)}\\) is compact, and since \\((\\mathcal{P}_p(\\Omega), SW_p)\\) is a metric space it is Hausdorff. We conclude that the identity map from \\((\\mathcal{P}_p(\\Omega), W_p)\\) to \\((\\mathcal{P}_p(\\Omega), SW_p)\\) is a continuous bijection from a compact space to a Hausdorff space, and such a map must be a homeomorphism. This shows that \\((\\mathcal{P}_p(\\Omega), W_p)\\) and \\((\\mathcal{P}_p(\\Omega), SW_p)\\) are equivalent from a topological perspective.\nThis result does not easily generalize beyond compact domains. In fact, it is known that \\((\\mathcal{P}_p(\\mathbb{R}^d), W_p)\\) and \\((\\mathcal{P}_p(\\mathbb{R}^d), SW_p)\\) have distinct topologies (Bayraktar and Guo 2021)."
  },
  {
    "objectID": "AuctionAlgorithm.html",
    "href": "AuctionAlgorithm.html",
    "title": "Auction Algorithm",
    "section": "",
    "text": "The auction algorithm is an algorithm in optimal transport in which a set of buyers exchange goods for varied prices until an eventual equilibrium is reached. It is an iterative approach. The algorithm pertains to the discrete formulation of optimal transport, as well as provides a connection to the dual problem. The algorithm is useful in the field of economics because of its ability to find an equilibrium. The algorithm was invented by Bertsekas, but it was eventually updated."
  },
  {
    "objectID": "AuctionAlgorithm.html#the-assignment-problem",
    "href": "AuctionAlgorithm.html#the-assignment-problem",
    "title": "Auction Algorithm",
    "section": "The Assignment Problem",
    "text": "The Assignment Problem\nIt is necessary to introduce the assignment problem because it applies a context in which we may apply our algorithm to find such an optimal equilibrium. Suppose we have both \\(N\\) buyers as well as \\(N\\) goods. We introduce \\(a_{ij}\\) to quantify the notion of some sort of utility, benefit, or happiness the buyer receives from their corresponding good. The assignment problem therefore seeks a way to maximize \\(\\sum_i a_{i \\sigma(i)}\\), i.e., we hope to maximize the total utility. Note this is different from maximizing the utility of a particular buyer, because we seek to benefit the whole group the most. We use the index \\(i\\) to denote a particular buyer we use the second index \\(j = \\sigma(i)\\) to denote the good, where \\(\\sigma\\) is some permutation of the goods among all of the buyers. A final thing to note is that the assignment of people to goods is one-to-one, i.e. there is one distinct good for every distinct buyer.\nWe’ve established what the aim of the assignment problem is, but we have yet to establish a sense of equilibrium that the auction algorithm hopes to achieve. First, we must define a price system. We say that a good \\(j\\) has price \\(p_j\\), which can be rewritten \\(p = (p_j)_j\\). Next, we define the equilibrium condition. The equilibrium is that all buyers are content with their purchases if\n a_{i(i)} - p_{(i)} = { a*{ij} - p_j }           (*) \nis satisfied. It is important to note that the absolute value is not taken. If the absolute value is taken, then it is possible for the system to be in equilibrium when each buyer has the worst possible good, and \\(\\sum_i a_{i \\sigma(i)}\\) fails to be maximized. Now, another common way to say that the system is in equilibrium is that all of the buyers are “happy.” Notationally, we say \\((p, \\sigma)\\) is an equilibrium. If this is an equilibrium, then \\(\\sigma\\) is an optimal assignment, and \\(p\\) is optimal in the dual problem."
  },
  {
    "objectID": "AuctionAlgorithm.html#the-algorithm",
    "href": "AuctionAlgorithm.html#the-algorithm",
    "title": "Auction Algorithm",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm starts from an arbitrary arrangement of buyers and goods. It does not matter to the algorithm who begins with what good. Denote this arbitrary arrangement with the prices and good permutation as \\((p_0, \\sigma_0)\\). The algorithm acts with iterations, and once all buyers satisfy the “happy” condition, our algorithm is done. The algorithm is as follows:\n*First, find a particular buyer. We will call this buyer \\(i^*\\). We will only choose a buyer such that \\((*)\\) does not hold. The buyer then finds the good maximizing the difference between their personal utility and the price, i.e. \\(\\max_{j = 1,...,N} \\{ a_{i^* j} - p_j \\}\\). This buyer exchanges their good with this other buyer that originally held this other good. Denote this new good \\(j^*\\).\n*This buyer that just purchased the good maximizing their utility is going to increase the price of this new good by some amount \\(\\gamma_i\\) until this buyer is indifferent between object \\(j^*\\) and the second best option. Mathematically, we say \\(a_{i^* j^*} - p_{j^*} = \\max_{j \\neq j^*} \\{a_{i^* j} - p_j \\}\\).\nOur iterative procedure continues until each buyer satisfies the “happy” condition."
  },
  {
    "objectID": "AuctionAlgorithm.html#complementary-slackness",
    "href": "AuctionAlgorithm.html#complementary-slackness",
    "title": "Auction Algorithm",
    "section": " -Complementary Slackness",
    "text": "-Complementary Slackness\nOne problem with the algorithm is the possibility that it never ends, i.e. it iterates indefinitely. To fix such a problem, we introduce a scalar \\(\\epsilon &gt; 0\\) and we alter our condition \\((*)\\) so that the buyer is within a tolerance of being content with their purchase. Such names for this are \\(\\epsilon-\\)happy or “almost happy.” Specifically, we alter \\((*)\\) by saying\n a_{i(i)} - p_{(i)} { a*{ij} - p_j } - .  Another name for the above condition is \\(\\epsilon-\\)complementary slackness. It may be necessary to modify the value for \\(\\epsilon\\) so that convergence of the algorithm is reached at a desirable rate. The algorithm maintains \\(\\epsilon-\\)complementary slackness at each iteration."
  },
  {
    "objectID": "AuctionAlgorithm.html#relations-with-optimal-transport",
    "href": "AuctionAlgorithm.html#relations-with-optimal-transport",
    "title": "Auction Algorithm",
    "section": "Relations with Optimal Transport",
    "text": "Relations with Optimal Transport\nThe auction algorithm has applications in optimal transport, mostly by extending situations in optimal transport to where the assignment problem applies. Converting optimal transport scenarios to the assignment problem gives us the opportunity to apply the auction algorithm to find a solution. Something to note is that this idea mostly applies to linear optimal transport problems. One area in which this idea can be done is network optimization problems. Shortest path and max-flow problems can be converted into assignment problems, giving the auction algorithm a chance to offer a solution. A second area is transportation problems, and another is minimum cost flow problems. A final area is convex separable network optimization problems.\nAllow us to provide an example of how the assignment problem and the auction algorithm have applications in optimal transport . Define two discrete measures by\n = ^n (a_i)(),     = ^n (b_i)() \nwhere \\(a_i\\) and \\(b_j\\) denote two sets of weights and \\(\\delta_{x_i}\\) and \\(\\delta_{y_j}\\) denote the Dirac function evaluated at certain locations (i.e., infinitely large at such position and trivial elsewhere). We seek a map that translocates the mass \\(\\alpha\\) to the mass \\(\\beta\\), where each point associated with the mass \\(\\alpha\\) is tied to a point for \\(\\beta\\). From this, we may apply the assignment problem. There exists a permutation \\(\\sigma\\) that associates each location for one mass with the other. We must find a permutation such that a cost function is minimized, i.e.,\n _{T} \nwhere \\(c(x,y)\\) is a cost function and \\(T\\# \\alpha = \\beta\\) denotes the map transferring the masses spread across locations in \\(\\alpha\\) to those in \\(\\beta\\). The reason this map can be interpreted as a permutation is because the masses are constructed from a discrete number of points. Note that this minimization problem is analogous to our utility maximization problem previously discussed."
  },
  {
    "objectID": "AuctionAlgorithm.html#references",
    "href": "AuctionAlgorithm.html#references",
    "title": "Auction Algorithm",
    "section": "References",
    "text": "References\n D.P. Bertsekas, Auction Algorithms, Laboratory for Information and Decision Systems. F. Santambrogio, Optimal Transport in Applied Mathematics, Chapter 1, 6. G. Peyré and M. Cuturi, Computational Optimal Transport, Chapter 2, 3."
  },
  {
    "objectID": "Est_Tr_Maps.html",
    "href": "Est_Tr_Maps.html",
    "title": "Estimation of Transport Maps",
    "section": "",
    "text": "Ordinarily in optimal transport, you’re given two probability measures \\(\\mu\\) and \\(\\nu\\) and tasked to find the cheapest way to transport \\(\\mu\\) to \\(\\nu\\). But in many applications, you don’t know explicitly what the probability measures \\(\\mu\\) and/or \\(\\nu\\) are, but are able to obtain samples from either of them through experiments or data collection.\nThis article discusses two approaches to estimate optimal transport maps, only given samples from \\(\\mu\\) and \\(\\nu\\). The below follows the presentation in (Chewi, Niles-Weed, and Rigollet 2019). The first approach is to immediately replace \\(\\mu\\) and \\(\\nu\\) by their empirical measures (which are discrete approximations of \\(\\mu\\), \\(\\nu\\)), solve for an optimal transport plan, then use this to construct an approximate transport map. The other approach is to go to the semidual reformulation of the optimal transport problem, replace \\(\\mu\\) ad \\(\\nu\\) by their empirical measures, then use the optimal transport map of this modified problem as an estimator for the optimal transport map."
  },
  {
    "objectID": "Est_Tr_Maps.html#definition-of-empirical-measures",
    "href": "Est_Tr_Maps.html#definition-of-empirical-measures",
    "title": "Estimation of Transport Maps",
    "section": "Definition of Empirical Measures",
    "text": "Definition of Empirical Measures\nFor a point \\(x \\in \\mathbb{R}^d\\), define the dirac mass at \\(x\\) to be \\[\n\\delta_x(A) = \\begin{cases} 1 & \\text{ if } x \\in A\\\\\n0 & \\text{ if } x \\not\\in A\n\\end{cases}\n\\] This is the measure which gives one unit of mass to \\(x\\), and zero to everything else. Now, given the samples \\(X_1,...,X_n\\) and \\(Y_1,...,Y_n\\) we can define the empirical measures \\[\n\\mu_n = \\frac{1}{n} ( \\delta_{X_1} + \\cdots + \\delta_{X_n})\n\\] \\[\n\\nu_n = \\frac{1}{n} ( \\delta_{Y_1} + \\cdots + \\delta_{Y_n})\n\\] Technically, the empirical measures are families of measures: one for each realization of the random variables \\(X_1,...,X_n\\) (for \\(\\mu_n\\)), \\(Y_1,...,Y_n\\) (for \\(\\nu_n\\)). The intuition for \\(\\mu_n\\) is this; \\[\n\\mu_n(A) = \\frac{(\\text{number of } X_i \\text{'s in } A)}{n},\n\\] and when \\(n\\) is large this will be approximately equal to \\(\\mu(A)\\). And indeed, for any measurable \\(A \\subseteq \\mathbb{R}^d\\), we have \\(\\mu_n(A) \\to \\mu(A)\\) almost surely."
  },
  {
    "objectID": "Est_Tr_Maps.html#kantorovich-problem",
    "href": "Est_Tr_Maps.html#kantorovich-problem",
    "title": "Estimation of Transport Maps",
    "section": "Kantorovich Problem",
    "text": "Kantorovich Problem\nThe Kantorovich formulation of optimal transport is very often used in place of the Monge problem. It has several better properties for analysis (discussed here). Moreover, the discrete Kantorovich problem is a linear progaming problem (as discussed on p.13–14 in (Chewi, Niles-Weed, and Rigollet 2019)).\nHere is the formal problem statement of the Kantorovich problem. Let \\(\\mu, \\nu \\in \\mathcal{P}(X)\\), and let \\(c:X \\times X \\to [0,\\infty)\\) be a cost function. The set of addmissible transport plans is defined to be \\[\n\\Gamma(\\mu,\\nu) = \\{ \\gamma \\in P(X \\times X): \\pi_1\\# \\gamma = \\mu \\text{ and } \\pi_2 \\# \\gamma = \\nu\\}\n\\] where \\(\\pi_1, \\pi_2:X \\times X \\to X\\) are the canonical projections \\(\\pi_1(x,y) = x\\), \\(\\pi_2(x,y)=y\\). The Kantorovich optimal transport problem is:\n\\[\n\\min_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\mathbb{K}( \\gamma) = \\min_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\int_{X \\times Y} c(x,y) d \\gamma.\n\\tag{2}\\]\nA \\(\\gamma \\in \\Gamma(\\mu,\\nu)\\) which achieves this minimum is called an optimal transport plan.\nGiven any transport map \\(T\\), you can construct a transport plan: \\(\\gamma = (id,T)\\# \\mu\\). This transport plan just sends mass at \\(x\\) to \\(T(x)\\). Under the hypothoses of Brenier’s theorem, the minimum in Monge’s and Kantorovich’s problems is the same, the minimizers of both are unique, and the relationship is this: if \\(T\\) is an optimal transport map, then \\((id,T) \\# \\mu\\) is an optimal transport plan, and any optimal transport plan \\(\\gamma\\) is of this form."
  },
  {
    "objectID": "Est_Tr_Maps.html#first-estimator",
    "href": "Est_Tr_Maps.html#first-estimator",
    "title": "Estimation of Transport Maps",
    "section": "First Estimator",
    "text": "First Estimator\nOne possible approach to estimating \\(T\\) is to replace \\(\\mu\\) and \\(\\nu\\) by their empirical measure counterparts \\(\\mu_n\\) and \\(\\nu_n\\). Then, solve the empirical version of the Kantorovich problem: \\[\n  \\min_{\\gamma \\in \\Gamma(\\mu_n,\\nu_n)} \\mathbb{K}_n( \\gamma) = \\min_{\\gamma \\in \\Gamma(\\mu_n,\\nu_n)} \\int_{X \\times X} c(x,y) d \\gamma(x,y)\n\\] Let \\(\\gamma_n\\) be the minimizer. Then, try to find a transport map \\(\\widehat{T}\\) which induces the transport plan \\(\\gamma_n\\) (that is, find a function \\(\\widehat{T}:X \\to X\\) such that \\((id,\\widehat{T}) \\# \\mu_n = \\gamma_n\\)).\nTo do this, note: every element of \\(\\Gamma(\\mu_n,\\nu_n)\\) is a convex combination of dirac masses at sample points \\((X_i,Y_j)\\). So, \\(\\gamma_n\\) will be a convex combination of dirac masses at sample points \\((X_i,Y_j)\\). Whenever one of the dirac masses \\(\\delta_{(X_i,Y_j)}\\) appears in \\(\\gamma_n\\) with positive coefficient, you should define \\(T(X_i) = Y_j\\). However, if the transport plan ever splits mass (that is, sends positive mass from \\(X_i\\) to \\(Y_{j_1}\\), and sends more positive mass from \\(X_i\\) to \\(Y_{j_2}\\)) there will be no optimal transport map.\nThere are two major drawbacks to this approach: (1) as mentioned above, \\(\\widehat{T}\\) might not even exist, and (2) this approach only tells you what \\(\\widehat{T}(X_1)\\), …, \\(\\widehat{T}(X_n)\\) are, but doesn’t tell you what \\(T_n(x)\\) is for any other \\(x\\). There is no standard choice from this for how \\(\\widehat{T}\\) should be defined anywhere but at \\(X_1,...,X_n\\). Several methods have been proposed. One simple way is to define \\(\\widehat{T}(x)\\) to be \\(\\widehat{T}(X_i)\\) where \\(X_i\\) is the nearest sample to \\(x\\)."
  },
  {
    "objectID": "Est_Tr_Maps.html#the-dual-problem",
    "href": "Est_Tr_Maps.html#the-dual-problem",
    "title": "Estimation of Transport Maps",
    "section": "The Dual Problem",
    "text": "The Dual Problem\nThe dual of the Kantorovich problem is \\[\\sup \\int_X f(x) d\\mu (x) + \\int_Y g(y) d \\mu(y)\\] where the supremum is taken over all \\(f \\in L^1(\\mu), g \\in L^1(\\nu)\\) such that \\(f(x) + g(y) \\leq c(x,y)\\). Here, \\[L^1(\\mu) = \\{ f:X \\to X \\text{ measurable }: \\int_X |f|d \\mu &lt; \\infty\\}.\\] Moreover, we have the follwing theorm (this is Theorem 1.3 in (Villani 2021), you can find a proof there):\n\nTheorem 1 (Equivalence of Primal and Dual) Suppose that \\(c: X \\times X \\to [0,\\infty)\\) is lower semi-continuous. Then, \\[\\sup_{f(x)+ g(y) \\leq c(x,y)} \\int_X f(x) d\\mu (x) + \\int_X g(y) d \\nu(y)= \\inf_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\mathbb{K}(\\gamma)\n\\] moreover, the infimum on the right hand side is attained."
  },
  {
    "objectID": "Est_Tr_Maps.html#the-semidual-problem",
    "href": "Est_Tr_Maps.html#the-semidual-problem",
    "title": "Estimation of Transport Maps",
    "section": "The Semidual problem",
    "text": "The Semidual problem\nAssume \\(\\mu, \\nu \\in \\mathcal{P}(\\mathbb{R}^d)\\) have finite second moments. That is, assume \\[\\int_{\\mathbb{R}^d} |x|^2 d \\mu(x), \\quad\\int_{\\mathbb{R}^d} |y|^2 d \\nu(y)&lt; \\infty.\n\\] Also, assume the cost function is \\(c(x,y) = |x-y|^2\\). The dual problem be further transformed into a problem of optimizing over just one function. \\[\\sup_{f(x)+ g(y) \\leq c(x,y)} \\int_X f(x) d\\mu (x) + \\int_Y g(y) d \\mu(y) = \\inf_{\\phi \\in L^1(\\mu)} S(\\phi)\\] where \\[S(\\phi) = \\int \\phi d \\mu + \\int \\phi^* d \\nu \\tag{3}\\] Here, \\(\\phi^*\\) is the convex conjugate of \\(\\phi\\), defined by \\[\n\\phi^*(y) = \\sup_{x \\in \\mathbb{R}^d} (\\langle x,y\\rangle - \\phi(x)).\n\\] Moreover, the minimizer of Equation 3 may be taken to be a convex, lower semicontinuous function \\(\\phi\\). For this convex, lower semicontinuous minimizer \\(\\phi\\), the function \\(T = \\nabla \\phi\\) is an optimal transport map. (see sections 1.5.2 and 1.5.3 in (Chewi, Niles-Weed, and Rigollet 2019))."
  },
  {
    "objectID": "Est_Tr_Maps.html#semidual-estimator",
    "href": "Est_Tr_Maps.html#semidual-estimator",
    "title": "Estimation of Transport Maps",
    "section": "Semidual Estimator",
    "text": "Semidual Estimator\nThis way of rewriting the Monge problem suggests a way to approximate \\(T\\): replace \\(\\mu, \\nu\\) by the empirical measures in the expression for \\(S(\\phi)\\). That is, find a minimizer \\(\\widehat{\\phi}\\) for \\[\n\\min_{\\phi \\in \\mathcal{F}} S_n(\\phi) =  \\min_{\\phi \\in \\mathcal{F}}\\int \\phi d \\mu_n + \\int \\phi^* d \\nu_n.\n\\] (the class of functions minimized over \\(\\mathcal{F}\\) is chosen to suit the problem at hand, see sections 3.2-3.6 of (Chewi, Niles-Weed, and Rigollet 2019)). Then define \\(\\widehat{T} = \\nabla \\widehat{\\phi}\\). This has the advantage over the empirical exitmator that \\(\\widehat{T}(x)\\) is defined for all \\(x \\in \\mathbb{R}^d\\).\n(Chewi, Niles-Weed, and Rigollet 2019) discusses the convergence of \\(\\widehat{T}\\) to \\(T\\) (in the \\(L^2(\\mu)\\) sense) under certain circumstances and proves bounds for the convergence rate."
  },
  {
    "objectID": "RicciCurvature.html",
    "href": "RicciCurvature.html",
    "title": "Optimal Transport and Ricci Curvature",
    "section": "",
    "text": "This article provides a brief introduction into a connection of optimal transport and the curvature tensor of a Riemannian manifold. In fact, we are going to study the transport map \\(T_t(x):=\\text{exp}_x( t\\xi(x)),\\) where \\(\\xi\\) denotes a \\(C^1\\) vector field on the manifold \\((M,g).\\)\nThese kind of maps appear very naturally in the context of optimal transport. Recall that in optimal transport one is particularly interested in the Monge Problem, being the following optimization problem: Let \\((M,g)\\) be a compact and connected Riemannian manifold. Let furthermore, \\(\\mu=fdV , \\nu=gdV\\) denote two probability measures on \\(M\\) which are absolutely continuous with respect to the measure on the manifold, induced by the metric. the Monge Problem is then given by \\(\\inf_{T\\#\\mu =\\nu} \\int d(x,T(x))^2\\,dV,\\) where the infimum is taken among all measurable maps \\(T:M\\rightarrow M\\) and \\(d\\) denotes the Metric on \\((M,g)\\) induced by \\(g.\\) Then the Monge Problem admits a unique solution \\(T.\\) Moreover, in that case \\(T(x)=\\exp_x(\\nabla \\psi(x))\\) for some \\(\\psi\\) (see for more details of this).\nTo conclude the introductory part of this article, let us also mention that these kind of transport maps, turned out to be useful in the area of geometric analysis. In fact, Simon Brendle could prove a Sobolev inequality on non compact Riemannian manifolds with nonnegative Ricci curvature, the proof of which makes use of defining a map which is of the type \\(T(x)=\\exp_x(\\nabla u(x))\\) ( see proof of Theorem 1.1 in  for more details)."
  },
  {
    "objectID": "RicciCurvature.html#introduction-and-motivation",
    "href": "RicciCurvature.html#introduction-and-motivation",
    "title": "Optimal Transport and Ricci Curvature",
    "section": "",
    "text": "This article provides a brief introduction into a connection of optimal transport and the curvature tensor of a Riemannian manifold. In fact, we are going to study the transport map \\(T_t(x):=\\text{exp}_x( t\\xi(x)),\\) where \\(\\xi\\) denotes a \\(C^1\\) vector field on the manifold \\((M,g).\\)\nThese kind of maps appear very naturally in the context of optimal transport. Recall that in optimal transport one is particularly interested in the Monge Problem, being the following optimization problem: Let \\((M,g)\\) be a compact and connected Riemannian manifold. Let furthermore, \\(\\mu=fdV , \\nu=gdV\\) denote two probability measures on \\(M\\) which are absolutely continuous with respect to the measure on the manifold, induced by the metric. the Monge Problem is then given by \\(\\inf_{T\\#\\mu =\\nu} \\int d(x,T(x))^2\\,dV,\\) where the infimum is taken among all measurable maps \\(T:M\\rightarrow M\\) and \\(d\\) denotes the Metric on \\((M,g)\\) induced by \\(g.\\) Then the Monge Problem admits a unique solution \\(T.\\) Moreover, in that case \\(T(x)=\\exp_x(\\nabla \\psi(x))\\) for some \\(\\psi\\) (see for more details of this).\nTo conclude the introductory part of this article, let us also mention that these kind of transport maps, turned out to be useful in the area of geometric analysis. In fact, Simon Brendle could prove a Sobolev inequality on non compact Riemannian manifolds with nonnegative Ricci curvature, the proof of which makes use of defining a map which is of the type \\(T(x)=\\exp_x(\\nabla u(x))\\) ( see proof of Theorem 1.1 in  for more details)."
  },
  {
    "objectID": "RicciCurvature.html#curvature-and-optimal-transport",
    "href": "RicciCurvature.html#curvature-and-optimal-transport",
    "title": "Optimal Transport and Ricci Curvature",
    "section": "Curvature and Optimal Transport",
    "text": "Curvature and Optimal Transport\nLet \\((M,g)\\) be a Riemannian manifold. In this article we assume basic knowledge about the notions of curvature and geodesics on a manifold. For some background information on these topics, we refer the reader to Chapter three to five in .\nThe Goal of this article is to show the follwing ### Proposition Let \\(T_t(x)=\\exp_x(t\\nabla \\psi (x)),\\) where \\(\\psi\\) denotes a \\(C^2\\) function on \\(M\\) and let \\(\\mathcal J (t):=\\text{log}(\\text{det}[d_xT_t]).\\) Then the following inequality holds true:\n\\(\\mathcal J''+\\frac{1}{n} \\mathcal J'+ \\text{Ric}_{\\gamma(t)}(\\gamma'(t),\\gamma'(t))\\leq 0, \\quad \\text{ for all } t \\text{ such that } \\mathcal J(t)&gt;0,\\)\nwhere \\(\\gamma\\) is defined to be the mapping \\(t \\mapsto T_t(x),\\) which is a geodesic.\nNotice that this inequality involves the transport map \\(T\\) and the Ricci curvature tensor and therefore constitutes a connection of the curvature and the optimal transport problem.\n\nRemarks\nBefore we prove the Proposition, let us do some remarks: let \\(x \\in M\\) be given and let \\(e_i\\) for \\(i = 1, \\dots, n\\) be an orthonormal basis. After doing parallel transport along \\(\\gamma\\), we have an orthonormal basis also in \\(T_{T_t(x)}M\\). Let \\(\\textbf J\\) denote the matrix representation of \\(d_xT_t: T_xM\\rightarrow T_{T_t(x)}M.\\) Then we have that \\(\\textbf J''+R\\textbf J=0\\) where \\(R\\) denotes the matrix \\((R(\\gamma', e_i,\\gamma', e_j))_{i,j}.\\) Indeed, this follows right away from the fact that \\(J_i(t):= d_xT_t(e_i)\\) is a Jacobi field along \\(\\gamma\\) for each \\(i =1,\\dots,n.\\)\n\n\nProof of the Proposition\nWith the notation of the proposition stated above, we compute the derivative of \\(\\mathcal J\\) as follows: \\(\\frac{d}{dt}\\mathcal J(t) = \\text{trace}(\\textbf J' \\textbf J^{-1})\\) which follows right away from Jacobi’s formula. Let now \\(U(t):=\\textbf J' \\textbf J^{-1}\\). Since \\(\\textbf J\\) satisfies the matrix Jacobi equation as noticed in the preceding remark, we may infer a Ricatti equation for \\(U\\). Indeed, taking derivative of \\(U( t)\\) we obtain the following equation\n\\(U' =\\textbf J ''  \\textbf J^{-1} +\\textbf J' \\frac{d}{dt}\\textbf J^{-1}.\\)\nNow observe that\n\\(\\frac{d}{dt}\\textbf J^{-1}= - \\textbf J^{-1}\\textbf J '  \\textbf J^{-1}.\\)\nIn fact, notice that we only consider \\(t\\) such that \\(\\det( \\textbf J) &gt;0\\). Thus the inverse matrix exists for all \\(t\\) in a small neighborhood of that \\(t\\). Then we have that\n\\(0= \\frac{d}{dt} 1_n= \\frac{d}{dt}\\textbf J\\textbf J^{-1}= \\textbf J'\\textbf J^{-1}+\\textbf J\\frac{d}{dt}\\textbf J^{-1}\\)\nfrom which, after rearranging and multiplying with the inverse from the left, one gets the desired equality. Plugging this into the equation concerning the derivative of \\(U\\), we get that\n\\(U' =\\textbf J _  \\textbf J^{-1}-\\textbf J'\\textbf J^{-1}\\textbf J '  \\textbf J^{-1}= \\textbf J _  \\textbf J^{-1}- U^2.\\)\nThen, using the matrix Jacobi equation, we get that\n\\(U' =-R  \\textbf J\\textbf J^{-1}-U^2= -R-U^2,\\)\nso that\n\\(U' +R+U^2=0.\\)\nNotice that this is a first order ODE. Since \\(U(0)= \\text{Hess } \\psi,\\) is symmetric, we have that \\(U^T\\) also satisfies the Ricatti equation with same initial condition, from which we get that \\(U=U^T\\).\nMoreover, since the Ricatti equation is an equation of matrices, we can now take the trace in that equation. We therefore obtain that\n\\(\\text{tr} (U')+\\text{Ric}_{\\gamma}(\\gamma', \\gamma')+ \\text{tr}(U^2)=0.\\)\nAs \\(U\\) is symmetric, we get that \\(\\text{tr}{U^2}\\geq \\frac{1}{n}\\text{tr}(U)^2\\),\nwe therefore obtain our desired inequality."
  },
  {
    "objectID": "RicciCurvature.html#ricci-curvature-lower-bounds-and-entropy",
    "href": "RicciCurvature.html#ricci-curvature-lower-bounds-and-entropy",
    "title": "Optimal Transport and Ricci Curvature",
    "section": "Ricci Curvature lower bounds and Entropy",
    "text": "Ricci Curvature lower bounds and Entropy\nIn this section we will consider a generic measure metric space and we will try to define the notion of Ricci curvature lower bound. In literature was first extended the notion of sectional curvature thanks to the work of Alexandrov Alexandrov, A. D., A theorem on triangles in a metric space and some applications. Trudy Mat. Inst. Steklov, 38 1951 comparing symmetries of the geodesics triangles, as expected this lower bounds are a natural extension of Sectional Curvature lower bounds, and they coincide in the setting of a Riemannian manifold. This construction became extremely interesting when (40 years later!) Grove and Petersen  Grove, K. & Petersen, P., Manifolds near the boundary of existence. J. Differential Geom., 33 (1991), 379–394 pointed out that this lower bounds are stable under the so called Gromov-Hausdorff convergence. It is well known that the family of Riemannian manifolds with Ricci lower bounds are not closed under the Gromov-Hausdorff convergence or any other reasonable notion of convergence. Is the previous sentence correct? See also the discussion here: [[. We present here an extension of Ricci curvature lower bounds for measure metric spaces that will also preserve this convergence! In the rest of this article we will follow Sturm papers  and . It is important to mention that at the same time of these papers the same results were obtained independently by Lott-Villani in .\n\nCD(K,) condition.\nThis is the dimension independent notion of Ricci curvature bounds. We will require our space \\((M,d,m)\\) to be complete and separable and the measure to be locally finite. These assumptions come from the discussion of curvature dimension condition in the well known paper from Bakry and Emery  Bakry, D. & Emery, M. ´ , Diffusions hypercontractives, in S´eminaire de Probabilit´es, XIX, 1983/84, pp. 177–206. Lecture Notes in Math., 1123. Springer, Berlin, 1985. . We denote with \\(\\mathcal{P}_2(M,d,m)\\) the subspace of all measures \\(\\nu \\in \\mathcal{P}_2(M,d)\\) that are absolutely continuous with \\(m\\). We can now apply Radon-Nikodym theorem  and we can write \\(\\nu=\\rho m\\), where \\(\\rho\\) is the density from the Radon-Nikodym theorem. Using this decomposition we can now define the relative entropy of \\(\\nu\\) respect to \\(m\\) in the following way:\n:\\(\\operatorname{Ent}(\\nu|m)= \\lim_{\\epsilon \\downarrow 0}\\int_{\\rho &gt;\\epsilon} \\rho \\log \\rho dm.\\) Note that if \\(\\int_{\\rho &gt;1} \\rho \\log \\rho dm &lt;\\infty.\\) then our definition of entropy coincides with \\(\\int_{\\rho &gt;0} \\rho \\log \\rho dm .\\). If that is not satisfied then we just say that \\(\\operatorname{Ent}(\\nu|m)=\\infty.\\) We also say that the entropy is infinity for any measure that is not absolutely continuous with \\(m\\). We can now define the space of absolutely continuous measures with \\(m\\) that have finite relative entropy respect to \\(m\\):\n:\\(\\mathcal{P}_2^{\\star}(M,d,m)=\\{\\nu \\in \\mathcal{P}_2(M,d) : \\operatorname{Ent}(\\nu|m)&lt;\\infty\\}\\).\nIt’s very surprising that the notion of curvature is related to the (weak) K-convexity of the relative entropy on \\(\\mathcal{P}_2^{\\star}(M,d,m)\\). in the following sense: We say that \\((M,d,m)\\) has curvature globally \\(\\geq K\\) for a generic \\(K \\in \\mathbb{R}\\) if for each pair \\(\\nu_0,\\nu_1 \\in \\mathcal{P}_2^{\\star}(M,d,m)\\) there exists a geodesics \\(\\Gamma : [0,1] \\longrightarrow \\mathcal{P}_2^{\\star}(M,d,m)\\) connecting \\(\\nu_0\\) and \\(\\nu_1\\) such that:\n:\\(\\operatorname{Ent}(\\Gamma(t)|m)\\leq (1-t)\\operatorname{Ent}(\\Gamma(0)|m)+t \\operatorname{Ent}(\\Gamma(1)|m) - \\frac{K}{2}t(1-t)d^2_W(\\Gamma(0),\\Gamma(1)) \\text{ for all } t \\in [0,1].\\).\nWe now have an intrinsic notion of curvature that we denote in the following way:\n:\\(\\underline{Curv}(M,d,m)=\\sup\\{ K \\in \\mathbb{R} : (M,d,m) \\text{ has curvature } \\geq K \\}.\\)\nTo make sure that we have introduced an interesting notion of curvature, we need these definition to agree with the classical Ricci curvature lower bounds in the case that our space is also a Riemannian manifold. This indeed happen and it’s Theorem 4.9 in Sturm’s paper and a detailed proof can be found in  von Renesse, M.-K. & Sturm, K.-T., Transport inequalities, gradient estimates, entropy, and Ricci curvature. Comm. Pure Appl. Math., 58 (2005), 923–940 : If M is a complete Riemannian manifold with Riemannian distance \\(d\\) and volume \\(m\\), denote with \\(m'=e^{-V}m\\), where \\(V\\) is a twice differentiable function \\(V: M \\longrightarrow \\mathbb{R}\\) then we have the following:\n:\\(\\underline{Curv}(M,d,m')=\\inf\\{\\operatorname{Ric}_M{\\xi,\\xi}+\\operatorname{Hess}V(\\xi,\\xi) : \\xi \\in TM \\text{ and } |\\xi|=1\\}\\).\nIn particular this tells us that in the Riemannian setting \\((M,d,m)\\) has curvature \\(\\geq K\\) if and only if Ricci curvature is \\(\\geq K\\).\n\n\nCD(K,N) condition.\nIn the previous section we have defined a dimensional independent notion of curvature lower bound. We want now to reinforce the curvature bound \\(Curv \\geq K\\) by adding a condition on the dimension. There are the so called \\(CD(K,N)\\) conditions where \\(K\\) has to be intended as lower bound for the curvature and \\(N\\) as an upper bound for the dimension. The notion of curvature we discussed in the previous section will coincide, in some sense, when \\(N=\\infty\\). Again this new construction will coincide with the usual notion of Ricci curvature lower bound in the case we have a Riemannian manifold. The main idea is to substitute the relative entropy with the Renyi entropy functional (note that this is interesting for finite N):\n:\\(S_N(\\nu|m)=-\\int \\nu^{1-\\frac{1}{N}}\\).\nIt is easy to state the \\(CD(0,N)\\) condition, it simply means that for all \\(N'\\geq N\\) the entropy functional we have just defined \\(S_N(\\cdot|m)\\) is convex on the \\(L_2-\\)Wasserstein space \\(\\mathcal{P}_2(M,d)\\). In the case of a Riemannian manifold this characterizes manifolds with dimension \\(\\leq N\\) and Ricci curvature \\(\\geq 0\\), this result can be found in. The argument is based on the fact that the Jacobian determinant \\(J_t=\\det dF_t\\) of any transport map \\(T_t=\\exp (-t\\nabla \\phi)\\) satisfies \\(\\frac{\\partial^2}{\\partial t^2}J_t^{\\frac{1}{N}} \\leq 0\\) if and only if \\(M\\) has dimension \\(\\leq N\\) and Ricci curvature \\(\\geq 0\\). It can also be proved that this condition is equivalent to the Brunn-Minkowski inequality:\n:\\(m(A_t)^{\\frac{1}{N'}}\\geq (1-t) m(A_0)^{\\frac{1}{N'}}+tm(A_1)^{\\frac{1}{N'}},\\)\nFor any \\(N'\\geq N\\), any \\(t \\in [0,1]\\) and any pair of sets \\(A_0,A_1 \\subset M\\) where \\(A_t\\) denotes the set of points \\(\\gamma_T\\) on geodesics with endpoints \\(\\gamma_0 \\in A_0\\) and \\(\\gamma_1 \\in A_1\\).\nThe condition \\(CD(K,N)\\) for a general \\(K \\in \\mathbb{R}\\) is more complicated, assuming measurable choice of a unique geodesic condition: the existence of a unique geodesic \\(\\gamma_t(x,y)\\) connecting \\(x\\) and \\(y\\) for \\(m\\otimes m\\)-almost any \\((x,y) \\in M^2\\). Then choose any two absolutely continues probability measures, thanks to Radon Nykodim we can write them as \\(\\rho_0 m\\) and \\(\\rho_1 m\\), then there existes an optimal coupling \\(q\\) that satisfies:\n:\\(\\rho_t(\\gamma_t(x,y)) \\leq \\Big( \\tau_{K,N}^{(1-t)}(d(x,y))\\rho_0^{-\\frac{1}{N}}(x)+\\tau_{K,N}^{(t)}(d(x,y))\\rho_1^{-\\frac{1}{N}}(y)\\Big)^{-N}.\\) for all \\(t \\in [0,1]\\) and \\(q\\)- almost everywhere \\((x,y)\\in M^2\\), denoted with \\(\\rho_t\\), the density of the push-forward of \\(q\\) under the map \\((x,y) \\mapsto \\gamma_t(x,y)\\) and \\(\\tau\\) is defined in the following way, denote \\(H=\\sqrt{\\frac{K}{N-1}}\\):\n:\\(\\tau_{K,N}^{(t)}=t^{\\frac{1}{N}}\\Big(\\frac{\\sin(Ht\\theta}{\\sin(H\\theta}\\Big)^{1-\\frac{1}{N}}\\). With the usual hyperbolic function interpretation when \\(K\\leq 0\\).\nIt can be shown that the condition on the Jacoby determinant is replaced now by:\n:\\(\\frac{\\partial^2}{\\partial t^2}J_t^{\\frac{1}{N}} \\leq -\\frac{K}{N} J_t^{\\frac{1}{N}}(x)d^2(x,F_1(x))\\).\nThis leads to a distorted Brunn-Minkowski type inequality:\n:\\(J_t^{\\frac{1}{N}}(x)\\geq \\tau_{K,N}^{(1-t)}(d(x,T_1(x)))J_0^{\\frac{1}{N}}(x)+\\tau_{K,N}^{(t)}(d(x,T_1(x)))J_1^{\\frac{1}{N}}(x)\\).\nAgain in this case, this is the usual notion of Ricci curvature lower bound if our space has the extra structure of a Riemannian manifold. The proof of this, together with all the result mentioned were also developed independently in  J. Lott, C. Villani, Ricci curvature for metric-measure spaces via optimal transport. (2005)."
  },
  {
    "objectID": "RicciCurvature.html#references",
    "href": "RicciCurvature.html#references",
    "title": "Optimal Transport and Ricci Curvature",
    "section": "References",
    "text": "References\n\n A. Figalli, C. Villiani, OPTIMAL TRANSPORT AND CURVATURE, Notes for a CIME lecture course in Cetraro, June 2009 \n S. Brendle, Sobolev inequalities in manifolds with nonnegative curvature, 2021. arXiv: 2009.13717. \n M. P. do Carmo, Riemannian Geometry,Mathematics: Theory & Applications. Birkhauser Boston, Inc., Boston, MA, 1992"
  },
  {
    "objectID": "HKdistance.html",
    "href": "HKdistance.html",
    "title": "Hellinger-Kantorovich distance",
    "section": "",
    "text": "A major disadvantage of the Wasserstein distance is that it is meaningful only for measures of equal total mass. For purpose of applications, in signal processing, image classification, machine learning, medical imaging etc, it is essential to describe an distance optimal transport distance on the space of positive measures. In 2015, three different research groups introduced a new optimal transport distance on the space of positive Radon measures with various applications in mind; imaging applications, primal/dual and static formulations, population dynamics and gradient flows (Chizat et al. 2015),(Chizat et al. 2018),(Kondratyev, Monsaingeon, and Vorotnikov 2016),(Liero, Mielke, and Savaré 2016),(Liero, Mielke, and Savaré 2018). Some authors call it the Hellinger-Kantorovich (\\(HK\\)) distance, since it is an interpolation between Kantorovich/Wasserstein and Hellinger distances; others call it Kantorovich-Fischer-Rao (\\(FR\\)) distance, since it is an interpolation between Kantorovich/Wasserstein and Fischer-Rao distance. For a detailed explanation of terminology issues, see Remark 2.2 (Mielke and Zhu 2025)."
  },
  {
    "objectID": "HKdistance.html#dynamic-formulation",
    "href": "HKdistance.html#dynamic-formulation",
    "title": "Hellinger-Kantorovich distance",
    "section": "Dynamic Formulation",
    "text": "Dynamic Formulation\nOne way to define the Hellinger-Kantorovich distance between positive measures is in the dynamic sense of Benamou-Brenier, Dynamic Optimal Transport via\n\\[\n\\mathbf{HK}(\\mu,\\nu) := \\min \\left\\{ \\int_{0}^{1} \\int_\\Omega (\\alpha | \\nabla u |^{2} + \\beta |u |^{2}) d\\rho_{t} dt : \\partial_{t}\\rho = \\alpha \\hspace{1mm} div (\\rho\\nabla u) - \\beta \\rho u, \\rho(0) = \\mu,  \\rho(1) = \\nu \\right\\},\n\\] where we minimize an action over the curves with endpoints \\(\\mu,\\) \\(\\nu\\) that solve continuity equation with source \\[\n    \\partial_{t}\\rho = \\alpha \\hspace{1mm} div(\\rho\\nabla u) - \\beta \\rho u\n\\] in the sense of distributions."
  },
  {
    "objectID": "HKdistance.html#static-formulation",
    "href": "HKdistance.html#static-formulation",
    "title": "Hellinger-Kantorovich distance",
    "section": "Static Formulation",
    "text": "Static Formulation\nOn the other hand, to generalize the optimal transport Kantorovich problem to the setting of the positive measures, we define the entropy-transport functional:\n\\[ \\mathbf{ET}_{c,\\Psi}(\\Pi|\\mu,\\nu) := \\int c(x_0,x_1) d\\Pi(x_0, x_1) + \\Psi(\\pi_{0}|\\mu) + \\Psi(\\pi_{1}|\\nu),\n\\] where \\(\\pi_0(dx_0):= \\Pi(dx_0,\\Omega), \\pi_1(dx_1):= \\Pi(\\Omega,dx_1).\\) In this approach, we compute the standard optimal transport distance between \\(\\pi_0\\) and \\(\\pi_1,\\) and penalize difference between \\(\\pi_0\\) and \\(\\mu,\\) \\(\\pi_1\\) and \\(\\nu,\\) using the divergence functional \\(\\Psi.\\) Finally, like in Kantorovich problem, we observe:\n\\[ \\inf \\hspace{1mm} \\mathbf{ET}_{c,\\Psi}(\\Pi|\\mu,\\nu),\n\\] over probability measures \\(\\Pi.\\)"
  },
  {
    "objectID": "HKdistance.html#connection-between-the-two-descriptions",
    "href": "HKdistance.html#connection-between-the-two-descriptions",
    "title": "Hellinger-Kantorovich distance",
    "section": "Connection between the two descriptions",
    "text": "Connection between the two descriptions\nFor suitable choice of \\(\\Psi = \\frac{1}{\\beta}D_{KL},\\) for \\(KL\\) divergence (Kullback-Leibler divergence), and a cost function:\n\\[  c(x_0,x_1) := \\begin{cases} \\frac{-2}{\\beta} \\log(\\cos \\left( \\sqrt{\\frac{\\beta}{4\\alpha}}|x_0-x_1| \\right))   &\\text{ if } |x_0-x_1|&lt;\\pi \\sqrt{\\frac{\\alpha}{\\beta}} , \\\\ +\\infty &\\text{ otherwise.} \\end{cases}\n\\] we obtain a connection between the two definitions\n\\[\n\\mathbf{HK}^{2}(\\mu,\\nu) := \\inf_{\\Pi \\in \\mathcal{M}^{+}(\\Omega \\times \\Omega)} \\mathbf{ET}_{c,\\Psi} (\\Pi|\\mu,\\nu).\n\\]See (Liero, Mielke, and Savaré 2016) for the proof."
  },
  {
    "objectID": "Wasserstein_Barycenters.html",
    "href": "Wasserstein_Barycenters.html",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "",
    "text": "In optimal transport, a Wasserstein barycenter (Santambrogio 2015) is a probability measure that represents a weighted average between several probability measures with regard to the Wasserstein distance. This generalizes the notions of physical barycenters and geometric centroids to the domain of measures."
  },
  {
    "objectID": "Wasserstein_Barycenters.html#motivation",
    "href": "Wasserstein_Barycenters.html#motivation",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "Motivation",
    "text": "Motivation\nBarycenters in physics and geometry are points that represent a notion of a mean of a number of objects. In astronomy, the barycenter is the center of mass of two or more objects that orbit each other, and in geometry, a centroid is the arithmetic mean of all the points in an object. Given countably many points \\(\\{x_i\\}_{i \\in I} \\subseteq \\mathbb{R}^n\\) and nonnegative weights \\(\\{\\lambda_i\\}_{i\\in I}\\), the weighted \\(L^2\\) barycenter of the points is the unique point \\(y \\in \\mathbb{R}^n\\) such that:\n\\[y = \\arg \\min_{y \\in \\mathbb{R}^n} \\sum_{i \\in I} \\lambda_i \\|y - x_i\\|^2\\]\nWasserstein barycenters capture this concept for probability measures by replacing the Euclidean distance with the Wasserstein distance of two probability measures, \\(W_2(\\mu, \\nu)\\)."
  },
  {
    "objectID": "Wasserstein_Barycenters.html#definition",
    "href": "Wasserstein_Barycenters.html#definition",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "Definition",
    "text": "Definition\nLet \\(\\Omega\\) be a domain and \\(\\mathcal{P}(\\Omega)\\) be the set of probability measures on \\(\\Omega\\). Given a collection of probability measures \\(\\{\\mu_i \\}_{i \\in I}\\) and nonnegative weights \\(\\{\\lambda_i\\}_{i \\in I}\\), we define the Wasserstein barycenter of \\(\\{\\mu_i\\}_{i \\in I}\\) as any probability measure \\(\\mu\\) that minimizes the functional:\n\\[\\sum_{i \\in I} \\lambda_i W_2( \\mu_i, \\mu)^2\\]\nover the space \\(\\mu \\in \\mathcal{P}(\\Omega)\\). Here \\(W_2\\) denotes the \\(2\\)-Wasserstein distance, which may be replaced with the \\(p\\)-Wasserstein distance, \\(W_p\\), though this is less common.\nThis minimization problem was originally introduced by Agueh and Carlier (Agueh and Carlier 2011), who also proposed an alternative formulation of the problem above when there are finitely many measures \\(\\{\\mu_i\\}_{i = 1}^n\\). Instead of considering the minimum over all probability measures \\(\\mu \\in \\mathcal{P}(\\Omega)\\), one can equivalently consider the optimization problem over all multi-marginal transport plans \\(\\gamma \\in \\mathcal{P}(\\Omega^{n+1})\\) whose push forwards satisfy \\((\\pi_i)_{\\#} \\gamma = \\mu_i\\) for \\(i \\in \\{1, \\ldots, n\\}\\) and \\((\\pi_0)_{\\#} \\gamma = \\mu\\) for some unspecified probability measure \\(\\mu\\). The problem then tries to minimize:\n\\[\\int \\left ( \\sum_{i = 1}^N \\lambda_i \\| x_i - x_0 \\|^2 \\right ) d \\gamma\\]"
  },
  {
    "objectID": "Wasserstein_Barycenters.html#existence-and-uniqueness",
    "href": "Wasserstein_Barycenters.html#existence-and-uniqueness",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "Existence and Uniqueness",
    "text": "Existence and Uniqueness\nWe consider the Wasserstein barycenter minimization problem for finitely many given measures \\(\\{\\mu_i\\}_{i=1}^n \\subseteq \\mathcal{P}_2(\\mathbb{R}^d)\\) and nonnegative weights \\(\\lambda_i \\geq 0\\):\n\\[\\inf_{\\mu \\in \\mathcal{P}_2(\\mathbb{R}^d)} \\sum_{i=1}^n \\lambda_i W_2(\\mu_i, \\mu)\\]\nAgueh and Carlier (Agueh and Carlier 2011) prove that, if at least one \\(\\mu_i\\) is absolutely continuous with Lebesgue measure, the Wasserstein barycenter \\(\\mu\\) exists and is unique, and is given by \\(\\mu = \\nabla \\phi_i \\# \\mu_i\\) where \\(\\phi_i\\) is a convex potential.\nFurthermore, if all \\(\\mu_i\\) are absolutely continuous with Lebesgue measure, then the following are equivalent:\n\n\\(\\mu\\) is the Wasserstein barycenter\n\\(\\mu = \\nabla \\phi_i \\# \\mu_i\\) for all \\(i\\), where \\(\\phi_i\\) is a convex potential\nThere exist convex potentials \\(\\psi_i\\) such that \\(\\nabla \\psi_i\\) is the Brenier’s Theorem transport map from \\(\\mu_i \\rightarrow \\mu\\) and a constant \\(C\\) such that, for all \\(y \\in \\mathbb{R}^d\\) with equality \\(\\mu\\)-almost everywhere:\n\n\\[\\sum_{i=1}^n \\lambda_i \\psi_i^*(y) \\leq C + \\frac{\\|y\\|^2}{2}\\]"
  },
  {
    "objectID": "Wasserstein_Barycenters.html#one-dimensional-measures",
    "href": "Wasserstein_Barycenters.html#one-dimensional-measures",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "One Dimensional Measures",
    "text": "One Dimensional Measures\nLet \\(\\{\\mu_i\\}_{i=1}^n \\subseteq \\mathcal{P}_2(\\mathbb{R})\\) be nonatomic probability measures and \\(\\{\\lambda_i\\}_{i=1}^n\\) weights such that \\(\\lambda_i &gt; 0\\) for all \\(i\\) and \\(\\sum_{i=1}^n \\lambda_i = 1\\).\nThen the Wasserstein barycenter \\(\\mu\\) is given explicitly by:\n\\[\\mu = \\left( \\sum_{i=1}^n \\lambda_i T_i \\right) \\# \\mu_1\\] where \\(T_i\\) is the transport map \\(\\mu_1 \\rightarrow \\mu_i\\) from Bernier’s Theorem. (Agueh and Carlier 2011)"
  },
  {
    "objectID": "Wasserstein_Barycenters.html#computational-solution",
    "href": "Wasserstein_Barycenters.html#computational-solution",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "Computational Solution",
    "text": "Computational Solution\nIf all of the considered measures \\(\\{\\mu_i\\}_{i=1}^n\\) are known to be finitely supported, computing the Wasserstein barycenter reduces to a problem in linear programming (Peyré and Cuturi 2019). This is often the case for measures of interest in computational problems. The most common approach to computing Wasserstein barycenters is through Sinkhorn’s algorithm with entropic regularization, illustrated below.\n\n\n\nAn example of Wasserstein barycenter computation between two Gaussians over \\(\\mathbb{R}\\), as weights \\(\\lambda_1 + \\lambda_2 = 1\\) change through the interval \\((0,1)\\). Gaussian 1 (red) has has \\(\\mu = -2.5, \\sigma=1\\) while Gaussian 2 (blue) has \\(\\mu=3, \\sigma=1.5\\). The true Wasserstein barycenter (dotted black) is easily computed, as both measures are in \\(\\mathcal{P}_2(\\mathbb{R})\\) for which an explicit formula is available as discussed above. The computed Wasserstein barycenter (green) is yielded through Sinkhorn iterations with entropic regularization. Note that the general shape of the measure is accurate, with some smoothing issues alleviated partially through the regularization. The arithmetic weighted mean of the distributions is included for comparison (dashed purple)."
  },
  {
    "objectID": "Wasserstein_Barycenters.html#k-means-connection",
    "href": "Wasserstein_Barycenters.html#k-means-connection",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "K-Means Connection",
    "text": "K-Means Connection\nWhen the family of measures \\(\\{\\mu_i\\}_{i \\in I}\\) consists of exactly one finitely supported measure \\(\\mu_1\\) and we restrict the minimization problem solution space to exclusively \\(\\mu\\) which are finitely supported probability measures whose supports have at most \\(k\\) points, the problem of finding the Wasserstein barycenter is equivalent to the \\(k\\)-means clustering problem."
  },
  {
    "objectID": "Wasserstein_Barycenters.html#barycenters-in-image-processing",
    "href": "Wasserstein_Barycenters.html#barycenters-in-image-processing",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "Barycenters in Image Processing",
    "text": "Barycenters in Image Processing\nBarycenters have several applications in image processing. A core example (Claici, Chien, and Solomon 2018) arises in handwriting recognition. It is often useful to have a reference image of an expected letter, but simply taking the pixel-by-pixel Euclidean average fails to capture the geometric information of the letter. If the images are treated as probability distributions with pixel values as magnitudes, the Wasserstein barycenter provides a much more meaningful expected letter. We illustrate this application through the classical MNIST dataset.\n\n\n\nSome examples of the digit 8 sampled from the MNIST dataset. Images are grayscale, and interpreted as probability distributions for the Wasserstein barycenter calculation.\n\n\n\n\n\nThe Wasserstein barycenter (left) versus the arithmetic mean (right) of the sampled images above. Note the relatively severe distortion of the arithmetic mean, with many artifacts present from the multiple angles and styles of the digit, while the Wasserstein barycenter is still recognizably the desired digit with relatively little artifacting.\n\n\nAnother core image processing application is image interpolation. Wasserstein barycenters can be used to interpolate between two or more images in a meaningful way (Santambrogio 2015). For example, suppose we are given two images from the MNIST dataset. While taking an average of pixel values would result in an overlapped figure of the two digits in grayscale, there would be very little meaningful semantic information to extract from the image. By computing the Wasserstein barycenter for pairs of weights \\(\\lambda_1 + \\lambda_2 = 1\\) and changing these weights in the interval \\((0,1)\\), we can interpolate smoothly between these two images and watch the transport action, as below. Similar ideas can be applied to enhance the contrast of an image.\n\n\n\nA random digit 3 (left) and a random digit 8 (right) from the MNIST dataset. The Wasserstein barycenter (center) of these two images is computed for weights \\(\\lambda_1, \\lambda_2\\) ranging in \\((0,1)\\) for a smooth interpolation of these images."
  },
  {
    "objectID": "Wasserstein_Barycenters.html#model-ensembling",
    "href": "Wasserstein_Barycenters.html#model-ensembling",
    "title": "Wasserstein Barycenters and Applications in Image Processing",
    "section": "Model Ensembling",
    "text": "Model Ensembling\nModel ensembling is a popular machine learning technique for improving accuracy in difficult learning tasks. Typically this involves training several sub-models on the same task, and then averaging their predictions in a semantically meaningful way to yield one true prediction model, which often has higher accuracy than any singular sub-model.\nThe Wasserstein barycenter provides one approach to accomplishing this core task (Dognin et al. 2019) while maintaining a high level of semantic meaning. Sub-models are interpreted as defining a distribution over some label space, rather than a singular output, and the ensemble model defines the Wasserstein barycenter of these distributions over label space. This allows for interesting analysis of the entire ensemble distribution, potentially useful in hard classification tasks."
  },
  {
    "objectID": "Kantorovich_Problem.html",
    "href": "Kantorovich_Problem.html",
    "title": "Kantorovich Problem",
    "section": "",
    "text": "The Kantorovich problem (Villani 2003) is one of the two essential minimization problems in optimal transport (the other being the Monge problem). It is named after Russian mathematician and Nobel Laureate Leonid Kantorovich."
  },
  {
    "objectID": "Kantorovich_Problem.html#shipping-problem",
    "href": "Kantorovich_Problem.html#shipping-problem",
    "title": "Kantorovich Problem",
    "section": "Shipping problem",
    "text": "Shipping problem\nThe intuition behind the Kantorovich problem can be given by an explanation of optimizing shipments. Suppose there is a merchant who is attempting to ship items from one place to another. The merchant can hire trucks at some cost \\(c(x, y)\\) for each unit of merchandise which is shipped from point \\(x\\) to point \\(y\\). Now the shipper is approached by a mathematician, who claims that prices can be set such that they align with the shipper’s financial interests (Carlier 2010). This would be achieved by setting the price \\(\\phi(x)\\) and \\(\\phi(y)\\) such that the sum of \\(\\phi(x)\\) and \\(\\phi(y)\\) is always less than the cost \\(c(x, y)\\). This may even involve setting negative prices in certain cases. However, it can be shown that the shipper will spend almost as much as they would have if instead they opted for the original pricing method (Paris 2016)."
  },
  {
    "objectID": "Kantorovich_Problem.html#transport-plans",
    "href": "Kantorovich_Problem.html#transport-plans",
    "title": "Kantorovich Problem",
    "section": "Transport Plans",
    "text": "Transport Plans\nThe Monge problem was about the optimal way to rearrange mass (Craig 2020). Note that in the Monge formulation of the optimal transport problem, the mass cannot be split and thus it is mapped \\(x \\mapsto T(x)\\). When considering discrete cases, this results in problems when trying to establish maps T such that \\(T_{\\#} \\mu=\\nu\\). Kantorovich made the observation that the mass in question could be split, which makes the problem much easier to model  Craig, Katy. The Kantorovich Problem. Math 260L. Univ. of Ca. at Santa Barbara. Spring 2020 . Allowing the mass to be split results in a relaxation of the problem (e.g. half of the mass from \\(x_1\\) can go to \\(y_1\\) and half can go to \\(y_2\\), and so on). To model this consider \\(d \\pi(x, y)\\), which denotes the mass transported from x to y. This allows the mass to be moved to multiple places. Also consider \\(\\mu(A)\\) and \\(\\nu(B)\\): where the total mass taken from measurable set \\(A \\in X\\) must be equal to \\(\\mu(A)\\) and the total mass taken from measurable set \\(B \\in Y\\) must equal \\(\\nu(B)\\).\nThe constraints of the problem can be written in the following manner:\n\\[\n\\pi(A \\times Y)=\\mu(A)\n\\]\n\\[\n\\pi(X \\times B)=\\nu(B)\n\\]\nfor all measurable sets \\(A \\subseteq X, B \\subseteq Y\\). As such, we can interpret \\(\\pi( A\\times B)\\) as representing the amount of mass from \\(\\mu (A)\\) that is directed to \\(\\nu (B)\\)\nIf we have a measure \\(\\pi\\) that satisfies these constraints, then the set of such \\(\\pi\\) is referred to as \\(\\Pi(\\mu, \\nu)\\) – the set of transport plans between \\(\\mu\\) and \\(\\nu\\). Notice again that now we are dealing with transport plans instead of the transport maps that are used in the Monge formulation of the problem  Craig, Katy. The Kantorovich Problem. Math 260L. Univ. of Ca. at Santa Barbara. Spring 2020 ."
  },
  {
    "objectID": "Kantorovich_Problem.html#problem-statement",
    "href": "Kantorovich_Problem.html#problem-statement",
    "title": "Kantorovich Problem",
    "section": "Problem Statement",
    "text": "Problem Statement\nGiven \\(\\mu \\in \\mathcal{P}(X)\\) and \\(\\nu \\in \\mathcal{P}(Y)\\), solve\n\n\\(\\operatorname{min} \\mathbb{K}(\\pi):= \\operatorname{min} \\int_{X \\times Y} c(x, y) \\mathrm{d} \\pi(x, y)\\)\n\nover all such \\(\\pi \\in \\Pi(\\mu, \\nu)\\)\nAssuming there is a transport map \\(T^{\\dagger}: X \\rightarrow Y\\) for the Monge problem, we define \\(\\mathrm{d} \\pi(x, y)=\\mathrm{d} \\mu(x) \\delta_{y=T^{\\dagger}(x)}\\). Using this we can see that:\n\n\\(\\begin{aligned} \\pi(A \\times Y) &=\\int_{A} \\delta_{T^{\\dagger}(x) \\in Y} \\mathrm{d} \\mu(x)=\\mu(A) \\\\ \\pi(X \\times B) &=\\int_{X} \\delta_{T^{\\dagger}(x) \\in B} \\mathrm{d} \\mu(x)=T_{\\#}^{\\dagger} \\mu(B)=\\nu(B) \\end{aligned}\\)\n\nWe can see that \\(\\int_{X \\times Y} c(x, y) \\mathrm{d} \\pi(x, y)=\\int_{X} c\\left(x, T^{\\dagger}(x)\\right) \\mathrm{d} \\mu(x)\\)\nthus \\(\\inf \\mathbb{K}(\\pi) \\leq \\inf \\mathbb{M}(T)\\)."
  },
  {
    "objectID": "Kantorovich_Problem.html#kantorovich-duality",
    "href": "Kantorovich_Problem.html#kantorovich-duality",
    "title": "Kantorovich Problem",
    "section": "Kantorovich Duality",
    "text": "Kantorovich Duality\nSince the Kantorovich problem is a linear minimization problem with convex constraints it admits a dual problem. The astute reader may notice that this is a linear programming problem – Kantorovich is also considered to be the founder of linear programming."
  },
  {
    "objectID": "Kantorovich_Problem.html#calculus-of-variations-approach",
    "href": "Kantorovich_Problem.html#calculus-of-variations-approach",
    "title": "Kantorovich Problem",
    "section": "Calculus of Variations Approach",
    "text": "Calculus of Variations Approach\nUnder the right setting, one can show the Kantovorich problem indeed has a minimizer using the direct method of the calculus of variations. More specifically, if one turns to the narrow topology, then it turns out that we get compactness of the constraint set. Moreover, such a topology ensures us that our objective function is lower semi-continuous."
  },
  {
    "objectID": "Kantorovich_Problem.html#knott-smith-optimality-criterion",
    "href": "Kantorovich_Problem.html#knott-smith-optimality-criterion",
    "title": "Kantorovich Problem",
    "section": "Knott-Smith Optimality Criterion",
    "text": "Knott-Smith Optimality Criterion\nOne useful result we have that allows us to connect both the Monge and Kantorovich problems is the so-called the Knott-Smith Optimality Criterion (see below)."
  },
  {
    "objectID": "Continuous-time MOT and Skorohod Embedding.html",
    "href": "Continuous-time MOT and Skorohod Embedding.html",
    "title": "Continuous-Time MOT and Skorokhod Embedding",
    "section": "",
    "text": "This article is an extension of the previous Wiki article Martingale optimal transport and mathematical finance. We set up a robust and dynamic connection to bridge martingale optimal transport (MOT) with continuous-time mathematical finance, providing a model-independent framework for robust hedging, without assuming a specific stochastic model for the asset dynamics. The main technical difficulty in the continuous-time setting is the construction of a martingale coupling measure on the space of sample paths, e.g., \\(C([0, T], \\mathbb{R})\\); or equivalently, the construction of a continuous martingale price process matching all the given marginals. The Skorokhod embedding problem (SEP) plays the role of converting this technical difficulty into the problem of finding a stopping time for a Brownian motion that reproduces the target marginals. By illustrating several explicit SEP solutions, we give applications to robust hedging and pricing in the settings of continuous-time MOT.\nThis article requires basic knowledge of stochastic analysis, mainly stochastic calculus and Itô’s formula. The main reference is (henrylabordere2017model?), Chapter 4."
  },
  {
    "objectID": "Continuous-time MOT and Skorohod Embedding.html#continuous-time-mot",
    "href": "Continuous-time MOT and Skorohod Embedding.html#continuous-time-mot",
    "title": "Continuous-Time MOT and Skorokhod Embedding",
    "section": "Continuous-time MOT",
    "text": "Continuous-time MOT\nThe setting of continuous-time MOT is similar to the discrete case in the previous Wiki article Martingale optimal transport and mathematical finance, except the underlying price process is continuous. We describe it via a probability measure on a path space, e.g., \\(C([0, T], \\mathbb{R}_+)\\). Throughout this article, we assume a zero interest rate.\n\nProbabilistic setup\nLet \\(\\Omega \\equiv\\left\\{\\omega \\in C\\left([0, T], \\mathbb{R}_{+}\\right): \\omega_0=0\\right\\}\\) be the canonical space equipped with the uniform norm \\(\\|\\omega\\|_{\\infty} \\equiv \\sup _{0 \\leq t \\leq T}|\\omega(t)|, B\\) the canonical process, i.e., \\(B_t(\\omega) \\equiv \\omega(t)\\) and \\(\\mathcal{F} \\equiv\\left\\{\\mathcal{F}_t\\right\\}_{0 \\leq t \\leq T}\\) the filtration generated by \\(B: \\mathcal{F}_t=\\) \\(\\sigma\\left\\{B_s, s \\leq t\\right\\}. \\mathbb{P}^0\\) is the Wiener measure. \\(S_0\\) is some given initial value in \\(\\mathbb{R}_{+}\\), and we denote\n\\[\nS_t \\equiv S_0+B_t \\text { for } t \\in[0, T]\n\\]\nFor any \\(\\mathcal{F}\\)-adapted process \\(\\sigma\\) and satisfying \\(\\int_0^T \\sigma_s^2 d s&lt;\\infty, \\mathbb{P}^0\\)-a.s., we define the probability measure on \\((\\Omega, \\mathcal{F})\\) :\n\\[\n\\mathbb{P}^\\sigma \\equiv \\mathbb{P}^0 \\circ\\left(S^\\sigma\\right)^{-1} \\text { where } S_t^\\sigma \\equiv S_0+\\int_0^t \\sigma_r d B_r, t \\in[0, T], \\mathbb{P}^0-\\text { a.s. }\n\\]\nThen \\(S\\) is a \\(\\mathbb{P}^\\sigma\\)-local martingale. We denote by \\(\\mathcal{M}^c\\) the collection of all such probability martingale measures on \\((\\Omega, \\mathcal{F})\\), i.e.\n\\[\n\\mathbb{E}_{\\mathbb{P}^\\sigma}\\left[S_s \\mid \\mathcal{F}_t\\right]=S_t \\quad \\forall 0\\leq t\\leq s\\leq T.\n\\]\nIn other words, the set of all \\(S^{\\sigma}\\) is the set of all martingale price processes we consider and \\(\\mathcal{M}^c\\) is the set of all martingale probability measures induced on the canonical space by these price processes.\nThe quadratic variation process \\(\\langle S\\rangle=\\langle B\\rangle\\) takes values in the set of all nondecreasing continuous functions. Note that the quadratic variation can be defined pathwise as the limsup of the corresponding discrete counterpart with conveniently chosen mesh of the time partition. The dependence of the quadratic variation on the underlying probability measure \\(\\mathbb{P} \\in \\mathcal{M}^{\\text {c }}\\) can therefore be dropped. Finally, \\(\\mathcal{M}^c(\\mu) \\equiv\\left\\{\\mathbb{P}^\\sigma \\in \\mathcal{M}^c: S_T^\\sigma \\stackrel{\\mathbb{P}^\\sigma}{\\sim} \\mu\\right\\}\\) where \\(\\mu\\) is supported on \\(\\mathbb{R}_{+}\\). For the ease of notation, we will delete the superscript \\(\\sigma\\) on \\(S^\\sigma\\) below.\nAdditionally, for all \\(\\mathbb{P} \\in \\mathcal{M}^c\\), we denote the set of trading portfolios:\n\\[\n\\mathbb{H}_{\\mathrm{loc}}^2(\\mathbb{P}) \\equiv\\left\\{H \\in \\mathbb{H}^0(\\mathbb{P}): \\int_0^T H_t^2 d\\langle S\\rangle_t&lt;\\infty, \\mathbb{P}-\\text { a.s. }\\right\\}\n\\] Under the self-financing condition, for any admissible portfolio \\(H\\), the portfolio value process\n\\[\nY_t^H \\equiv Y_0+\\int_0^t H_s d S_s, t \\in[0, T]\n\\] is well-defined \\(\\mathbb{P}\\)-a.s. for every \\(\\mathbb{P} \\in \\mathcal{M}^c\\), whenever \\(H \\in \\mathbb{H}_{\\text {loc }}^2\\). In order to avoid doubling strategies, we introduce the set of admissible portfolios:\n\\[\n\\mathcal{H} \\equiv\\left\\{H: H \\in \\mathbb{H}_{\\text {loc }}^2 \\text { and } Y^H \\text { is a } \\mathbb{P}-\\text { supermartingale for all } \\mathbb{P} \\in \\mathcal{M}^c\\right\\}\n\\]\n\n\nDuality formulation\nLet \\(\\xi\\) be the payoff of an option, defined as an \\(\\mathcal{F}_T\\)-measurable random variable. This means that the payoff might not depend only on the final price \\(S_T\\), but also on the whole price path. In addition to the continuous-time trading, we assume that the investor can take static positions in Vanilla options with maturities \\(\\left(t_i\\right)_{i=1, \\ldots, n}\\). From the theory of , \\(t_i\\)-Vanilla defined by the payoff \\(\\lambda_i\\left(S_{t_i}\\right) \\in \\mathrm{L}^1\\left(\\mathbb{P}^i\\right)\\) has an unambiguous market price given by \\(\\mathbb{E}^{\\mathbb{P}^i}\\left[\\lambda_i\\left(S_{t_i}\\right)\\right]\\), see also . The robust super-replication price is then defined by:\n\\[\n\\begin{array}{r}\n\\operatorname{MK}_n^{\\mathrm{c}}\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right) \\equiv \\inf \\left\\{Y_0: \\exists\\left(\\lambda_i \\in \\mathrm{~L}^1\\left(\\mathbb{P}^i\\right)\\right)_{i=1, \\ldots, n} \\text { and } H \\in \\mathcal{H}\\right. \\\\\n\\left.\\bar{Y}_T^{H, \\lambda} \\geq \\xi, \\mathbb{P}-\\text { a.s. for all } \\mathbb{P} \\in \\mathcal{M}^c\\right\\}\n\\end{array}\n\\] where \\(\\bar{Y}^{H, \\lambda}\\) denotes the portfolio value of a self-financing strategy with continuous trading \\(H\\) in the underlying, and static trading \\(\\left(\\lambda_i\\right)_{i=1, \\ldots, n}\\) in the \\(t_i\\)-Vanillas:\n\\[\n\\bar{Y}_T^{H, \\lambda} \\equiv Y_0+\\int_0^T H_s d S_s+\\sum_{i=1}^n \\lambda_i\\left(S_{t_i}\\right)-\\sum_{i=1}^n \\mathbb{E}^{\\mathbb{P}^i}\\left[\\lambda_i\\left(S_{t_i}\\right)\\right]\n\\]\nThe financial interpretation is the following: The investor buys at time 0 any Vanilla with payoff \\(\\lambda_i\\left(S_{t_i}\\right)\\) for the price \\(\\mathbb{E}^{\\mathbb{P}^i}\\left[\\lambda_i\\left(S_{t_i}\\right)\\right]\\), while trading using the portfolio \\(H\\). That \\(\\bar{Y}_T^{H, \\lambda} \\geq \\xi,\\) \\(\\mathbb{P}-\\) a.s. for all \\(\\mathbb{P} \\in \\mathcal{M}^c\\) essentially means that under all possible price process, the final wealth of our self-financing strategy is larger than the pay-off of the option \\(\\xi\\) with probability one. Thus, \\(\\mathrm{MK}_n^c\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right)\\) is an upper bound on the price of \\(\\xi\\) necessary for absence of strong (model-independent) arbitrage opportunities: selling \\(\\xi\\) at a higher price, the hedger could set up a portfolio with a negative initial cost and a non-negative payoff under any market scenario.\nSimilarly, in the case of continuous-time static hedging in Vanillas, we define\n\\[\n\\begin{array}{r}\n\\mathrm{MK}_{\\infty}^{\\mathrm{c}}\\left(\\left(\\mathbb{P}^t\\right)_{t \\in(0, T]}\\right) \\equiv \\inf \\left\\{Y_0: \\exists\\left(\\lambda(t, \\cdot) \\in \\mathrm{L}^1\\left(\\mathbb{P}^t\\right)\\right)_{t \\in(0, T]} \\text { and } H \\in \\mathcal{H}\\right. \\\\\n\\left.\\bar{Y}_T^{H, \\lambda} \\geq \\xi, \\mathbb{P}-\\text { a.s. for all } \\mathbb{P} \\in \\mathcal{M}^c\\right\\},\n\\end{array}\n\\] where \\(t \\mapsto \\mathbb{E}^{\\mathbb{P}^t}\\left[\\lambda\\left(t, S_t\\right)\\right] \\in \\mathrm{L}^1([0, T])\\) and \\[\n\\bar{Y}_T^{H, \\lambda} \\equiv Y_0+\\int_0^T H_s d S_s+\\int_0^T \\lambda\\left(t, S_t\\right) d t-\\int_0^T \\mathbb{E}^{\\mathrm{P}^t}\\left[\\lambda\\left(t, S_t\\right)\\right] d t\n\\]\nNow we give the key result from the perspective of OT, the dual formulation of the above robust superhedging price:\nProposition 4.1 (henrylabordere2017model?)\nAssume \\(\\sup_{\\mathbb{P} \\in \\mathcal{M}^c} \\mathbb{E}^\\mathbb{P}[\\xi^+] &lt; \\infty\\). Then:\n\n\\[\n\\operatorname{MK}_n^c(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n) = \\inf_{(\\lambda_i)} \\sum_{i=1}^n \\mathbb{E}^{\\mathbb{P}^i}[\\lambda_i(S_{t_i})] + \\sup_{\\mathbb{P} \\in \\mathcal{M}^c} \\mathbb{E}^\\mathbb{P}\\left[\\xi - \\sum_{i=1}^n \\lambda_i(S_{t_i})\\right].\n\\]\n\\[\n\\operatorname{MK}_\\infty^c((\\mathbb{P}^t)) = \\inf_{\\lambda(t,\\cdot)} \\int_0^T \\mathbb{E}^{\\mathbb{P}^t}[\\lambda(t, S_t)] dt + \\sup_{\\mathbb{P} \\in \\mathcal{M}^c} \\mathbb{E}^\\mathbb{P}\\left[\\xi - \\int_0^T \\lambda(t, S_t) dt\\right].\n\\]\n\nProof: See (galichon2014stochastic?; guo2016optimal?).\n\n\nAn intuitive duality formulation assuming a minimax argument\nThe above dual formulation might seem less intuitive at first glance. Now we give a more intuitive dual formulation assuming a minimax argument holds.\nTaking for granted that we can permute the supremum over \\(\\mathbb{P} \\in \\mathcal{M}^c\\) and the infimum over \\(\\left(\\lambda_i \\in \\mathrm{~L}^1\\left(\\mathbb{P}^i\\right)\\right)_{i=1, \\ldots, n}\\), we get \\[\n\\operatorname{MK}_n^{\\mathrm{c}}\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right)=\\sup _{\\mathbb{P} \\in \\mathcal{M}^c\\left(\\lambda_i \\in \\mathrm{~L}^1\\left(\\mathbb{P}^i\\right)\\right)_{i=1, \\ldots, n}} \\sum_{i=1}^n \\mathbb{E}^{\\mathbb{P}^i}\\left[\\lambda_i\\left(S_{t_i}\\right)\\right] +\\mathbb{E}^{\\mathbb{P}}\\left[\\xi-\\sum_{i=1}^n \\lambda_i\\left(S_{t_i}\\right)\\right]\n\\] Then, taking the infimum over \\(\\left(\\lambda_i \\in \\mathrm{~L}^1\\left(\\mathbb{P}^i\\right)\\right)_{i=1, \\ldots, n}\\), we deduce \\[\\begin{equation}\\label{4.8}\n    \\operatorname{MK}_n^{\\mathrm{c}}\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right)=\\sup _{\\mathbb{P} \\in \\mathcal{M}^c\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right)} \\mathbb{E}^{\\mathbb{P}}[\\xi]\n\\end{equation}\\] where \\[\n\\mathcal{M}^c\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right) \\equiv\\left\\{\\mathbb{P} \\in \\mathcal{M}^c: S_{t_i} \\stackrel{\\mathbb{P}}{\\sim} \\mathbb{P}^i, \\quad \\forall i=1, \\ldots, n\\right\\}\n\\] Similarly, taking for granted that we can permute the supremum over \\(\\mathbb{P} \\in \\mathcal{M}^c\\) and the infimum over \\(\\lambda(t, \\cdot) \\in \\mathrm{L}^1\\left(\\mathbb{P}^t\\right), \\forall t \\in(0, T]\\), we get \\[\\begin{equation}\\label{4.10}\n\\mathrm{MK}_{\\infty}^{\\mathrm{c}}\\left(\\left(\\mathbb{P}^t\\right)_{t \\in(0, T]}\\right)=\\sup _{\\mathbb{P} \\in \\mathcal{M}^c\\left(\\left(\\mathbb{P}^t\\right)_{t \\in(0, T]}\\right)} \\mathbb{E}^{\\mathbb{P}}[\\xi]\n\\end{equation}\\] where \\[\n\\mathcal{M}^c\\left(\\left(\\mathbb{P}^t\\right)_{t \\in(0, T]}\\right) \\equiv\\left\\{\\mathbb{P} \\in \\mathcal{M}^c: S_t \\stackrel{\\mathbb{P}}{\\sim} \\mathbb{P}^t, \\quad \\forall t \\in(0, T]\\right\\}\n\\]\nSo providing we could justify this minimax argument, our robust superhedging is connected to a MOT: we maximize the cost \\(\\mathbb{E}^{\\mathbb{P}}[\\xi]\\) over the space of martingale measures with marginals \\(\\left(\\mathbb{P}^i\\right)_{i=1, \\ldots, n}\\left(\\right.\\) or \\(\\left.\\left(\\mathbb{P}^t\\right)_{t \\in(0, T]}\\right)\\) and \\(\\mathbb{P}^0=\\delta_{S_0}\\). If the dual is attained, \\(\\mathrm{MK}_n^{\\mathrm{c}}\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right)=\\mathbb{E}^{\\mathbb{P}^*}[\\xi]\\) should be attained by a martingale measure \\(\\mathbb{P}^* \\in\\) \\(\\mathcal{M}^c\\left(\\mathbb{P}^1, \\ldots, \\mathbb{P}^n\\right)\\). Similarly, \\(\\mathrm{MK}_{\\infty}^{\\mathrm{c}}\\left(\\mathbb{P}^t\\right)\\) should be attained by a martingale measure in \\(\\mathcal{M}^c\\left(\\left(\\mathbb{P}^t\\right)_{t \\in(0, T]}\\right)\\). We note that this corresponds to the core idea of option pricing in mathematical finance: the price of an option from the seller’s perspective (the right-hand side of the equations) should coincide with its price from the buyer’s perspective (the left-hand side of the equations). In the rest of this article, we mean the above strong dualities when we say the dual formulation.\nRemark: Although we do not provide proofs for the above duality, we mention that the mathematical justification of the dualities above has been covered by various authors under various assumptions on the canonical space \\(\\Omega\\) and on \\(\\xi\\). See for example (beiglboeck2015complete?; bouchard2015arbitrage?; dolinsky2014robust?; dolinsky2014robusttc?; dolinsky2016martingale?; guo2016optimal?)."
  },
  {
    "objectID": "Continuous-time MOT and Skorohod Embedding.html#link-with-skorokhod-embedding-problem",
    "href": "Continuous-time MOT and Skorohod Embedding.html#link-with-skorokhod-embedding-problem",
    "title": "Continuous-Time MOT and Skorokhod Embedding",
    "section": "Link with Skorokhod embedding problem",
    "text": "Link with Skorokhod embedding problem\nIn this section, we assume that the payoff \\(\\xi\\left(S_T, M_T, m_T,\\langle S\\rangle_T\\right)\\) depends on the spot \\(S_T\\), the running maximum \\(M_T\\), minimum \\(m_T\\) or the quadratic variation \\(\\langle S\\rangle_T\\) at \\(T\\).\nTo link the strong dual formula above with the Skorohod embedding problem, we first review the classical Dubins–Schwarz theorem from stochastic analysis, indicating that every continuous martingale is a stochastic time-change of a standard Brownian motion:\nLemma (Dubins–Schwarz)\nLet \\(M\\) be a continuous martingale with respect to a filtration \\(\\mathcal{F}_t\\) and \\(\\langle M\\rangle_{\\infty}=\\infty\\) and define for all \\(t \\geq 0\\) the time-changes (i.e. stopping times) \\[\nT_t=\\inf \\left\\{s:\\langle M\\rangle_s&gt;t\\right\\}\n\\] Then \\(B:=\\left(B_t\\right):=\\left(M_{T_t}\\right)\\) is a \\(\\mathcal{F}_{T_t}\\)-Brownian motion and \\(\\left(M_t\\right)=\\left(B_{\\langle M\\rangle_t}\\right)\\).\nA proof of this lemma can be found in any textbook on stochastic analysis. Moreover, applying this lemma to the payoff \\(\\xi\\left(S_T, M_T, m_T,\\langle S\\rangle_T\\right)\\), the dual price given one marginal,\\(\\mathrm{MK}_1^c(\\mu)\\) as given by strong dual formulation above, can be framed as a constrained perpetual American options: \\[\\begin{equation}\\label{SEPinvolved}\n    \\operatorname{MK}_1^c(\\mu)=\\sup _{\\tau \\in \\mathcal{T}: B_\\tau \\sim \\mu} \\mathbb{E}^{\\mathbb{P}}\\left[\\xi\\left(B_\\tau, \\max _{0 \\leq s \\leq \\tau} B_s, \\min _{0 \\leq s \\leq \\tau} B_s, \\tau\\right)\\right]\n\\end{equation}\\] This problem corresponds then to the determination of an (optimal) stopping time \\(\\tau^*\\) such that \\(B_{\\tau^*} \\sim \\mu\\). This is a Skorokhod embedding problem (in short SEP). More precisely,\nDefinition (Skorokhod Embedding Problem)\nFind a stopping time \\(\\tau\\) such that \\(B_\\tau \\sim \\mu\\) and \\(B^\\tau \\equiv\\left(B_{t \\wedge \\tau}\\right)_{t \\geq 0}\\) is uniformly integrable.\n\nExplicit SEP solutions\nIn this section, we cite several explicit solutions to SEP. For more details, see Section 4.6. We set \\(M_t \\equiv \\max _{0&lt;s&lt;t} B_t\\) and \\(m_t \\equiv \\min _{0&lt;s&lt;t} B_t\\).\nAzéma–Yor Solution (henrylabordere2017model?)\nDefine \\(\\tau_{AY} = \\inf \\{ t \\geq 0 : B_t \\leq \\psi_\\mu(M_t) \\}\\) with \\(\\psi_\\mu^{-1}(x) = \\frac{\\mathbb{E}^\\mu[S_T 1_{S_T \\geq x}]}{\\mathbb{E}^\\mu[1_{S_T \\geq x}]}\\). Then \\(B_{\\tau_{AY}} \\sim \\mu\\).\nAs the strong dual formulation of the option price requires finding an optimal solution of the SEP under some loss function, we also cite the following optimality solution of the Az`ema-Yor solution.\nOptimality: For increasing \\(g\\), \\(\\mathbb{E}[g(M_\\tau)] \\leq \\mathbb{E}[g(M_{\\tau_{AY}})]\\) (henrylabordere2017model?).\nPerkins Solution(perkins1986cereteli?)\nDefine \\(\\tau_{\\text{Perkins}} = \\inf \\{ t &gt; 0 : B_t \\notin (\\gamma_+(M_t), \\gamma_-(m_t)) \\}\\) with \\(\\gamma_{+}(j)=\\operatorname{argmax}_{x&lt;S_0} \\frac{C^\\mu(j)-P^\\mu(x)}{j-x}\\) and \\(\\gamma_{-}(i)=\\operatorname{argmin}_{x&gt;S_0} \\frac{P^\\mu(i)-C^\\mu(x)}{x-i}\\). Here \\(C^\\mu(x) \\equiv \\mathbb{E}^\\mu\\left[\\left(S_T-x\\right)^{+}\\right]\\) and \\(P^\\mu(x) \\equiv \\mathbb{E}^\\mu\\left[\\left(x-S_T\\right)^{+}\\right]\\). Then \\(B_{\\tau_{Perkins}} \\sim \\mu\\). The Perkins embedding has the property that it simultaneously minimizes the law of the maximum \\(M\\) and maximizes the law of the minimum \\(m\\) :\nOptimality: Let \\(\\tau\\) be a solution to SEP. Then for all increasing function \\(g\\), \\[\n\\begin{aligned}\n\\mathbb{E}\\left[g\\left(M_\\tau\\right)\\right] & \\geq \\mathbb{E}\\left[g\\left(M_{\\tau_{\\text {Perkins }}}\\right)\\right] \\\\\n\\mathbb{E}\\left[g\\left(m_\\tau\\right)\\right] & \\leq \\mathbb{E}\\left[g\\left(m_{\\tau_{\\text {Perkins }}}\\right)\\right].\n\\end{aligned}\n\\]\nVallois’ Solution (vallois1983probleme?; vallois1992quelques?)\nVallois’ embedding uses the local time \\(L_t^a\\) of the Brownian motion at a level \\(a\\). The stopping time is given by\n\\[\n\\tau_{\\mathrm{V}} = \\inf\\left\\{ t \\ge 0 : \\int_0^t \\varphi(B_s)\\,dL_s^a \\ge \\lambda \\right\\},\n\\]\nwith \\(\\varphi\\) and \\(\\lambda\\) chosen to ensure\n\\[\nB_{\\tau_{\\mathrm{V}}} \\sim \\mu.\n\\] Details can be found in (henrylabordere2017model?).\n\n\nApplication to lookback options\nApplying the optimality results above to special types of payoff \\(\\xi\\) (more precisely, lookback options) in the strong dual pricing formula, we obtain the following robust pricing formula.\nCorollary\nSuppose that the payoff \\(\\xi\\) depends non-decreasingly only on the running maximum of the price process, i.e., \\(\\xi = g(\\max_{0\\leq t\\leq T} S_t)\\), the, we have \\[\\begin{equation}\n        \\mathrm{MK}_1^c(\\mu) = \\mathbb{E}[g(\\max_{0\\leq t\\leq \\tau_{AY}} B_t)]\n    \\end{equation}\\] where \\(B\\) is a Brownian motion. Similarly, suppose that the payoff \\(\\xi\\) depends non-decreasingly only on the running minimum of the price process, i.e., \\(\\xi = g(\\min_{0\\leq t\\leq T} S_t)\\), the, we have \\[\\begin{equation}\n        \\mathrm{MK}_1^c(\\mu) = \\mathbb{E}[g(\\min_{0\\leq t\\leq \\tau_{Perkins}} B_t)]\n    \\end{equation}\\] where \\(B\\) is a Brownian motion.\nProof: Follows from the optimality of Azéma–Yor and Perkins embeddings.\nAt the end of this section, we emphasize the significance of this result. The above robust pricing formulas for lookback options significantly simplify the numerical procedure. Previously, one had to optimize over an entire family of hedging strategies or martingale measures to determine the robust price. Now, by leveraging the optimality of the Azéma–Yor and Perkins embeddings, the pricing problem is reduced to simulating a standard Brownian motion along with the corresponding stopping time. This transformation—from a complex, high-dimensional optimization problem to a simulation-based approach—greatly enhances the tractability and efficiency of computing robust prices for path-dependent options."
  },
  {
    "objectID": "Continuous-time MOT and Skorohod Embedding.html#conclusion",
    "href": "Continuous-time MOT and Skorohod Embedding.html#conclusion",
    "title": "Continuous-Time MOT and Skorokhod Embedding",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we have developed a dynamic formulation of continuous-time martingale optimal transport and its connection with the Skorokhod embedding problem. Starting from a robust hedging problem, we derived a dual minimax formulation of the pricing formula. We then showed how explicit SEP solutions can be used to price lookback options and indicate the significance.\nThe following topics are not sufficiently discussed in this article (or in existing research) and we refer interested readers to (henrylabordere2017model?) (or independent research) for some more details: First, our SEP approach allows matching only one marginal; general SEP, however, includes embedding several marginals into a Brownian motion via stopping times; it is worth a finer study of the general SEP in the setting of continuous-time MOT; the author believes that extension to matching a finite number of marginals shall not bring new technical difficulties, however, matching continuous marginals might require new techniques. Second, the application of SEP in pricing is currently only limited to certain lookback options; it seems rather hard to extend the application to more complex options since the optimality results above are in general hard to acquire for general option payoffs. Third, in the SEP approach, the price is obtained by computing an optimal stopping time; it is not discussed what optimal price process (and what hedging strategy) this stopping time corresponds to; fortunately, there are some discussions on this topic in (henrylabordere2017model?) Section 4.5 and Section 4.7."
  },
  {
    "objectID": "Continuous-time MOT and Skorohod Embedding.html#references",
    "href": "Continuous-time MOT and Skorohod Embedding.html#references",
    "title": "Continuous-Time MOT and Skorokhod Embedding",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Kantorovich_Dual_Problem_Quad_Cost.html",
    "href": "Kantorovich_Dual_Problem_Quad_Cost.html",
    "title": "Kantorivich Dual Problem with Quadratic Cost",
    "section": "",
    "text": "The Kantorovich dual problem is a common reformulation of the Kantorovich problem. The case \\(c(x,y) =\\frac{1}{2} |x-y|^2\\) is an important special case for several reasons. First off, the minimum cost is the 2-Wasserstein distance. From the point of view of mathematical analysis, the quadratic cost is special because the \\(c\\)-transform for \\(c(x,y)= \\frac{1}{2} |x-y|^2\\) is closely related to convex conjugates, and many tools from convex analysis can be used. Moreover, in the quadratic case the Monge optimal transport map is of the form \\(T = \\nabla \\phi\\) for a convex function \\(\\phi\\) (as discussed in the section on Brenier’s theorem)."
  },
  {
    "objectID": "Kantorovich_Dual_Problem_Quad_Cost.html#definition-of-the-c-transform-and-c-concavity",
    "href": "Kantorovich_Dual_Problem_Quad_Cost.html#definition-of-the-c-transform-and-c-concavity",
    "title": "Kantorivich Dual Problem with Quadratic Cost",
    "section": "Definition of the c-Transform and c-Concavity",
    "text": "Definition of the c-Transform and c-Concavity\nLet \\(X\\) and \\(Y\\) be Polish spaces, and let \\(c: X \\times Y \\to \\mathbb{R} \\cup \\{ + \\infty\\}\\) be a measurable function. The \\(c\\)-transform of \\(f:X \\to \\mathbb{R} \\cup \\{ -\\infty\\}\\) is defined by \\[ f^c (y) = \\inf_{ x \\in X} \\left( c(x,y) - f(x) \\right)\\] the \\(\\overline{c}\\)-transform of \\(g:Y \\to \\mathbb{R} \\cup \\{ - \\infty\\}\\) is \\[g^{\\overline{c}}(x) = \\inf_{y \\in Y} \\left(c(x,y) - g(y)\\right).\\] Moreover, a function \\(f\\) is called \\(c\\)-concave if there exists \\(g:Y \\to \\mathbb{R} \\cup \\{ - \\infty\\}\\) such that \\(f = g^{\\overline{c}}\\).\nA comment: when \\(c\\) is symmetric (i.e., \\(X=Y\\) and \\(c(x,y) = c(y,x)\\) for all \\(x,y \\in X=Y\\)), the \\(c\\)-transform and \\(\\overline{c}\\)-transform are identical. In this case we will just call them both the \\(c\\)-transform."
  },
  {
    "objectID": "Kantorovich_Dual_Problem_Quad_Cost.html#definition-of-the-convex-conjugate",
    "href": "Kantorovich_Dual_Problem_Quad_Cost.html#definition-of-the-convex-conjugate",
    "title": "Kantorivich Dual Problem with Quadratic Cost",
    "section": "Definition of the convex conjugate",
    "text": "Definition of the convex conjugate\nSuppose \\(f: \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}\\). The convex conjugate of \\(f\\) is defined by \\[f^*(y) = \\sup_{x \\in \\mathbb{R}^d} \\left( x \\cdot y - f(x)\\right).\\] Here, \\(x \\cdot y\\) is the dot product of \\(x\\) and \\(y\\). The map \\(f \\mapsto f^*\\) is called the Legendre transform."
  },
  {
    "objectID": "Kantorovich_Dual_Problem_Quad_Cost.html#the-dual-problem",
    "href": "Kantorovich_Dual_Problem_Quad_Cost.html#the-dual-problem",
    "title": "Kantorivich Dual Problem with Quadratic Cost",
    "section": "The Dual problem",
    "text": "The Dual problem\nLet \\(X\\) and \\(Y\\) be Polish spaces. Let \\(\\mu\\) and \\(\\nu\\) be probability measures on \\(X\\) and \\(Y\\) respectively. Let \\(c: X \\times Y \\to [0,\\infty]\\) be lower semicontinuous. The dual problem of the Kantorovich problem is \\[\n\\sup_{f(x)+g(y) \\leq c(x,y)} \\int f(x) d \\mu(x) + \\int g(y) d \\nu(y)\n\\tag{1}\\] where the supremum is take over all \\(f \\in L^1(\\mu), g \\in L^1(\\nu)\\) such that \\(f(x)+g(y) \\leq c(x,y)\\).\nThere is one thing here that isn’t standard: sometimes you take instead \\(f \\in C_b(X)\\), \\(g \\in C_b(Y)\\) instead of \\(f \\in L^1(\\mu), g \\in L^1(\\nu)\\). That is, you maximize over the smaller set of continuous and bounded functions. Whenever \\(X\\) and \\(Y\\) are Polish spaces \\(C_b\\) is dense in \\(L^1\\) so the supremum is the same. However, working in \\(L^1\\) is can be convenient because sometimes maximizers \\((f,g)\\) of Equation 1 are non-continuous \\(L^1\\) functions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Contributors wanted!\nDo you want to help others learn about optimal transport? Do you need a concrete goal to motivate yourself to learn about a new topic? Please consider becoming a contributor to our wiki!\nYour contributions will be gratefully recognized in footers of each page. Please contact Katy Craig to receive an account.\nThis wiki is maintained by Katy Craig with help from the awesome students from Math 260L in Spring 202, Math 201A in Fall 2020, and Math 260J in Winter 2022."
  },
  {
    "objectID": "RegularityOTMap.html",
    "href": "RegularityOTMap.html",
    "title": "Regularity of Optimal Transport Maps and the Monge-Ampére Equation on Riemannian Manifolds",
    "section": "",
    "text": "When considering the [[Monge Problem]], it is natural to ask about the regularity of optimal transport maps (when they exist). In particular, we can consider the Monge Problem variant\n\\(\\inf_{T} \\left \\{ F(T) := \\int_{X} |x - T(x)|^2 d \\mu \\right \\}\\)\nwhere we are taking the infimum over all transport plans for our associated measures. The transport maps, \\(T\\), have the condition \\(\\nu = T \\# \\mu\\) where \\(\\nu\\) and \\(\\mu\\) are probability measures. One can reformulate the Monge problem into a boundary value problem for a specific partial differential equation. From there, one can ask about the regularity of solutions to the PDE, which are associated with optimal transport plans. In this exposition, we follow the references from Santambrogio and Figalli."
  },
  {
    "objectID": "RegularityOTMap.html#the-monge-ampère-equation",
    "href": "RegularityOTMap.html#the-monge-ampère-equation",
    "title": "Regularity of Optimal Transport Maps and the Monge-Ampére Equation on Riemannian Manifolds",
    "section": "The Monge Ampère Equation",
    "text": "The Monge Ampère Equation\nThe Monge Ampère Equation is a nonlinear second-order elliptic partial differential equation. Let us consider the [[Monge Problem]] from earlier, with starting measure \\(\\mu\\), a target measure \\(\\nu\\). If we require \\(\\mu\\) and \\(\\nu\\) be absolutely continuous with respect to the Lebesgue measure, we can show that a transport plan, \\(T\\) must satisfy the equation\n:\\(g(y) = \\frac{f(T^{-1}(y))}{\\det (DT(T^{-1}(y)))}\\)\nwhere \\(f(y)\\) and \\(g(y)\\) are the densities for \\(\\mu\\) and \\(\\nu\\) respectively. Recall that we are considering the Monge problem variant :\\(\\inf_{T} \\left \\{ F(T) := \\int_{X} |x - T(x)|^2 d \\mu \\right \\}\\). By Brenier’s Theorem, we know that \\(T = \\nabla u\\), where \\(u\\) is a convex function. If we require \\(u\\) to be strictly convex, substituting \\(\\nabla u\\) for \\(T\\) gives us the Monge Ampère equation\n:\\(\\det (D^2 u(x)) = \\frac{f(x)}{g(\\nabla u(x))}\\)\n\nFrom here, we can ask about regularity of solutions of to the PDE, which in turns gives us regularity on \\(T = \\nabla u\\). For example, we have the following theorem.\n\nTheorem. If \\(f\\) and \\(g\\) are \\(C^{0, \\alpha}(\\Omega)\\) and are both bounded from above and from below on the whole \\(\\Omega\\) by positive constants and \\(\\Omega\\) is a convex open set, then the unique Brenier solution of \\(u\\) of the Monge Ampère equation belongs to \\(C^{2, \\alpha} (\\Omega) \\cap C^{1, \\alpha} {\\overline{\\Omega}}\\), and \\(u\\) satisfies the equation in the classical sense. Here, a Brenier solution simply implies that \\(T\\) is a transport plan from \\(f\\) to \\(g\\).\n\n\nNote that the support of \\(g\\) is very relevant when trying to show regularity of our transport plan. Without any conditions on the support of \\(g\\), \\(T\\) may have singularities. To see this, consider the following simple example: let \\(f(x):=2\\) and \\(g(x):=2\\) be real functions supported on \\((-1,1)\\) and \\((-2,-1)\\cup(1,2)\\), respectively. The optimal transport map will be the gradient of the function \\(u(x):=|x|+\\frac{1}{2}x^2\\), which is convex but lacks the desired regularity. The issue here arises from the disconnectedness of the support of \\(g\\).\n\nOn the other hand, conditions on the support of \\(g\\) can eliminate such singularities. For example, Caffarelli has shown that if \\(f,g\\) are smooth and strictly positive on their support, and the support of \\(g\\) is convex, the optimal transport plan will be smooth in the support of \\(f\\). Moreover, if both supports are smooth and uniformly convex, one can show that the optimal transport plan is a smooth diffeomorphism between the support of \\(f\\) and the support of \\(g\\). In addition to the previous theorem, we get the following restated from Figalli:\n\nTheorem. Let \\(\\mu\\) and \\(\\nu\\) be two compactly supported probability measures on \\(R^n\\). If \\(\\mu\\) is absolutely continuous with respect to the Lebesgue measure, then: :* There exists a unique solution \\(T\\) to the Monge problem. :* The optimal map \\(T\\) is characterized by the structure \\(T(x) = \\nabla u(x)\\), for some convex function \\(u: R^n \\to R\\). :Furthermore, if \\(\\mu(dx) = f(x)dx\\), and \\(\\nu(dy) = g(y) dy\\), :\\(| \\mathrm{det} (\\nabla T(x)) | = \\frac{f(x)}{g(T(x))}\\) for \\(\\mu\\)-a.e. \\(x \\in R^n\\)\n\n\nNote that many of these notions, such as Lebesgue measure, gradients, and the Monge Ampère Equation, all have well-defined generalizations on all Riemannian manifolds. In particular, the above theorem was able to be extended to compact Riemannian manifolds."
  },
  {
    "objectID": "RegularityOTMap.html#existence-and-uniqueness-on-riemannian-manifolds",
    "href": "RegularityOTMap.html#existence-and-uniqueness-on-riemannian-manifolds",
    "title": "Regularity of Optimal Transport Maps and the Monge-Ampére Equation on Riemannian Manifolds",
    "section": "Existence and Uniqueness on Riemannian Manifolds",
    "text": "Existence and Uniqueness on Riemannian Manifolds\nIn order to extend the theory from the previous section to Riemannian manifolds, we need the following definitions.\n:Definition. Let \\(c: X \\to Y\\) be an arbitrary function. A function is \\(\\psi: X \\to R \\cup \\{+\\infty\\}\\) is c-convex if :\\(\\psi(x) = \\sup_{y \\in Y} \\left[\\psi^c(y) - c(x,y)\\right]\\) for all \\(x \\in X\\), where :\\(\\psi^c(y) = \\inf_{x \\in X} \\left[\\psi(x) + c(x, y) \\right]\\) for all \\(y \\in Y\\).\n:Moreover, for a \\(c\\)-convex function \\(\\psi(x)\\), we can definite its \\(c\\)-subdifferential at \\(x\\) as\n:\\(\\partial^c \\psi(x) := \\{ y \\in Y \\, | \\, \\psi(x) = \\psi^c(y) - c(x, y) \\}\\)\nRemark: Let \\(X\\) and \\(Y\\) be \\(R^n\\). Observe that if \\(-c(x,y)\\) is the Euclidean inner product, then if \\(\\psi\\) is \\(c\\)-convex we have,\n:\\(\\psi^c(y) = \\inf_{x \\in X} \\left[\\psi(x) -x\\cdot y \\right]=-\\sup_{x \\in X} \\left[x\\cdot y - \\psi(x)\\right]=\\psi^*(y)\\)\nwhich corresponds with the convex conjugate of \\(\\psi\\). Consequently,\n:\\(\\psi(x)=\\sup_{y \\in Y} \\left[\\psi^c(y) - c(x,y)\\right]=\\sup_{y \\in Y} \\left[x\\cdot y-\\psi^*(x)\\right]=\\psi^{**}(x)\\)\nThus, if \\(\\psi\\) is proper, \\(\\psi\\) is convex since it is equivalent to \\(\\psi^{**}\\).\nWith this, we can write down the desired theorem, restated from Figalli. :Theorem. Let \\((M, g)\\) be a Riemannian manifold, take \\(\\mu\\) and \\(\\nu\\) two compactly supported measures on \\(M\\), and consider the optimal transport problem from \\(\\mu\\) to \\(\\nu\\) with cost \\(c(x,y) = d(x,y)^2/2\\), where \\(d(x,y)\\) denotes the Riemannian distance on \\(M\\) If \\(\\mu\\) is absolutely continuous with respect to the volume measure, then: :* There exists a unique solution \\(T\\) to the Monge problem. :* \\(T\\) is characterized by the structure \\(T(x) = \\exp_x (\\nabla \\psi(x)) \\in \\partial^c \\psi(x)\\) for some \\(c\\)-convex function \\(\\psi: M \\to R\\) :* For \\(\\mu_0\\)-a.e. \\(x \\in M\\), there exists a unique minimizing geodesic from \\(x\\) to \\(T(x)\\), which is given by \\(t \\to \\exp_x(t\\nabla \\psi(x)) \\in [0,1]\\) :Furthermore, if \\(\\mu(dx) = f(x) \\mathrm{vol}(dx)\\), and \\(\\nu(dy) = g(y) \\mathrm{vol}(dy)\\), :\\(|\\det(\\nabla T(x))| = \\frac{f(x)}{g(T(x))}\\) for \\(\\mu\\)-a.e. \\(x \\in M\\).\nRemark: Some care should be taken with the above formula. The determinant of \\(\\nabla T(x)\\) depends on the the tangent space at \\(x\\). Fortunately, \\(|\\det(\\nabla T(x))|\\) may be computed independently of \\(T_xM\\)."
  },
  {
    "objectID": "RegularityOTMap.html#regularity-on-compact-riemannian-manifolds",
    "href": "RegularityOTMap.html#regularity-on-compact-riemannian-manifolds",
    "title": "Regularity of Optimal Transport Maps and the Monge-Ampére Equation on Riemannian Manifolds",
    "section": "Regularity on Compact Riemannian Manifolds",
    "text": "Regularity on Compact Riemannian Manifolds\nWe discuss the results of Ma, Trudinger, Wang, and Loeper to extend regularity of optimal transport maps to Riemannian manifolds. Once again, we will make use of the Monge Ampère Equation, :\\(|\\det(\\nabla T(x))| = \\frac{f(x)}{g(T(x)}\\) to make claims about regularity. Recall that we want the condition \\(T(x) = \\exp_x (\\nabla \\psi(x)) \\in \\partial^c \\psi(x)\\). It can be shown that this is equivalent to :\\(\\nabla \\psi(x) + \\nabla_x c(x, T(x)) = 0\\) By differentiating the above identity with respect to \\(x\\) and writing everything in charts, we get the equation :\\(\\det (D^2\\psi(x)) + D_x^2 c(x, \\exp_x (\\nabla \\psi(x))) = \\frac{f(x) \\mathrm{vol}_x}{g((T(x))\\mathrm{vol}_{T(x)}|\\det (d_{\\nabla \\psi(x)}\\exp_x |}\\) which is very similar to the Monge Ampère Equation we derived for transport maps in \\(R^n\\). We simply have a perturbation of \\(D_x^2 c(x, \\exp_x (\\nabla \\psi(x)))\\). This perturbation can obstruct smoothness. One can try to take the second derivative of the previous equation in order to make an a priori estimate on the second derivatives of \\(\\psi\\). Doing so requires a condition on the sign of the Ma-Trundinger-Wang tensor: :\\(\\vartheta_{(x,y)} (\\xi, \\eta) := \\frac{3}{2} \\sum_{ijklrs} (c_{ij,r}c^{r,s}c_{s,kl} - c_{ij,kl})\\xi^i \\xi^j \\eta^k \\eta^l, \\, \\xi \\in T_xM, \\eta \\in T_yM\\).\nWe often write “MTW tensor” intead of Ma-Trundinger-Wang tensor. Moreover, the MTW Condition is: :\\(\\vartheta_{(x,y)}(\\xi, \\eta) \\ge 0\\) whenever \\(\\sum_{ij} c_{i,j}\\xi^i\\eta^j = 0\\).\nFrom here, one can prove the Riemannian analogue to one of the regularity theorems we mentioned for the Monge problem in \\(R^n\\). The theorem is as follows:\n:Theorem. Let \\((M, g)\\) be a Riemannian manifold. Assume the MTW condition holds, that \\(f\\) and \\(g\\) are smooth and bounded away from zero and infinity on their respective supports \\(\\Omega\\) and \\(\\Omega'\\), and that the cost function \\(c=d^2/2\\) is smooth on the set \\(\\overline{\\Omega} \\times \\overline{\\Omega'}\\). Finally, suppose that: :* \\(\\Omega\\) and \\(\\Omega'\\) are smooth; :* \\((\\exp_x)^{-1}(\\Omega') \\subset T_xM\\) is uniformly convex for all \\(x \\in \\Omega\\); :* \\((\\exp_y)^{-1}(\\Omega) \\subset T_yM\\) is uniformly convex for all \\(y \\in \\Omega'\\). :Then \\(\\psi \\in C^{\\infty} (\\overline{\\Omega})\\), and \\(T: \\overline{\\Omega} \\to \\overline{\\Omega'}\\) is a smooth diffeomorphism.\n\nThe MTW Condition and its Consequences\nAt a first glance, the MTW condition seems highly technical and it is not clear why it plays a role in the regularity theorem above. Indeed, working through the proof, one sees where the MTW condition is crucial, but it is not intuitively clear why one should think of employing the condition. Loeper realized that connectedness of the \\(c\\)-subdifferential was required for the desired regularity conditions discussed above. This connection arises naturally as an extension of the fact that regularity classical convex solutions to the Monge Ampère Equation requires connectivity of the sub-differential. In fact, Loeper showed that connectedness of the \\(c\\)-subdifferential is equivalent to the MTW condition, hence its need for regularity.\nFortunately, the MTW condition is satisfied by the most canonical manifolds, including \\(R^n\\), \\(T^n\\), and \\(S^n\\) (in fact, quotients of \\(S^n\\) also satisfy the MTW condition)."
  },
  {
    "objectID": "RegularityOTMap.html#references",
    "href": "RegularityOTMap.html#references",
    "title": "Regularity of Optimal Transport Maps and the Monge-Ampére Equation on Riemannian Manifolds",
    "section": "References",
    "text": "References\n F. Santambrogio, Optimal Transport for Applied Mathematicians, p. 18, 54-57 A. Figalli, Regularity of optimal transport maps"
  },
  {
    "objectID": "1DimensionOT_Revised.html",
    "href": "1DimensionOT_Revised.html",
    "title": "Optimal Transport Problem on the Real Line",
    "section": "",
    "text": "In this article, we briefly explore the optimal transport problem on the real line along with some examples.\n\nExistence Of Monotone Transport Maps\nWe first introduce the definition of pseudo-inverse of a cumulative density function (CDF) of probability measure \\(\\mu\\).\nDefinition. Given a nondecreasing and right-continuous function \\(F: \\mathbb{R} \\rightarrow\\) \\([0,1]\\), its pseudo-inverse is the function \\(F^{[-1]}:[0,1] \\rightarrow \\overline{\\mathbb{R}}\\) given by\n\\[\nF^{[-1]}(x):=\\inf \\{t \\in \\mathbb{R}: F(t) \\geq x\\}\n\\]\nwhere the infimum is a minimum as soon as the set is nonempty (otherwise it is \\(+\\infty\\) ) and bounded from below (otherwise it is \\(-\\infty\\) ), thanks to right continuity of \\(F\\).\nNote, as a simple consequence of the definition of pseudo-inverse, that we have\n\\[\nF^{[-1]}(x) \\leq a \\Leftrightarrow F(a) \\geq x ; \\quad F^{[-1]}(x)&gt;a \\Leftrightarrow F(a)&lt;x .\n\\]\nNote if the measure \\(\\mu\\) is atomless, the CDF of \\(\\mu\\) is simply strictly increasing and continuous, and so is its inverse.\nDefinition. We will call the transport plan \\(\\eta:=\\left(F_\\mu^{[-1]}, F_{\\nu}^{[-1]}\\right) \\#\\left(\\mathscr{L}^1\\llcorner[0,1])\\right.\\) the co-monotone transport plan between \\(\\mu\\) and \\(\\nu\\) and denote it by \\(\\gamma_{\\text {mon }}\\).\nThe following theorem asserts that for all atomless measure \\(\\mu\\) and measure \\(\\nu\\) (not necessarily atomless) on \\(\\mathbb{R}\\), we can always find a transport map (not just a plan) from \\(\\mu\\) to \\(\\nu\\):\nTheorem. Given \\(\\mu, \\nu \\in \\mathscr{P}(\\mathbb{R})\\), suppose that \\(\\mu\\) is atomless. Then, there exists a unique nondecreasing map \\(\\mathrm{T}_{\\text {mon }}: \\mathbb{R} \\rightarrow \\mathbb{R}\\) such that \\(\\left(\\mathrm{T}_{\\mathrm{mon}}\\right)_{\\#} \\mu=\\nu\\).\nRemark. The map is given by \\(\\mathrm{T}_{\\mathrm{mon}}(x):=F_{\\nu}^{[-1]}\\left(F_{\\mu}(x)\\right)\\) . When \\(\\nu\\) is also atomless, it becomes simply \\(\\mathrm{T}_{\\mathrm{mon}}(x)=F_{\\nu}^{-1}\\left(F_{\\mu}(x)\\right)\\) .\n\n\nOptimality Of The Monotone Transport Map\nThe monotone map is optimal in a variety of cases.\nTheorem. Let \\(h: \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\)be a strictly convex function and \\(\\mu, \\nu \\in \\mathscr{P}(\\mathbb{R})\\) be probability measures. Consider the cost \\(c(x, y)=h(y-x)\\) and suppose that (KP) has a finite value. Then, (KP) has a unique solution, which is given by \\(\\gamma_{\\text {mon }}\\). In the case where \\(\\mu\\) is atomless, this optimal plan is induced by the map \\(\\mathrm{T}_{\\text {mon}}\\). Moreover, if the strict convexity assumption is withdrawn and \\(h\\) is only convex, then the same \\(\\gamma_{\\text {mon}}\\) is actually an optimal transport plan, but no uniqueness is guaranteed anymore.\nThe following are some examples.\n\nLinear Cost Example\nFor this example, consider the cost function \\(c(x,y) = L(x-y)\\) along with a given linear map \\(A: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\). Moreover, if we let ( \\(\\gamma\\) ) be any transport plan, then by direct computation we see that: \\[\n\\int L(x-y) d \\gamma = \\int L(x) d \\gamma - \\int L(y) d \\gamma = \\int L(x) d \\mu - \\int L(y) d \\nu\n\\]\nThis suggests that this result only depends on the marginals of \\(\\gamma\\) (where \\(\\mu\\) and \\(\\nu\\) are compactly supported probability measures). In fact, in such cases, every transport plan/map is optimal.\n\n\nDistance Cost Example\nConsider the cost function \\(c(x,y) = |x-y|\\) along with probability measures (on \\(\\mathbb{R}\\)) \\(\\mu\\) and \\(\\nu\\). Then, for any \\(((x,y) \\in spt(\\mu) \\times spt(\\nu) )\\), we see that \\(c(x,y) = y-x\\), which immediately puts us back in the linear cost position. Therefore, any transport map/plan is also optimal for such costs.\n\n\nBook Shifting Example\nConsider the cost function \\(c(x,y) = |x-y|\\) along with \\(\\mu = \\frac{1}{2} \\lambda*{\\[0,2\\]}\\) and \\(\\nu = \\frac{1}{2}\\lambda{\\[1,3\\]}\\) (where \\(\\lambda\\) is the one-dimensional Lebesgue measure). A (monotone) transport plan that rearranges \\(\\mu\\) to look like $$ is given by \\(T_0(x) = x+1\\), and its corresponding cost is:\n\\[\nM(T_0) = \\int \\|T_0(x)-x\\| d\\mu \\equiv 1\n\\]\nFurthermore, notice that the piecewise map \\(T_1(x)=x+2\\) (for \\(x \\leq 1\\) ) and \\(T_1(x)=x\\) (for \\(x&gt;1\\) ) satisfies \\(T_1 \\# \\mu=\\nu\\), i.e. \\(T_1\\) is a transport map from \\(\\mu\\) to \\(\\nu\\);; moreover, the corresponding cost is:\n\\[\nM\\left(T_1\\right)=\\int\\left|T_1(x)-x\\right| d \\mu=\\frac{1}{2} \\int_0^2 2 d x \\equiv 1\n\\]\nThus, we conclude that \\(T_1\\) is indeed optimal as well.\n\n\nQuadratic Cost\nTheorem: Let \\(\\mu, \\nu\\) be probability measures on \\(\\mathbb{R}\\) with cumulative distribution functions (CDFs) \\(F\\) and \\(G\\), respectively. Also, let \\(\\pi\\) be the probability measure on \\(\\mathbb{R}^2\\) with the CDF:\n\\[\nH(x,y) = \\min(F(x), G(y))\n\\]\nThen, \\(\\pi \\in \\Gamma(\\mu, \\nu)\\) and is optimal (in the Kantorovich problem setting) between \\(\\mu\\) and \\(\\nu\\) for the (quadratic) cost function \\(c(x,y) = |x-y|^2\\), and the corresponding cost is:\n\\[\nT_2(\\mu, \\nu)=\\int_0^1\\left|F^{-1}(t)-G^{-1}(t)\\right|^2 d t\n\\]\nwhere \\(F^{-1}\\) and \\(G^{-1}\\) are the pseudo-inverses of the respective CDFs.\n\nIdeas and Remarks for the Proof\nNote the proof from Cedric-Villani gives this result in arbitrary dimensions. Below is a rough outline of the proof, and the full details can be found in “Topics in Optimal Transportation” (Villani, cite later). Moreover, the measure \\(\\pi\\) constructed in the theorem is indeed optimal provided that the cost function \\(c(x,y)\\) is of the form \\(c(x-y)\\), where \\(c\\) is a convex, nonnegative symmetric function on \\(\\mathbb{R}\\).\nOne of the first major steps in proving this theorem is showing that\n\\[\\operatorname{supp}(\\pi) \\subset\\left\\{(x, y) \\in \\mathbb{R}^2: F\\left(x^{-}\\right) \\leq G(y)\\right.  \\text{ and } \\left.G\\left(y^{-}\\right) \\leq F(x)\\right\\}\\]\nby considering specific cases. Upon showing this, we may conclude that \\(\\pi\\) is supported in a monotone subset of \\(\\mathbb{R}^2\\) and hence also supported in the sub-differential of some lower semi-continuous convex function. From here, we make use of the Knott-Smith optimality criterion (Villani, pg. 66) which establishes that \\(\\pi\\) is an optimal transference plan. Then, upon showing that:\n\\(\\pi=\\left(F^{-1} \\times G^{-1}\\right) \\# \\lambda_{[0,1]}\\)\nwe see that for any nonnegative, measurable function \\(u\\) on \\(\\mathbb{R}^2\\):\n\\[\n\\int_{\\mathbb{R}^2} u(x, y) d \\pi(x, y)=\\int_0^1 u\\left(F^{-1}(t), G^{-1}(t)\\right) d t\n\\]\nThis then immediately yields the cost ( T_2(, ) ) and completes the proof."
  },
  {
    "objectID": "Gaussian_Measures.html",
    "href": "Gaussian_Measures.html",
    "title": "Gaussian Measures",
    "section": "",
    "text": "Gaussian Measures are ubiquitous throughout analysis, statistics, and many applied subjects. To a large extent this may be seen as a consequence of the central limit theorem, which in plain terms says that rescaled sums of sufficiently uncorrelated random variables converge to Gaussian variables. In particular quantities like height that depend on many variables will often times follow a gaussian distribution. For reasons we will see later, they also help us understand analysis in infinite dimensional spaces."
  },
  {
    "objectID": "Gaussian_Measures.html#lebesgue-measure-does-not-exist-in-infinite-dimensions",
    "href": "Gaussian_Measures.html#lebesgue-measure-does-not-exist-in-infinite-dimensions",
    "title": "Gaussian Measures",
    "section": "Lebesgue Measure Does not Exist in Infinite Dimensions",
    "text": "Lebesgue Measure Does not Exist in Infinite Dimensions\nHere is the formal statement that captures the fact that we can’t have a Lebesgue measure in infinite dimensions.\nTheorem: Let \\(X\\) be an infinite dimensional separable Banach space. If \\(\\mu\\) is a translation invariant Borel measure on \\(X\\) then either \\(\\mu\\) assigns infinite measure to each ball or it assigns measure \\(0\\) to each ball in \\(X\\).\nFor a full proof, see (Eldredge 2016) or (Stroock [2023]). Here is a sketch of the proof in the case where \\(X=l^2\\) is the space of square summable sequences. This special case, however, illustrates the key idea of the proof: namely that any ball contains infinitely many disjoint balls. Let \\(e_n\\) be the ``standard basis” in \\(l^2\\). That is, the entries of \\(e_n\\) are given by \\[\ne_n(m) = \\begin{cases}\n    0 & m\\neq n \\\\\n    1 & m=n\n\\end{cases}\n\\] Now observe that if \\(m\\neq n\\), \\(|e_n-e_m|=\\sqrt{2}\\) so if \\(B(x,r)\\) denotes the open ball of radius \\(r\\), we see that \\(\\left\\{B\\left(e_n,\\frac{\\sqrt{2}}{2}\\right)\\right\\}_{n=1}^\\infty\\) is a countable collection of disjoint open balls, each of which is contained in \\(B(0,2)\\) since if \\(x\\in B\\left(e_n,\\frac{\\sqrt{2}}{2}\\right)\\) then \\[\n    |x|\\leq |x-e_n|+|e_n|&lt;\\frac{\\sqrt{2}}{2}+1&lt;2\n\\] But then by additivity and translation invariance, \\[\n\\begin{align*}\n    \\mu(B(0,2)) &= \\sum_{n=1}^\\infty \\mu\\left(B\\left(e_n,\\frac{\\sqrt{2}}{2}\\right)\\right) \\\\\n    &= \\sum_{n=1}^\\infty \\mu\\left(B\\left(0,\\frac{\\sqrt{2}}{2}\\right)\\right)\n\\end{align*}\n\\] hence if \\(\\mu\\) assigns positive measure to \\(B\\left(0,\\frac{\\sqrt{2}}{2}\\right)\\) then it must give infinite measure to \\(B\\left(0,2\\right)\\). By translation invariance we can show that any ball of radius \\(2\\) must have positive measure. More generally we can use this argument with the radii appropriately rescales to show that if \\(\\mu\\) assigns positive measure to some ball, it must assign infinite measure to all balls."
  },
  {
    "objectID": "Gaussian_Measures.html#probability-measures-in-infinite-dimension",
    "href": "Gaussian_Measures.html#probability-measures-in-infinite-dimension",
    "title": "Gaussian Measures",
    "section": "Probability Measures in Infinite Dimension",
    "text": "Probability Measures in Infinite Dimension\nThroughout, suppose that \\(\\mu\\) is a borel probability measure on a a separable Banach space \\(X\\). In finite dimensions, we saw that the covariance matrix and mean characterized Gaussian measures. As usual, when we jump to infinite dimensions, matrices should be replaced with operators. Since any \\(f\\in X^*\\) is continuous, it is in particular borel measurable. In this setting, we define the covariance of two linear functionals as \\[\n    q(f,g) := \\langle f,g\\rangle_{L^2(X,\\mu)}=\\int_X f(x)g(x)d\\mu(x)\n\\] which is a bounded bilinear form. For gaussian measures on finite dimensional spaces, one can check that \\(q(f,g)=\\langle f,\\Sigma g\\rangle\\).\nSince there is no infinite dimensional Lebesgue measure, we’d hope to be able to define a Gaussian measure in terms of its Fourier transform. One can indeed define a Gaussian measure on \\(X\\) as one whose Fourier transform is \\(\\exp\\{-\\frac{1}{2}q(x,x)^2\\}\\) for a sufficiently nice bilinear form \\(q\\) on \\(X^*\\). The details of this definition are not relevant to us, but the theory of Fourier transforms of measures on Banach spaces (called characteristic functionals) is developed in (Bogachev 2006) Chapter 7 and (Kuo [1975]) Chapter 1. Intuitively what it means to say that \\(\\mu\\) is a Gaussian measure is that it “looks” Gaussian on every finite dimensional subspace. A more concrete definition that coincides with the definition in terms of the Fourier tranform is the following.\nDefinition: A borel probability measure \\(\\mu\\) on \\(X\\) is Gaussian if for every linear functional \\(f\\in X^*\\), \\(f_\\#\\mu\\) is a Gaussian measure on \\(\\mathbb{R}\\). Necesarily\nIf we specifically look at the case where \\(X\\) is a Hilbert space, more can be said about Gaussian measures on \\(X\\). Note that by the Riesz representation theorem, \\(H\\) is canonically identified with its dual, so \\(q\\) is identified with a unique bounded bilinear form on \\(H\\). Just like the corresponded outlined in fact (3) for the finite dimensional setting, there is the following correspondence.\nTheorem: (Kuo [1975]) There is a bijective correspondence between gaussian measures on \\(X\\) and positive semi-definite, self adjoint, trace class operators on \\(X\\). Specifically,\n\nFor each Gaussian measure \\(\\mu\\) on \\(X\\), its covariance form \\(q\\) is given by $q(f,g)=f, S_g$ for a unique positive semi-definite, self adjoint, trace class operator \\(S_\\mu\\).\nFor each positive semi-definite, self adjoint, trace class operator \\(S\\) on \\(X\\), there exists a unique mean \\(0\\) Gaussian measure \\(\\mu_S\\) with covariance form \\(q_S(f,g):=\\langle f,S g\\rangle_X\\).\n\nThe construction of \\(\\mu\\) from \\(q_S\\) is given by an infinite dimensional analogue of Bochner’s Theorem and Levy-Inversion. The upshot of this result is that we can construct whole families of Gaussian measures on Hilbert spaces very easily whereas it can be difficult for general Banach spaces."
  },
  {
    "objectID": "Gaussian_Measures.html#cameron-martin-theorem-integration-by-parts",
    "href": "Gaussian_Measures.html#cameron-martin-theorem-integration-by-parts",
    "title": "Gaussian Measures",
    "section": "Cameron-Martin Theorem & Integration by Parts",
    "text": "Cameron-Martin Theorem & Integration by Parts\nAn infinite dimensional analogue of equation (4) would be useful for giving us an integration by parts analogue and related analytic techniques. These are frequently used (not always rigorously) in Quantum Field Theory and the study of Gaussian processes (Albeverio, Høegh-Krohn, and Mazzucchi [2008]). The main theorem that answers this question is the Cameron-Martin theorem (see (Bogachev [1998]), (Kuo [1975]), or (Stroock [2023]) for a detailed exposition). Intuitively, this theorem says is that for almost every \\(h\\in X\\), \\(\\mu_h\\) and \\(\\mu\\) are mutually singular, while for very particular choices of \\(h\\) and analogue of equation (4) holds.\nNow for the technical statement. Note that for any continuous linear functional \\(f\\in X^*\\), \\[\n\\int_X|f(x)|^2 d\\mu(x) = \\int_\\mathbb{R}x^2d(f_\\#\\mu)(x) &lt;\\infty\n\\] since \\(f_\\#\\mu\\) is a Gaussian measure and \\(|x|^2\\) is integrable w.r.t. any Gaussian measure. In particular, there is an inclusion \\(X^*\\hookrightarrow L^2(X,\\mu)\\). Considering \\(X^*\\) as a subset of \\(L^2(X,\\mu)\\), let the completion \\(\\overline{X^*}^{L^2(X,\\mu)}\\) be denoted \\(K\\). Now define \\[\n    H:=\\{h\\in X: \\text{ the evaluation map }\\phi_h:X^*\\rightarrow \\mathbb{R} \\text{ given by } f\\mapsto f(h) \\text{ is continuous}\\}\n\\] By the continuous linear extension theorem it follows that for any \\(h\\in H\\), \\(\\phi_h\\) extends to a continuous linear functional on \\(K\\), the \\(L^2(X,\\mu)\\) closure of \\(X^*\\). Since \\(K\\) is a hilbert space with the \\(L^2(X,\\mu)\\) norm, it is identified with its dual. Thus we have a map \\(T:H\\rightarrow K\\) where \\(T(h)\\) is characterized by the fact that for any \\(f\\in X^*\\), \\(q(Th,f)=f(h)\\). One can prove that this is an isometry, hence it induces a complete inner product on \\(H\\), which we denote by \\(\\langle\\cdot,\\cdot \\rangle_H\\). We now have the language to state the following striking result.\nTheorem (Cameron-Martin): Let \\(\\mu\\) be a Gaussian measure on a a separable Banach space \\(X\\) with Cameron-Martin space \\(H\\). Then \\(\\mu(H)=0\\) and\n\nIf \\(h\\in H\\) then \\(\\mu_h &lt;&lt; \\mu\\) and \\[\n\\frac{d\\mu_h}{d\\mu} = \\exp\\left\\{-\\frac{1}{2}\\|h\\|_H^2+\\langle h,x\\rangle\\right\\}\n\\]\nIf \\(h\\in X\\setminus H\\) then \\(\\mu_h\\) and \\(\\mu\\) are mutually singular.\n\nThis makes precise the intuitive statement that equation (4) fails for \\(\\mu\\) almost every \\(h\\) For these pathological \\(h\\), there is no hope for an integration by parts formula or similar tools. For an explanation on the associated integration by parts formula (and why translation invariance leads to integration by parts) see (Driver 1991).\nThe situation is analogous to have a measure on \\(\\mathbb{R}^2\\) which is a Gaussian measure on the \\(x\\)-axis and zero elswhere. If we translate this measure along the \\(x\\) direction, the resulting measure will be mutually absolutely continuous. If we translate this measure by any other direction, however, the two measures will be mutually singular."
  },
  {
    "objectID": "WassersteinOnGraphs.html",
    "href": "WassersteinOnGraphs.html",
    "title": "Wasserstein-like Metrics on Graphs",
    "section": "",
    "text": "Motivation\nOn Euclidean space, it is known that the solution to the heat equation can be viewed as a gradient flow. In other words, consider the heat equation \\[ \\begin{cases}\n          \\partial_t u(t, x) = \\Delta u(t, x) \\\\\n          u(0,\\cdot)=u_0(\\cdot) . \\\\  \n       \\end{cases}\n    \\] We define the energy functional \\(\\mathcal{F}\\) by \\[ \\mathcal{F}(f)=\\int_{\\mathbb{R}^d}f(x)\\ln(f(x))dx . \\] We restrict the domain of \\(\\mathcal{F}\\) to a suitable class of functions on \\(\\mathbb{R}^d\\) such that \\(\\mathcal{F}(f)\\) is well-defined and finite. Consider the metric space \\((P_2(\\mathbb{R}^d), W_2)\\), where \\(W_2\\) denotes the 2-Wasserstein metric, and \\[ P_2(\\mathbb{R}^d) = \\left \\{ \\mu \\in P(\\mathbb{R}^d) : \\int_{\\mathbb{R}^d} |x|^2 d \\mu(x) &lt; \\infty \\right \\} . \\] This metric space has a formal Riemannian structure (see this article for more information), so the operator \\(\\nabla_{W_2}\\) is well-defined. Based on the seminal work of Jordan, Kinderlehrer and Otto in (Jordan 1998), we know that the solution to the heat equation, \\(u(t, x)\\), satisfies \\[\n    \\partial_t u(t, x) = -\\nabla_{W_2}\\mathcal{F}(u(t,x)) .\n\\tag{1}\\] This result has been extended to non-Euclidean base spaces. In other words, in many instances, we may replace \\(\\mathbb{R}^d\\) with a more general space (e.g. a Riemannian manifold). Then, we may develop an analogue to \\(W_2\\) so that Equation 1 still holds. In this article, we will replace \\(\\mathbb{R}^d\\) with a finite directed graph \\(G\\) and then construct an analogue to the 2-Wasserstein metric so that heat flow is the gradient flow of the entropy.\n\n\nSetting: Graphs and Markov Chains\nConsider a complete finite weighted graph \\(G\\) with nodes \\(X = \\{x_1, \\ldots, x_n \\}\\). Let the edge weight from \\(x_i\\) to \\(x_j\\) be given by \\(K(x_i, x_j)\\). We may view our graph as a continuous-time Markov chain with states \\(\\{ x_1, \\ldots, x_n \\}\\). By a slight abuse of notation, we let \\(K\\) denote the \\(n\\) by \\(n\\) matrix with entry \\((i, j)\\) equal to \\(K(x_i, x_j)\\). Then, we let \\(Q = K - I\\) be the transition matrix associated with our Markov chain. The associated continuous time semigroup is given by \\[ H(t) = e^{tQ} . \\] We will assume that \\[ K_{ij} \\geq 0 \\; \\forall i, j, \\; \\sum \\limits_{j = 1}^n K_{ij} = 1 \\; \\forall i . \\] We will further assume that \\(K\\) is irreducible, and hence the Markov chain has a unique steady state \\(\\pi\\), where \\(\\pi = \\pi K\\). By elementary Markov chain theory, we know that \\(\\pi\\) is strictly positive. Consider the set \\[ P(X) = \\left \\{ \\rho: X \\rightarrow \\mathbb{R} : \\rho(x) \\geq 0 \\; \\forall x \\in X : \\sum \\limits_{x \\in X} \\rho(x)\\pi(x) = 1 \\right \\} . \\] Intuitively, \\(P(X)\\) can be thought of as the set of all probability density functions on \\(X\\); for each \\(\\rho \\in P(X)\\), there is a corresponding measure \\(\\mu\\) on \\(X\\), where \\[ \\mu(A) = \\sum \\limits_{x \\in A} \\rho(x) \\pi(x) \\; \\forall A \\subseteq X . \\]\n\n\nA New Metric on \\(P(X)\\)\nLet \\(\\theta: \\mathbb{R}_+ \\times \\mathbb{R}_+ \\rightarrow \\mathbb{R}_+\\) denote the logarithmic mean, i.e. \\[ \\theta(s, t) = \\begin{cases}\n    s & s = t \\\\\n    \\frac{s - t}{\\ln(s) - \\ln(t)} & \\text{otherwise}.\n\\end{cases}\\] Then, we define \\(\\rho(x, y) = \\theta(\\rho(x), \\rho(y))\\). For \\(\\rho_0, \\rho_1 \\in P(X)\\), we define \\[ W(\\rho_0, \\rho_1)^2 = \\inf \\limits_{\\rho, \\psi} \\left \\{ \\frac{1}{2} \\int_0^1 \\sum \\limits_{x, y \\in X} \\left ( \\psi_t(x) - \\psi_t(y)\\right )^2 K(x, y) \\rho_t(x, y) \\pi(x) dt \\right \\} , \\] where the infimum runs over all piecewise \\(C^1\\) curves \\(\\rho:[0, 1] \\rightarrow P(X)\\) and all measurable functions \\(\\psi:[0, 1] \\rightarrow \\mathbb{R}^X\\) satisfying for a.e. \\(t \\in [0, 1]\\), \\[ \\begin{cases}\n    \\frac{d}{dt} \\rho_t(x) + \\sum \\limits_{y \\in X}  ( \\psi_t(x) - \\psi_t(y) ) K(x, y) \\rho_t(x, y) = 0 & \\forall x \\in X \\\\\n    \\rho(0) = \\rho_0, \\rho(1) = \\rho_1 .\n\\end{cases} \\] Readers who are familiar with the Benamou-Brenier formula will note the similarity between this definition and that formula. We have the following:\n\nTheorem 1 The following assertions hold:\n\n\\(W\\) defines a pseudo-metric on \\(P(X)\\).\n\\(W(\\rho_0, \\rho_1) &lt; \\infty\\) for all \\(\\rho_0, \\rho_1 \\in P(X)\\).\n\\(W\\) metrises the topology of weak convergence.\n\n\n\nTheorem 2 Let \\(H(\\rho) = \\sum \\limits_{x \\in X} \\pi(x) \\rho(x) \\ln(\\rho(x))\\) be the entropy functional. For \\(\\rho \\in P(X)\\) and \\(t \\geq 0\\), set \\(\\rho_t = e^{t(K - I)}\\). Then the gradient flow equation \\[ \\frac{d}{dt} \\rho = - \\nabla H(\\rho_t) \\] holds for all \\(t &gt; 0\\).\n\nProofs for these theorems can be found in (Maas 2011).\n\n\nThe Two Point Space\nConsider the case \\(X = (a, b)\\), \\[ K = \\begin{bmatrix}\n    1 - p & p \\\\\n    q & 1 - q\n\\end{bmatrix}, \\] where \\(p, q \\in (0, 1]\\). If we define \\(\\rho^{\\beta} = \\frac{1}{2} \\left ( (1 - \\beta)\\delta_a + (1 + \\beta) \\delta_b \\right )\\), then \\[ P(X) = \\left \\{ \\rho^{\\beta} : \\beta \\in [-1, 1] \\right \\} . \\] In this special case, our metric \\(W\\) reduces to \\[ W(\\rho^{\\alpha}, \\rho^{\\beta}) =  \\int_{\\alpha}^{\\beta} \\frac{1}{\\sqrt{2\\theta(q(1 + r), p(1 - r))}} dr , \\] where \\(\\alpha \\leq \\beta\\). Notice that if we define \\[ \\varphi(\\beta) = \\int_{0}^{\\beta}  \\frac{1}{\\sqrt{2 \\theta(q(1 + r), p(1 - r))}}dr , \\] then for any \\(\\alpha, \\beta \\in [-1, 1]\\), \\[ W(\\rho^{\\alpha}, \\rho^{\\beta}) = |\\varphi(\\alpha) - \\varphi(\\beta)| . \\] As a result, the function \\(J: P(X) \\rightarrow [-1, 1]\\), \\[ J(\\rho^{\\beta}) = \\varphi(\\beta) \\] defines an isometry from \\((P(X), W)\\) to \\(([-1, 1], |\\cdot|)\\). We can exploit this isometry when identifying constant-speed geodesics in \\((P(X), W)\\).\n\nTheorem 3 Choose \\(\\alpha, \\beta \\in [-1, 1]\\). There exists a unique constant speed geodesic \\(\\{ p^{\\gamma(t)} \\}_{t \\in [0, 1]}\\), where \\(\\gamma \\in C^1([0, 1], \\mathbb{R})\\) and \\[ \\begin{cases}\n    \\gamma'(t) = \\omega \\sqrt{ 2 \\theta(q(1 + \\gamma(t)), p(1 - \\gamma(t))) } \\\\\n    \\gamma(0) = \\alpha \\\\\n    \\gamma(1) = \\beta,\n\\end{cases} \\] where \\(\\omega = sgn(\\beta - \\alpha) W(\\rho^{\\alpha}, \\rho^{\\beta})\\).\n\n\nProof. Since \\(J\\) is an isometry, existence and uniqueness of \\(\\gamma(t)\\) immediately follows. To show \\(\\rho^{\\gamma(t)}\\) is a constant speed geodesic, let \\(\\gamma\\) be the curve described above. Choose \\(0 \\leq s &lt; t \\leq 1\\), and note \\[\\begin{align*}\n    W \\left (\\rho^{\\gamma(t)}, \\rho^{\\gamma(s)} \\right )\n    & = |\\varphi(\\gamma(t)) - \\varphi(\\gamma(s))| \\\\\n    & = \\left | \\int_s^t \\varphi'(\\gamma(r)) \\cdot \\gamma'(r) dr \\right | \\\\\n    & = \\left | \\int_s^t  \\frac{1}{\\sqrt{2 \\theta(q(1 + \\gamma(t)), p(1 - \\gamma(t)))}} \\cdot \\omega \\sqrt{ 2 \\theta(p(1 - \\gamma(t)), q(1 + \\gamma(t))) } dr \\right | \\\\\n    & = |t - s| W\\left(\\rho^{\\alpha}, \\rho^{\\beta}\\right) .\n\\end{align*}\\]\n\n\n\n\n\n\nReferences\n\nJordan, et al. 1998. “The Variational Formulation of the Fokker–Planck Equation.”\n\n\nMaas, Jan. 2011. “Gradient Flows of the Entropy for Finite Markov Chains.”"
  },
  {
    "objectID": "Dynamic_OT.html",
    "href": "Dynamic_OT.html",
    "title": "Dynamic Optimal Transport",
    "section": "",
    "text": "The dynamic formulation of optimal transport is one of the four main formulations of the optimal transport problem (the other three being the Monge Problem, the Kantorovich Problem, and the Kantorovich Dual Problem). Rather than minimizing over transport maps or transport plans between measures \\(\\mu\\) and \\(\\nu\\), the dynamic formulation of optimal transport minimizes over curves which connect \\(\\mu\\) and \\(\\nu\\).\nMinimizing over curves (as opposed to maps or plans) has at least two major advantages. First, it allows for continuous interpolation between measures in the Wasserstein space. This is desirable in several applications.\nSecond, minimization over curves generalizes to many settings where transport maps or plans may not be well defined or may be difficult to define. This allows for the construction of optimal transport-based metrics in nonclassical settings, such as Optimal Transport on Graphs or Unbalanced Optimal Transport.\nThe original formulation of the dynamic optimal transport problem is due to (Benamou and Brenier 2000). However, a somewhat different formulation is presented in this article following (Ambrosio et al. 2021) which is based on integrals defined on the path space of the underlying metric space. We will compare these two approaches later in this article."
  },
  {
    "objectID": "Dynamic_OT.html#preliminaries",
    "href": "Dynamic_OT.html#preliminaries",
    "title": "Dynamic Optimal Transport",
    "section": "Preliminaries",
    "text": "Preliminaries\nLet \\((X,d)\\) be a metric space. Recall that a curve \\(\\gamma: [a,b] \\to X\\) is said to be absolutely continuous if there exists \\(g \\in L^1(a,b)\\) such that \\[\nd(\\gamma(r),\\gamma(s)) ~\\leq~ \\int_r^s g(t) \\, dt\n\\] for all \\(a \\leq r \\leq s \\leq b\\). The space of absolutely continuous curves from \\([a,b]\\) to \\(X\\) is denoted \\(\\text{AC}([a,b],X)\\).\nThe metric dertivative of \\(\\gamma \\in \\text{AC}([a,b],X)\\) is defined as \\[\n|\\gamma'|(t) ~:=~ \\lim_{h \\to 0} \\frac{d(\\gamma(t),\\gamma(t+h))}{|h|} .\n\\] The metric derivative formalizes the notion of “speed” of the curve \\(\\gamma\\), and is defined for a.e.-\\(t\\). The metric derivative is the minimal \\(g\\) that can be chosen in the definition of absolute continuity above.\nThe length of \\(\\gamma\\) is then defined as \\[\n\\ell(\\gamma) ~:=~ \\int_a^b |\\gamma'|(t) \\, dt .\n\\]\nNote that by reparameterizing the time variable, any curve in \\(\\text{AC}([a,b],X)\\) can be transformed into a curve in \\(\\text{AC}([0,1],X)\\) with constant speed \\(|\\gamma'| \\equiv \\ell(\\gamma) = \\text{constant}\\).\nThe above definitions imply that for any curve \\(\\gamma \\in \\text{AC}([a,b],X)\\), it holds that \\(\\ell(\\gamma) \\geq d(\\gamma(a),\\gamma(b))\\), i.e., that the length of \\(\\gamma\\) is greater than the distance between its endpoints. Any curve which achieves equality \\(\\ell(\\gamma) = d(\\gamma(a),\\gamma(b))\\) is termed a geodesic. The space of constant-speed geodesics on the interval \\([0,1]\\) is denoted \\(\\text{Geo}(X)\\).\nA metric space \\(X\\) is said to be geodesic if for all \\(x,y \\in X\\) there exists \\(\\gamma \\in \\text{Geo}(X)\\) such that \\(\\gamma(0) = x\\) and \\(\\gamma(1) = y\\).\nSee Analysis in Metric Spaces for a review of these ideas.\nGeneralizing the notion of length, the (\\(p\\)-) action of a curve \\(\\gamma \\in \\text{AC}([0,1],X)\\) is defined as \\[\nA_p(\\gamma) ~:=~ \\int_0^1 |\\gamma'|^p (t) \\, dt .\n\\] Note that this quantity is possibly infinite. This definition can be extended to all of \\(C([0,1],X)\\) by setting \\(A_p(\\gamma) = + \\infty\\) if \\(\\gamma \\in C([0,1],X) \\backslash AC([0,1],X)\\).\nGeodesics can then be characterized by a variational principle involving the action.\n\nLemma.\nFor \\(\\gamma \\in \\text{AC}([0,1],X)\\) and \\(1 \\leq p &lt; \\infty\\), \\(\\gamma \\in \\text{Geo}(X)\\) if and only if \\(A_p(\\gamma) = d^p(\\gamma(0),\\gamma(1))\\).\n\n\nProof.\nThe forward direction is immediate, since \\(\\gamma \\in \\text{Geo}(X)\\) means that \\(|\\gamma'| \\equiv \\ell(\\gamma) = d(\\gamma(0),\\gamma(1))\\) by definition. The reverse direction follows from Jensen’s inequality, since \\[\nA_p(\\gamma) ~:=~ \\int_0^1 |\\gamma'|^p(t) \\, dt ~\\geq~ \\left( \\int_0^1 |\\gamma'|(t) \\, dt \\right)^p ~=:~ \\ell^p(\\gamma) ~\\geq~ d^p(\\gamma(0),\\gamma(1)) ,\n\\] with equality being achieved if and only if \\(\\gamma \\in \\text{Geo}(X)\\). \\(\\square\\)\nThe space \\(C([0,1],X)\\) is termed the path space over \\(X\\). The dynamic formulation of optimal transport is posed in terms of probability measures on this path space, i.e., in terms of \\(\\eta \\in P(C([0,1],X))\\).\nThe evaluation map \\(e_t: C([0,1],X) \\to X\\) is defined by \\(e_t(\\gamma) := \\gamma(t)\\). The evaluation map acts on probability measures on the path space by pushforward \\((e_t)_\\# : P(C([0,1],X)) \\to P(X)\\). Thus \\((e_t)_\\# \\eta\\) is itself a path in \\(P(X)\\)."
  },
  {
    "objectID": "Dynamic_OT.html#the-dynamic-optimal-transport-problem",
    "href": "Dynamic_OT.html#the-dynamic-optimal-transport-problem",
    "title": "Dynamic Optimal Transport",
    "section": "The Dynamic Optimal Transport Problem",
    "text": "The Dynamic Optimal Transport Problem\nThe dynamic optimal transport problem is stated as follows. Given probability measures \\(\\mu, \\nu \\in P(X)\\), solve \\[\n\\min_\\eta ~ \\int_{C([0,1],X)} A_p(\\gamma) \\, d \\eta(\\gamma) \\qquad \\text{s.t.} \\qquad \\eta \\in P(C([0,1],X)), ~~(e_0)_\\# \\eta = \\mu , ~~ (e_1)_\\# \\eta = \\nu .\n\\] An admissible \\(\\eta\\) is termed a dynamic transport plan and an optimal \\(\\eta\\) is termed an optimal dynamic tranport plan. This terminology is justified by the fact that \\(\\eta\\) is an admissible dynamic transport plan if and only if \\((e_0,e_1)_\\# \\eta\\) is an admissible tranport plan. A dynamic transport plan \\(\\eta\\), however, encodes not only information about which particles in \\(\\mu\\) are transported to which particles in \\(\\nu\\), but about which trajectories these particles take during the transport process.\n\nTheorem.\nIf \\((X,d)\\) is a Polish and geodesic metric space, then the minimum attained for the dynamic optimal transport problem is equal to the minimum attained for the Kantorovich problem. Furthermore, \\(\\eta\\) is optimal if and only if \\((e_0,e_1)_\\# \\eta\\) is optimal for the Kantorovich problem and \\(\\eta\\) is supported on \\(\\text{Geo}(X)\\).\n\n\nProof.\nIf \\(\\eta\\) is an admissible dynamic transport plan, then \\[\n\\begin{align}\n\\int_{C([0,1],X)} A_p(\\gamma) \\, d\\eta(\\gamma) ~&\\geq~ \\int_{C([0,1],X)} d^p(\\gamma(0),\\gamma(1)) \\, d\\eta(\\gamma) \\\\\n~&=~ \\int_{X \\times X} d^p(x,y) \\, d(e_0,e_1)_\\# \\eta(x,y) \\\\\n~&\\geq~ \\min_\\pi \\mathbb{K}_p(\\pi) .\n\\end{align}\n\\] To prove the converse, start from an optimal transport plan \\(\\pi\\) and for each \\((x,y) \\in X \\times X\\), choose \\(\\gamma_{x,y} \\in \\text{Geo}(X)\\) such that \\(\\gamma_{x,y}(0) = x\\) and \\(\\gamma_{x,y}(1) = y\\). Define the map \\(\\Gamma: X \\times X \\to \\text{Geo}(X)\\) by \\(\\Gamma(x,y) = \\gamma_{x,y}\\) and the measure \\(\\eta := \\Gamma_\\# \\pi \\in P(C([0,1],X))\\). By construction, \\(\\eta\\) is supported in \\(\\text{Geo}(X)\\). Thus \\[\n\\begin{align}\n\\min_\\pi \\mathbb{K}_p(\\gamma) ~&=~ \\int_{X \\times X} d^p(x,y) \\, d\\pi(x,y) \\\\\n~&=~ \\int_{C([0,1],X)} d^p(\\gamma(0),\\gamma(1)) \\, d\\eta(\\gamma) \\\\\n~&=~ \\int_{C([0,1],X)} A_p(\\gamma) \\, d\\eta(\\gamma)\n\\end{align}\n\\] by the previous lemma, and equality is achieved.\nLastly, observe that \\(\\eta\\) is optimal if and only if equality is achieved above, which happens if and only if \\(\\gamma\\) is supported in \\(\\text{Geo}(X)\\) and \\((e_0,e_1)_\\# \\eta\\) is optimal for the Kantorovich problem. \\(\\square\\)"
  },
  {
    "objectID": "Dynamic_OT.html#comparison-with-original-formulation",
    "href": "Dynamic_OT.html#comparison-with-original-formulation",
    "title": "Dynamic Optimal Transport",
    "section": "Comparison With Original Formulation",
    "text": "Comparison With Original Formulation\nThe original formulation of dynamic optimal transport due to (Benamou and Brenier 2000) is posed as follows. Given probability measures \\(\\mu, \\nu \\in P(X)\\), solve \\[\n\\begin{align}\n\\min_{\\rho,v} ~ \\int_0^1 \\int_X \\| v(t,x) \\|^p \\, d \\rho(t,x) \\, dt \\qquad \\text{s.t.} \\qquad &\\partial_t \\rho(t,x) = - \\nabla \\cdot (\\rho(t,x) v(t,x)), \\\\\n~~ &\\rho(0) = \\mu, ~~ \\rho(1) = \\nu .\n\\end{align}\n\\] Here, \\(\\rho\\) is a time-varying probability measure on \\(X\\) and \\(v\\) is a time-varying vector field which acts to transport \\(\\rho\\). Individual particles are considered to be transported according to \\(x'(t) = v(x,t)\\), and thus the continuity equation \\(\\partial_t \\rho = - \\nabla \\cdot (\\rho v)\\) describes how the density \\(\\rho\\) evolves under the action of \\(v\\). (Note the abuse of notation here: the symbol \\(\\rho\\) is used both for the measure and for its density function.)\nIn order to interpret the statement of this problem appropriately, \\(X\\) needs to be a subset of Euclidean space (or some Riemannian manifold) and the continuity equation must be understood in some appropriate weak sense.\nSee The Continuity Equation and Benamou Brenier Formula and Geodesics and Generalized Geodesics for more on these ideas.\nIf \\(\\mu\\) has a density function, then particles start from unique locations, and we can write \\(\\phi(t,x)\\) to denote the location (at time \\(t\\)) of the particle which started from location \\(x\\) at time \\(0\\). It then holds that \\[\n\\rho(t) ~=~ (\\phi(t,\\cdot))_\\# \\rho(0) ~=~ (\\phi(t,\\cdot))_\\# \\mu .\n\\] Substituting this into the objective function and using properties of the pushforward, the objective can be rewritten \\[\n~=~ \\int_0^1 \\int_X \\| v(t, \\phi(t,x)) \\|^p \\, d \\mu(x) \\, dt .\n\\] Recognizing the quantity \\(\\| v(t,\\phi(t,x)) \\| = \\| \\tfrac{d}{dt}\\phi(t,x) \\|\\) as the metric derivative of the trajectory \\(\\phi(\\cdot,x)\\) and interchanging the order of integration, we obtain \\[\n~=~ \\int_X \\int_0^1 |\\phi'(\\cdot,x)|^p(t) \\, dt \\, d\\mu(x) ~=:~ \\int_X A_p(\\phi(\\cdot,x)) \\, d\\mu(x) .\n\\] Lastly, defining \\(\\eta := \\phi_\\# \\mu\\) and letting \\(\\gamma\\) range over all possible paths from \\([0,1]\\) into \\(X\\) gives \\[\n~=~ \\int_{C([0,1],X)} A_p(\\gamma) \\, d\\eta(\\gamma) ,\n\\] thus recovering the formulation presented earlier.\nTherefore, when \\(X\\) is a subset of Euclidean space (or a Riemannian manifold) and when \\(\\mu\\) has a density with respect to Lebesgue measure, the two formulations presented are equivalent. When \\(\\mu\\) does not have a density, the original formulation can be further weakened to maintain equivalence. However, when \\(X\\) is a general metric space, only the earlier formulation in terms of integrals on the path space is valid."
  },
  {
    "objectID": "SchrodingerBridge.html",
    "href": "SchrodingerBridge.html",
    "title": "Different Formulations of the Schrödinger Bridge Problem",
    "section": "",
    "text": "We are mainly following section 2 - 4 of Léonard (2013) to describe different formulations of the Schrödinger bridge problem."
  },
  {
    "objectID": "SchrodingerBridge.html#motivation-and-problem-setup",
    "href": "SchrodingerBridge.html#motivation-and-problem-setup",
    "title": "Different Formulations of the Schrödinger Bridge Problem",
    "section": "Motivation and problem setup",
    "text": "Motivation and problem setup\nThe Schrödinger bridge problem could be viewed as applying entropy maximizing principle on finding the most likely path of a particle given its spatial distribution at two time instances. Mathematically speaking, consider \\(\\mathcal{X}:= \\mathbb{R}^d\\) as our spatial state space and \\(\\Omega:= C([0, 1]; \\mathcal{X})\\) our space of continuous path equipped with the uniform metric. Let \\(R\\) be the reversible Brownian motion on \\(\\mathcal{X}\\), which is the law of a Brownian motion starting at the Lebesgue measure of \\(\\mathcal{X}\\); more precisely if \\(W\\in P(\\Omega)\\) (space of probability Borel measure over \\(\\Omega\\)) is the law of a Brownian motion (Wiener measure) over \\(\\mathcal{X}\\); then we define the reversible Brownian motion to be the positive measure defined as \\[\\begin{align}\n    R(A):= \\int_{\\mathcal{X}} W_x(A) dx\n\\end{align}\\] and so \\(R\\in M^+(\\Omega)\\) (space of positive Borel measure over \\(\\Omega\\)). Treating \\(R\\) as an analogue of uniform distribution over path space, the Schrödinger bridge problem asks to solve for the following minimization problem \\[\\begin{align} \\tag{SB}\\label{pathformulation}\n    \\min \\{H(P\\mid R)\\mid P\\in P(\\Omega),  P_0 = \\mu_0, P_1=\\mu_1\\}\n\\end{align}\\] where \\(P_0:=X_0\\# P\\) and \\(P_1:= X_1\\# P\\) with \\(X_t:\\Omega\\to \\mathbb{R}\\) being the canonical projection; \\(\\mu_0, \\mu_1\\in P(\\mathcal{X})\\) are given, and \\[\\begin{align}\n    H(P\\mid R):=\n\\begin{cases}\n    \\frac{dP}{dR}(\\omega) &P&lt;&lt;R\\\\\n    0& \\textit{otherwise}\n\\end{cases}\n\\end{align}\\] being the differential entropy, which could possibly be negative and hence not bounded below.\nIn the following we will be describing different formulation of the Schrödinger bridge problem under the slight generalization to allow \\(R\\in M_+(\\Omega)\\) to be any reference measure (not necessarily reversible Brownian motion), namely\n\nthe static formulation \\(\\ref{staticformulation}\\)\nthe dual formulation \\(\\ref{staticdual}\\), \\(\\ref{dual}\\)\nthe dynamic formulation \\(\\ref{dynformulation}\\) (only for reversible Brownian motion)\n\nDue to technicality of proofs, we would either give only proof ideas or skip them entirely; this article is thus meant to be a companion to Léonard (2013)."
  },
  {
    "objectID": "SchrodingerBridge.html#static-formulation",
    "href": "SchrodingerBridge.html#static-formulation",
    "title": "Different Formulations of the Schrödinger Bridge Problem",
    "section": "Static Formulation",
    "text": "Static Formulation\nA common theme in optimization with boundary constrains is the equivalence between searching over path measures \\(P(\\Omega)\\) and the seemingly easier problem of searching over joint distributions \\(P(\\mathcal{X}^2)\\). Let \\(R\\in M^+(\\Omega)\\) be our reference measure and \\(R_{01}:= (X_0, X_1)\\#R\\in M^+(\\mathcal{X}^2)\\). Then the static analog of the problem asks to find the minimizing joint distribution of \\[\\begin{align}\\tag{StaticSB}\\label{staticformulation}\n    \\min \\{H(\\pi\\mid R_{01}): \\pi\\in P(\\mathcal{X}^2), \\pi_0:=X_0\\# \\pi = \\mu_0, \\pi_1:= X_1\\#\\pi = \\mu_1\\}\n\\end{align}\\] where \\(\\mu_0, \\mu_1\\in P(\\mathcal{X})\\) are given marginals and \\(H\\) is defined similarly. The first approach to Schrödinger bridge is to establish equivalence between the static and orignal (path) formulation.\n\nTheorem 1 (Proposition 2.3) If \\(\\hat P\\) is the solution to the path formulation \\(\\ref{pathformulation}\\) then \\(\\hat\\pi := \\hat P_{01}\\) is the solution to the static one \\(\\ref{staticformulation}\\). Conversely if \\(\\hat \\pi\\) is the static solution then the solution to \\(\\ref{pathformulation}\\) is obtained via the disintegration formula \\[\\begin{align}\n    \\hat P(\\cdot) := \\int_{\\mathcal{X}^2} R^{xy}(\\cdot)\\hat\\pi (dx, dy)\n\\end{align}\\] where \\(R^{xy}\\) is the law of the reference path measure conditioning on \\(X_0 = x, X_1 = y\\).\n\nThe disintegration formula is typical and is related to some technicality that follows from \\(\\Omega, \\mathcal{X}\\) being Polish. To give an example of the law induced by the bridge \\(R^{xy}\\), if \\(R\\) is the reversible Brownian motion, then \\(R^{xy}\\) is simply the Brownian bridge connecting the points \\(x, y\\).\n\nWell-posedness of Entropic formulation\nThe optimization \\(\\ref{pathformulation}\\) and \\(\\ref{staticformulation}\\) are called the entropic formulation of Schrödinger bridge. The next question is to ask for their well-posedness. Uniqueness is rather direct, which follows from the strict convexity of entropy functionals. Nonetheless, existence is an issue since differential entropy could be negative and unbounded in general; we are also in the case of allowing general \\(R\\). Here we state a sufficient condition that could overcome these obstacles,\n\nTheorem 2 (Proposition 2.5) The problems \\(\\ref{pathformulation}\\) and \\(\\ref{staticformulation}\\) admit a unique solution if the problem data \\((\\mu_0, \\mu_1, R)\\) satisfy\n\n\\(R_0 = R_1=:m\\)\n\\(H(\\mu_0\\mid m), H(\\mu_1\\mid m)&lt;\\infty\\)\nthere exists non-negative measurable functions \\(A, B:\\mathcal{X}\\to [0, \\infty)\\) such that we have the moment estimates for the problem data \\(R\\)\n\n\\(R_{01}(dx, dy)\\geq \\exp(-A(x)-A(y))m(dx)m(dy)\\)\n\\(\\int_{\\mathcal{X}^2}\\exp(-B(x)-B(y))R_{01}(dxdy)&lt;\\infty\\)\n\\(\\int_\\mathcal{X} \\exp(\\alpha(A+B))dm&lt;\\infty\\) for some \\(\\alpha&gt;0\\)"
  },
  {
    "objectID": "SchrodingerBridge.html#dual-formulation",
    "href": "SchrodingerBridge.html#dual-formulation",
    "title": "Different Formulations of the Schrödinger Bridge Problem",
    "section": "Dual Formulation",
    "text": "Dual Formulation\nAs usual, we consider the dual formulation of our optimization problem. It has been shown that the correct candidate is given by \\[\\begin{align}\\label{staticdual}\n    \\max \\{\\int_\\mathcal{X} \\phi d\\mu_0 + \\int_\\mathcal{X} \\psi d\\mu_1 - \\log\\int_{\\mathcal{X}^2} \\exp(\\phi\\oplus\\psi) dR_{01}: \\phi, \\psi\\in C_B(\\mathcal{X})\\} \\tag{StaticDual}\n\\end{align}\\] for the static formulation and by \\[\\begin{align}\\label{dual}\n        \\max \\{\\int_\\mathcal{X} \\phi d\\mu_0 + \\int_\\mathcal{X} \\psi d\\mu_1 - \\log\\int_{\\Omega} \\exp(f(X_0)+g(X_1)) dR: \\phi, \\psi\\in C_B(\\mathcal{X})\\}\\tag{Dual}\n\\end{align}\\] for the original formulation, where we have the flexibility to choose \\(B\\) that satisfies the condition in Theorem 2, that is, \\(\\int_{\\mathcal{X}^2}\\exp(-B(x)-B(y))R_{01}(dxdy)&lt;\\infty\\) and \\(u\\in C_B(\\mathcal{X})\\) if and only if \\(\\sup_{\\mathcal{X}}\\frac{u(x)}{1+B(x)}&lt;\\infty\\). The proof could be found in Léonard (2001). It is clear that the two formulation above yields the same solution of \\((\\phi, \\psi)\\) and as a remark, the need of \\(B\\) is due to the possible unboundedness of \\(R\\).\n\nPrimal-Dual relation\nAn important feature of the Schrödinger bridge problem is that we have a well-established relation between the solution to the dual problem with the original problem. To motivate the relation, we recall that if \\(\\Omega\\) is Polish, and \\(R\\) is a positive measure over \\(\\Omega\\); then by one can use convex conjugate technique to deduce the variational formulation of the entropy functional Léonard (2014) \\[\\begin{align}\n    H(P\\mid R):= \\sup \\{\\int u dP - \\log (\\int \\exp(u)dR):u\\in C_W(\\Omega)\\}\n\\end{align}\\] where \\(W:\\Omega\\to [0, \\infty)\\) is a function of your choice such that \\(\\int_\\Omega \\exp(-W) dR&lt;\\infty\\) and \\(u\\in C_W(\\Omega)\\) if and only if \\(\\sup_\\Omega\n\\frac{|u|}{(1+W)}\\). In addition . Hence if \\((\\hat\\phi, \\hat\\psi)\\) is a solution to the dual formulation, \\(\\hat\\pi\\) is the solution to the static formulation and \\(\\hat P\\) is the solution to the path formulation, we must have (modulo some technicality) \\[\\begin{align}\n    \\hat \\pi(dx, dy) = \\exp(\\hat \\psi(x)\\oplus \\hat\\psi(y))R_{01}(dx, dy) && d\\hat P = \\exp(\\hat\\psi(X_0) + \\hat\\phi(X_1)) dR  \n\\end{align}\\] where the Radon-Nikodym derivative terms \\(\\exp(\\hat\\phi(x) \\oplus \\hat \\psi(y))\\) are strictly positive with respect to the reference measure. If we let \\(f :=e^\\phi\\) and \\(g^\\psi\\) then the relation could be written as \\[\\begin{align}\n    \\hat \\pi (dx, dy) = f(x)g(y) R_{01}(dx, dy) && d\\hat P = f(X_0)g(X_1) dR\n\\end{align}\\]\n\n\nThe \\((f,g)\\)-transform\nThe primal dual relation has restricted us to consider only \\((f, g)\\) transform of a path measure \\(R\\), which yields powerful result when the reference measure \\(R\\) is Markov and reversible. As an example, reversible Brownian motion is both Markov and reversible.\n\nDefinition 1 (\\((f, g)\\)-transform) Let \\(f_0, g_1:\\mathcal{X}\\to [0, \\infty)\\) be non-negative functions such that \\(\\mathbb{E}_R(f_0(X_0)g_1(X_1)) = 1\\). Then we call that path measure \\[\\begin{align}\n    P:= f_0(X_0) g_1(X_1) R\\in P(\\Omega)\n\\end{align}\\] the \\((f,g)-\\)transform of \\(R\\) with respect to \\((f_0, g_1)\\). We further define the conditional expectations \\[\\begin{align}\n    f_t(z):= \\mathbb{E}_R( f_0(X_0) \\mid X_t = z) && g_t(z):= \\mathbb{E}_R (g_1(X_1)\\mid X_t = z)\n\\end{align}\\] for \\(P_t-\\)almost all \\(z\\in \\mathcal{X}\\).\n\n\nRemark 1. Note that \\(f_t, g_t\\) are solutions to the Feymann-Kac formula on the backward and forward dynamics of \\(R\\) respectively: \\[\\begin{align}\n    (-\\partial_t  + L)f(t, x) = 0 && f(0, x)= f_0(x)\\\\\n    (\\partial_t + L)g(t, x) = 0 && g(1, x) = g_1(x)\n\\end{align}\\] with \\(f_t(x) = f(t, x)\\) and \\(g_t(x) = g(t, x)\\) where \\(L\\) is the infinitestimal generator of \\(R\\).\n\nThe important observation is that \\((f, g)\\)-transform preserves Markov property and every marginal is described fully by \\(f, g\\) via conditional expectation.\n\nTheorem 3 (Theorem 3.4) Let \\(P = f_0(X_0) g_1(X_1)R\\) be a \\((f, g)\\)-transform of \\(R\\). Then \\(P_t\\in P(\\mathcal{X})\\) has Randon-Nikodym derivative with respect to the reversible measure \\(m\\) given by \\[\\begin{align}\n        P_t = f_t g_t m\n    \\end{align}\\] which follows that \\(f_t g_t&gt;0\\) a.s. in \\(P_t\\).\n\n\nProof (Idea of Proof). Markov property could be shown by a careful analysis by acting on bounded measurable functions to \\(P\\). The statement for Radon-Nikodym derivative follows from the calculations: \\[\\begin{align}\n    \\frac{dP_t}{dm}(X_t) &\\overset{R\\textit{ is reversible}}{=} \\frac{dP_t}{dR_t}(X_t) \\overset{(*)}{=} \\mathbb{E}_R(\\frac{dP}{dR} \\mid X_t)\\\\\n    & = \\mathbb{E}_R(f_0(X_0)g_1(X_1)\\mid X_t)\\\\\n    &  \\overset{Markov}{=} \\mathbb{E}_R(f_0(X_0)\\mid X_t) \\mathbb{E}_R(g_1(X_1)\\mid X_t) = f_tg_t(X_t)\n\\end{align}\\] in which \\((*)\\) could be checked by definition of conditional expectation.\n\nWe are now motivated to describe the dynamics, in particular, the Markov generator of \\(P\\) using \\(f_t, g_t\\). It turns out \\(f_t, g_t\\) can describe the forward generator and the backward generator of \\(P\\) respectively, which is the content of\n\nTheorem 4 (Informal Statement 3.6) Under hypotheses on the reference measure \\(R\\) and suitably chosen test functions \\(u\\in U_R\\) with \\(u:[0, 1]\\times \\mathcal\\to \\mathbb{R}\\), then we have the forward generator given by \\[\\begin{align}\n    \\vec{A_t}u(x) = Lu(x) + \\frac{\\Gamma(g_t, u)(x)}{g_t(x)} && (t, x)\\in [0, 1)\\times \\mathcal{X}\n\\end{align}\\] and the backward generator given by \\[\\begin{align}\n    A_t^{\\textit{back}} u(x) = Lu(x) + \\frac{\\Gamma(f_t, u)(x)}{f_t(x)}&& (t, x)\\in 0, 1]\\times \\mathcal{X}\n\\end{align}\\] where \\(L\\) is the generator of the reversible process \\(R\\) (which does not differentiate between the forward and backward case) and \\(\\Gamma(u, v):= L(uv) - uLv - vLu\\) is the of \\(R\\) which measures the tendency of \\(L\\) from being a derivation.\n\n\n\nSchrödinger’s system\nWhat remains is to characterize the \\((f, g)\\) transform that would give the solution to \\(\\ref{pathformulation}\\). We see that it is the case when \\((f_0, g_1)\\) solves the Schrödinger system.\n\nTheorem 5 (Theorem 2.12) Under regularity assumption of \\(R\\) (still assumed to be Markovian and reversible),then \\(\\hat P = f_0(X_0) g_1(X_1) R\\) is the solution to Schrödinger problem if and only if \\(f_0, g_1\\) are solutions to \\[\\begin{align} \\label{SchrodingerSystem}\\tag{S-system}\n    \\begin{cases}\n        f_0 g_0(x) = f_0(x) \\mathbb{E}_R(g_1(X_1)\\mid X_0 = x) = \\frac{d\\mu_0}{dm}(x) \\\\\n        f_1 g_1(y) = g_1(y) \\mathbb{E}_R(f_0(X_0)\\mid X_1 = y) = \\frac{d\\mu_1}{dm}(y)\n    \\end{cases}\n\\end{align}\\] where \\(R_0 =R_1 := m\\) and the boundary constrain \\(\\mu_0, \\mu_1\\) are absolute continuous to \\(m\\) hence having Radon-nikodym derivatives."
  },
  {
    "objectID": "SchrodingerBridge.html#dynamic-formulation",
    "href": "SchrodingerBridge.html#dynamic-formulation",
    "title": "Different Formulations of the Schrödinger Bridge Problem",
    "section": "Dynamic formulation",
    "text": "Dynamic formulation\nNow we go back to the case where \\(R\\) is the reversible Brownian motion (which we recall is the Brownian motion with initial distribution given by \\(Leb\\)) and see how the machneries of \\((f, g)\\) transform provides us with the dynamic formulation of Schrödinger bridge, which is the content of\n\nTheorem 6 (Proposition 4.1) Let \\(\\mu_0, \\mu_1\\in P_2(\\mathbb{R}^d)\\) be such that \\(H(\\mu_0\\mid Leb), H(\\mu_1\\mid Leb)&lt;\\infty\\). Then we are in the assumption of Theorem 2 for the reversible Brownian motion \\(R\\). Let \\(\\hat P\\in P(\\Omega)\\) be the unique path measure that satisfies \\(\\ref{pathformulation}\\) then \\(\\mu_t:= \\hat P_t\\) with \\(v_t = \\nabla \\psi_t\\) where \\(\\psi_t = \\log g_t\\) is the unique minimizer of the problem\n\\[\\begin{align}\\tag{dynSB}\\label{dynformulation}\n    \\min \\{\\int_{0}^1\\int_{\\mathcal{X}} \\frac{|v_t(x)|^2}{2}\\mu_t(dx)dt: (\\mu, v) \\textit{ satisfies Fokker-Planck}\\}\n\\end{align}\\]\nwhere \\(g_t(z):= \\mathbb{E}_P(g_1(X_1)\\mid X_t=z)\\) is part of the solution to \\(\\ref{SchrodingerSystem}\\) and the constrain to \\(\\mu, v\\) is \\[\\begin{align}\\label{FokkerPlanck}\\tag{Fokker-Planck}\n    \\partial_t \\mu_t - \\frac{1}{2}\\Delta\\mu_t - \\nabla\\cdot(\\mu_t \\nabla v_t) = 0\n\\end{align}\\] Conversely, if \\((\\mu, v)\\) is the minimizer of \\(\\ref{dynformulation}\\), then they describes the optimizer of \\(\\ref{pathformulation}\\) via its marginals and conditional expectations.\n\nTo see an idea of the proof, we recall an important corollary about path entropy from Girsanov’s theory (stated in Léonard (2012)) in stochastic calculus\n\nLemma 1 (Girsanov’s corollary) Let \\(Q\\in P(\\Omega)\\) and \\(H(Q\\mid R)&lt;\\infty\\) with respect to the reversible Brownian motion \\(R\\). Then there exists some (predictable) \\(\\mathcal{X}-\\)valued drift \\(\\beta\\) such that \\(\\mathbb{E}_Q\\int_{[0, 1]} |\\beta_t|^2dt&lt;\\infty\\) and \\(Q\\) solves the martingale problem (so is a Makorv diffusion) associted with the forward generator \\[\\begin{equation}\n    (\\partial_t + \\Delta /2 + \\beta_t \\cdot \\nabla)_{t\\in [0, 1]}\n\\end{equation}\\]. In addition in such case, we have \\[\\begin{align}\n    H(Q\\mid R) - H(Q_0\\mid m) = E_Q\\int_{[0, 1]}\\frac{|\\beta_t|^2}{2}dt \\label{Girsanov}\n\\end{align}\\]\n\n\nProof (For the theorem). To apply the Lemma, we shall look at the forward generators of any given \\((f, g)\\) transform of \\(R\\). To this end we first compute the infinitestimal generator and the carr’{e} du champ operator of \\(R\\): \\[\\begin{align}\n    L &= \\frac{1}{2}\\Delta \\\\\n    \\Gamma(u, v) &= L(uv) - uLv-vLu = \\nabla u \\cdot \\nabla v\n\\end{align}\\] It follows that the generators of an \\((f, g)\\) transform is given by \\[\\begin{align}\n    \\vec{A}_t u(x) = Lu(x) + \\frac{\\Gamma(g_t, u)(x)}{g_t(x)} = \\frac{1}{2}\\Delta u(x) + \\frac{\\nabla g_t\\cdot \\nabla u(x)}{g_t(x)} = \\frac{1}{2}\\Delta u+\\nabla (\\log g_t)\\cdot \\nabla u\n\\end{align}\\] and similarly \\[\\begin{align}\n    A_t^{\\textit{back}} u(x) = Lu(x) + \\frac{\\Gamma(f_t, u)(x)}{f_t(x)} = \\frac{1}{2}\\Delta u(x) + \\frac{\\nabla f_t\\cdot \\nabla u(x)}{f_t(x)} = \\frac{1}{2}\\Delta u+\\nabla (\\log f_t)\\cdot \\nabla u\n\\end{align}\\] It then follows that if \\(Q\\) is an \\((f, g)\\) transform with the boundary distribution \\(\\mu_0, \\mu_1\\), we must have from Lemma 1 that \\[\\begin{align}\n    H(Q\\mid R) - H(\\mu_0\\mid m) = E_Q\\int_{[0, 1]}\\frac{|\\nabla \\log g_t|^2}{2}dt\n\\end{align}\\] To further expand the expectation, we know from standard Markov diffusion theory that the path marginals have to satisfies the Fokker-Planck equation induced by \\(\\vec{A_t}\\), that is, \\[\\begin{align}\n    \\partial_t \\mu_t(x) - \\vec{A_t^*}\\mu(x) = \\partial_t \\mu_t - \\frac{1}{2}\\Delta\\mu_t - \\nabla\\cdot(\\mu_t \\nabla (\\log g_t)) = 0 \\label{FPE}\n\\end{align}\\] which implies precisely that \\((\\mu_t, \\nabla\\log g_t)\\) satisfies \\(\\ref{FokkerPlanck}\\). Hence taking infimum on Lemma 1 (over \\((f, g)\\) transform) gives us \\[\\begin{align}\n    \\inf_{Q} H(Q\\mid R) - H(\\mu_0\\mid m) = \\inf_{(\\mu_t, g_t)} \\int_{\\mathcal{X}}\\int_0^1 \\frac{1}{2}|\\nabla \\log g_t|^2 \\mu_t(x) dtdx\n\\end{align}\\] where \\((\\mu_t, g_t)\\) satisfies \\(\\ref{FokkerPlanck}\\).\n\nFrom the derivation, we actually see that the value of \\(\\ref{pathformulation}\\) and \\(\\ref{dynformulation}\\) is off by a constant \\(H(\\mu_0\\mid m)\\) from the problem data."
  },
  {
    "objectID": "MOT.html",
    "href": "MOT.html",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "",
    "text": "A central topic in mathematical finance is the pricing of financial derivatives—contracts that facilitate the exchange or allocation of resources across different time periods. We introduce the principle of arbitrage-free pricing in financial markets and focus on a fundamental class of financial derivatives known as options. Following the classical approach of model-based finance, we price options based on two different ideas: hedging and risk-neural pricing, which have essential underlying connections. Moving to an incomplete market, where the stock price dynamics are unknown, we motivate model-free finance, where the corresponding optimization problems for option pricing could be naturally interpreted in the context of optimal transport and martingale optimal transport."
  },
  {
    "objectID": "MOT.html#arbitrage-free-pricing",
    "href": "MOT.html#arbitrage-free-pricing",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Arbitrage-Free Pricing",
    "text": "Arbitrage-Free Pricing\nThe fundamental principle of derivative pricing requires no arbitrage to exist in the market. Intuitively, arbitrage refers to the existence of a self-financing portfolio that requires no external endowment, yet guarantees a non-negative payoff almost surely, and yields a strictly positive profit with a positive probability. A mathematically rigorous definition of arbitrage relies on the notion of market viability, as discussed in (Karatzas and Kardaras 2021). Nevertheless, the concept of no-arbitrage pricing can be illustrated through the following simple example.\nConsider a forward contract currently signed, which obligates the holder to receive one unit of stock at a future time \\(T\\) and to pay the forward price \\(F(0,T)\\) at time \\(T\\). A self-financing arbitrage portfolio can be constructed as follows:\n\nEnter the forward contract at time \\(0\\).\nSell \\(1\\) unit of stock at time \\(0\\), receiving \\(S_0\\) cash.\nPay the forward price \\(F(0,T)\\) and receive \\(1\\) unit of stock at time \\(T\\).\n\nSince the stock received at time \\(T\\) exactly offsets the short position, this strategy gives a risk-free portfolio. The net cash flow (amount paid by the portfolio holder) at time \\(T\\) is \\(F(0,T)-S_0e^{rT}\\), which shall be non-negative to prevent arbitrage, i.e., \\(F(0,T)-S_0e^{rT}\\geq 0\\). Conversely, by reversing the strategy (buying the stock, shorting the forward contract), the no-arbitrage condition requires \\(F(0,T)-S_0e^{rT}\\leq 0\\). Combining both inequalities yields the arbitrage-free price of the forward contract: \\(F(0,T) = S_0e^{rT}\\).\nThis example, though simple, illustrates the concept of hedging—reducing or eliminating risk by trading other assets. In the example above, the forward contract carries risk because the future stock price \\(S_T\\) is unknown at time \\(0\\). However, perfect hedging is achieved by holding one unit of stock into the future. Since the initial stock price \\(S_0\\) is observable at time \\(0\\), this strategy eliminates uncertainty, ensuring a risk-free position."
  },
  {
    "objectID": "MOT.html#options",
    "href": "MOT.html#options",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Options",
    "text": "Options\nIn real financial markets, more complex financial derivatives are traded, one of which is called options. An option is a contract written on an underlying asset, signed at present but granting the holder the right, rather than the obligation, to execute it in the future. Unlike a forward contract, the option holder can choose whether to exercise the option based on its profitability at maturity. In our discussion, we assume the underlying asset to be the stock with price \\(\\{S_t\\}\\).\nA European call option with time to maturity \\(T\\) and strike price \\(K\\) grants the holder the right to buy one unit of stock at time \\(T\\) at price \\(K\\). If \\(S_T&gt;K\\), the holder exercises the option, receiving an immediate payoff of \\(S_T - K\\). If \\(S_T\\leq K\\), the holder does not exercise the option, resulting in a payoff of \\(0\\). Written compactly, such a European call option has payoff \\[\n(S_T - K)_+ := \\max\\{S_T-K,0\\}.\n\\] Similarly, a European put option with time to maturity \\(T\\) and strike price \\(K\\) grants the holder the right to sell one unit of stock at time \\(T\\) at price \\(K\\). Its payoff function is given by: \\[\n(K-S_T)_+ := \\max\\{K-S_T,0\\}.\n\\] European options are the simplest ones, yet there exist various other types of options on the market. For example, American options allow the holder to exercise at any time before maturity. Path-dependent options have payoff functions of the form \\(g(S_{[0,T]})\\), which depend on the entire stock price trajectory over \\([0,T]\\). A common example is the lookback option, where the payoff is given by \\[\ng(S_{[0,T]}) = \\sup_{t\\in[0,T]} S_t.\n\\] For a comprehensive introduction to continuous-time finance and option pricing, we refer the readers to (Björk 2009)."
  },
  {
    "objectID": "MOT.html#pricing-through-hedging",
    "href": "MOT.html#pricing-through-hedging",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Pricing through Hedging",
    "text": "Pricing through Hedging\nThe first approach to pricing a European call option is based on hedging, which is motivated by the fundamental principle: “If two portfolios have the same payoff, they must have the same price to prevent arbitrage”. Therefore, if we can replicate the payoff of a European call option using a portfolio that consists of cash and stock, the option price is just the price of the replicating portfolio, which only requires trivial calculations. Replication is equivalent to perfect hedging, since selling the replicating portfolio while holding the option eliminates all risk.\nConsider a replicating portfolio consisting of:\n\n\\(a_t\\) units of stock at time \\(t\\).\n\\(b_t\\) units of cash at time \\(t\\).\n\nThe portfolio value at time \\(t\\) is: \\[\nV_t = a_tS_t + b_te^{rt}.\n\\] By Itô’s formula, \\[\ndV_t = a_t\\,dS_t + S_t\\,da_t + \\,d\\langle S,a\\rangle_t + e^{rt}\\,db_t + rb_te^{rt}\\,dt,\n\\] where \\(\\langle \\cdot,\\cdot\\rangle_t\\) denotes the quadratic variation up to time \\(t\\). The changes in the portfolio value are due to market movements \\(a_t\\,dS_t + rb_te^{rt}\\,dt\\) and investor adjustments \\(S_t\\,da_t + \\,d\\langle S,a\\rangle_t + e^{rt}\\,db_t\\). Since no external endowments are allowed, the portfolio must be self-financing, meaning that the change in its value is only caused by market movements: \\[\ndV_t = a_t\\,dS_t  + rb_te^{rt}\\,dt.\n\\] To proceed, we assume the Markovian representation \\(V_t = u(t,S_t)\\), where \\(u(t,s)\\) denotes the portfolio value at time \\(t\\) on observing \\(S_t = s\\). Itô’s formula implies \\[\ndV_t = \\partial_t u\\,dt + \\partial_s u\\,dS_t + \\frac{1}{2}\\partial_{ss} u\\,d\\langle S,S\\rangle_t.\n\\] Substituting the dynamics of the Black-Scholes model yields \\[\ndV_t = \\left(\\partial_t u+ \\mu S_t\\partial_s u + \\frac{\\sigma^2}{2}S_t^2\\partial_{ss}u\\right)\\,dt + \\sigma S_t\\partial_s u\\,dW_t.\n\\] By comparing the coefficients, we get \\[\n\\begin{cases}\na_t = \\partial_s u(t,S_t)\\\\\nb_t = \\frac{1}{r}e^{-rt}(\\partial_t u(t,S_t)+ \\frac{\\sigma^2}{2}S_t^2\\partial_{ss}u(t,S_t))\n\\end{cases}.\n\\] Recall that \\(a_tS_t + b_te^{rt} = V_t = u(t,S_t)\\). Substituting \\(a_t,b_t\\) yields the Black-Scholes PDE for \\(u = u(t,s)\\): \\[\n\\partial_t u + rs\\partial_s u + \\frac{\\sigma^2}{2}s^2\\partial_{ss}u - ru = 0,\\quad u(T,s) = (s - K)_+.\n\\] Solving this PDE (a technical step omitted here) provides the celebrated Black-Scholes formula for the European call option price: \\[\nV_0 = u(0,S_0).\n\\]"
  },
  {
    "objectID": "MOT.html#risk-neutral-pricing",
    "href": "MOT.html#risk-neutral-pricing",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Risk-Neutral Pricing",
    "text": "Risk-Neutral Pricing\nThe difficulty of pricing lies in the fact that under the physical measure \\(\\mathbb{P}\\), the discounted expected payoff, is not equal to the price, due to the existence of risk premium to compensate for the uncertainty taken by the holders of the derivative. That means, if a holder is risk-neutral, the discounted expected payoff is just the price, in which situation the pricing problem reduces to simple calculations of expectations.\nRisk-neutral pricing follows this main idea and aims to view the current world through the lens of a risk-neutral (martingale) measure \\(\\mathbb{Q}\\), under which all investors are risk-neutral, i.e., \\(\\{e^{-rt}S_t\\}\\) is a martingale. The European call price is nothing but \\[\ne^{-rT}\\mathbb{E}_{\\mathbb{Q}}(S_T - K)_+.\n\\] When it comes to the calculation, one needs to guarantee the existence of \\(\\mathbb{Q}\\) and the knowledge on the distribution of \\(S_T\\) under \\(\\mathbb{Q}\\). Followed from the fact that \\(\\{e^{-rt}S_t\\}\\) is a martingale, we rewrite the stock price dynamics: \\[\ndS_t = rS_t\\,dt + \\sigma S_t\\left(\\frac{\\mu - r}{\\sigma}\\,dt+dW_t\\right),\n\\] denoting \\(dW_t^\\mathbb{Q}:= \\frac{\\mu - r}{\\sigma}\\,dt+dW_t\\) as the BM under \\(\\mathbb{Q}\\). An application of Girsanov theorem guarantees the existence of \\(\\mathbb{Q}\\) and provides the Radon-Nikodym derivative between the physical measure and the risk-neutral measure. Solving for the SDE yields \\[\nS_t = e^{(r-\\frac{\\sigma^2}{2})t + \\sigma W_t^{\\mathbb{Q}}},\n\\] which tells the explicit distribution of \\(S_T\\) under \\(\\mathbb{Q}\\). After performing calculations, one yields exactly the same Black-Scholes formula for the European call option price as the one derived from hedging."
  },
  {
    "objectID": "MOT.html#market-completeness-and-super-replication",
    "href": "MOT.html#market-completeness-and-super-replication",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Market Completeness and Super-Replication",
    "text": "Market Completeness and Super-Replication\nMartingale measures serve as a crucial component in mathematical finance and are known to be associated with several fundamental theorems. To name a few, the First Fundamental Theorem of Asset Pricing states that the market is viable (arbitrage-free) if and only if at least one martingale measure exists. The Second Fundamental Theorem of Asset Pricing claims that the market is complete iff the martingale measure exists and is unique. Without even realizing it, the market completeness greatly simplifies the problem of option pricing in a Black-Scholes market.\nA viable market is called complete if any European payoffs at any time of maturity \\(T\\) can be perfectly replicated. It turns out that the perfect replicability of European call and put payoffs implies the perfect replicability of any European payoff functions (Carr-Madan formula). That means, the European call and put payoffs serve as the “basis” for all European payoffs. Option pricing in a complete market (e.g., the Black-Scholes market) is therefore easy, but unfortunately is almost never the case in real life.\nOne of the common examples of an incomplete market is the stochastic volatility model, where the volatility \\(\\{\\sigma_t\\}\\) is also a stochastic process with its dynamics given by an SDE under a different Brownian motion. There is only one risky asset but two sources of randomness (i.e., one Brownian motion for \\(S\\) and one for \\(\\sigma\\)), which causes the failure of perfect replication. Intuitively understanding through an analogue to the solvability of linear systems, the market is complete (a linear system has a unique solution) if and only if the number of different risky assets (number of equations) equals the number of different sources of randomness (number of unknowns).\nAs we would naturally expect, pricing becomes much harder in an incomplete market, since the arbitrage-free price is an interval, rather than a single number. On the side of hedging, perfectly hedging becomes impossible. On the side of risk-neutral pricing, there exist infinitely many martingale measures. A new pricing criterion is given by super-replication instead, i.e., ensuring the payoff of the replicating portfolio is larger than the payoff of the option. The minimal cost of super-replication for an option writer, denoted by \\(C_{\\text{sell}}\\), is the highest arbitrage-free price on the market. Conversely, taking the perspective of an option buyer, one can define \\(C_{\\text{buy}}\\) as the maximal cost of sub-replication, which is the lowest arbitrage-free price on the market. Therefore, the interval of arbitrage-free price is provided by \\([C_{\\text{buy}},C_{\\text{sell}}]\\).\nThe super-replication approach sounds natural and attractive from the hedging perspective, but what can we say about martingale measures? Does that mean the hedging perspective would be superior over the martingale measure approach in an incomplete market? Surprisingly, the super-replication duality (Henry-Labordère 2017) holds, which can be proved using the Fenchel-Rockafeller Duality, stating that the minimum cost of super-replication is equal to the maximum martingale price. This result implies that hedging strategies and martingale measures are two sides of the same coin, in the sense that they should be understood as dual optimization variables of each other."
  },
  {
    "objectID": "MOT.html#pricing-options-with-joint-payoffs",
    "href": "MOT.html#pricing-options-with-joint-payoffs",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Pricing Options with Joint Payoffs",
    "text": "Pricing Options with Joint Payoffs\nAssume there are two stocks on the market with stock prices \\(\\{S^1_t\\}\\) and \\(\\{S^2_t\\}\\). We wish to price a European option with maturity \\(T\\) and a given payoff function \\(C\\), which is a joint function in both stock prices, i.e. \\(c(S^1_T,S^2_T)\\). Denote by \\(\\mathbb{P}^1\\) and \\(\\mathbb{P}^2\\) the laws of \\(S^1_T\\) and \\(S^2_T\\) under the martingale measure (known). The seller’s price is the minimum cost of super-replication: \\[\n\\text{MK}_2:=\\inf_{(\\lambda_1,\\lambda_2)\\in \\mathscr{P}^*(\\mathbb{P}^1,\\mathbb{P}^2)} \\mathbb{E}_{\\mathbb{P}^1}\\lambda_1(S^1_T) + \\mathbb{E}_{\\mathbb{P}^2}\\lambda_2(S^2_T),\n\\] where \\[\n\\mathscr{P}^*(\\mathbb{P}^1,\\mathbb{P}^2) := \\{(\\lambda_1,\\lambda_2):\\lambda_1(s_1) + \\lambda_2(s_2)\\geq c(s_1,s_2)\\}.\n\\] The objective function is the cost of replication with European payoffs \\(\\lambda_1\\) and \\(\\lambda_2\\), while \\(\\mathscr{P}^*(\\mathbb{P}^1,\\mathbb{P}^2)\\) is the collection of payoff functions that admits a super-replication.\nThrough the Kantorovich Duality, it is clear that \\[\n\\text{MK}_2=\\sup_{\\mathbb{P}\\in\\mathscr{P}(\\mathbb{P}^1,\\mathbb{P}^2)}\\mathbb{E}_{\\mathbb{P}}[c(S^1_T,S^2_T)],\n\\] where \\(\\mathscr{P}(\\mathbb{P}^1,\\mathbb{P}^2)\\) is the collection of probability measures, under which \\(S^1_T\\sim \\mathbb{P}^1\\) and \\(S^2_T\\sim \\mathbb{P}^2\\).\nWe remark that, the primal and dual optimization problems align with those in optimal transport. However, those optimization problems no longer have the physical interpretation as transport problems, but are interpreted in the sense of hedging. Nevertheless, this optimization problem can be numerically solved through techniques that have been developed for optimal transport, e.g., Sinkhorn’s Algorithm."
  },
  {
    "objectID": "MOT.html#pricing-options-with-path-dependent-payoffs",
    "href": "MOT.html#pricing-options-with-path-dependent-payoffs",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Pricing Options with Path-dependent Payoffs",
    "text": "Pricing Options with Path-dependent Payoffs\nAssume there is a single stock considered at two different time points \\(t_1&lt;t_2\\). For simplicity, we denote \\(S_1:= S_{t_1}\\) and \\(S_2:= S_{t_2}\\). The path-dependent option payoff has the form \\(c(S_1,S_2)\\). Denote by \\(\\mathbb{P}^1\\) and \\(\\mathbb{P}^2\\) the laws of \\(S^1\\) and \\(S^2\\) under the martingale measure (known).\nThe seller’s price is the minimum cost of super-replication: \\[\n\\widetilde{\\text{MK}}_2:=\\inf_{(\\lambda_1,\\lambda_2,H)\\in \\mathscr{M}^*(\\mathbb{P}^1,\\mathbb{P}^2)} \\mathbb{E}_{\\mathbb{P}^1}\\lambda_1(S_1) + \\mathbb{E}_{\\mathbb{P}^2}\\lambda_2(S_2),\n\\] where \\[\n\\mathscr{M}^*(\\mathbb{P}^1,\\mathbb{P}^2) := \\{(\\lambda_1,\\lambda_2,H):\\lambda_1(s_1) + \\lambda_2(s_2) + H(s_1)(s_2-s_1)\\geq c(s_1,s_2)\\}.\n\\] The objective function is the cost of replication with European payoffs \\(\\lambda_1\\) and \\(\\lambda_2\\), while \\(\\mathscr{M}^*(\\mathbb{P}^1,\\mathbb{P}^2)\\) is the collection of payoff functions and the static trading strategy \\(H\\) that admits a super-replication. Note that due to the presence of two different time points, one can trade the stock at time \\(0,t_1,t_2\\), which results in the extra \\(H\\) variable in the optimization problem.\nThrough Fenchel-Rockafeller Duality, one can prove that \\[\n\\widetilde{\\text{MK}}_2=\\sup_{\\mathbb{P}\\in\\mathscr{M}(\\mathbb{P}^1,\\mathbb{P}^2)}\\mathbb{E}_{\\mathbb{P}}[c(S_1,S_2)],\n\\] where \\(\\mathscr{M}(\\mathbb{P}^1,\\mathbb{P}^2)\\) is the collection of probability measures, under which \\(S^1_T\\sim \\mathbb{P}^1\\) and \\(S^2_T\\sim \\mathbb{P}^2\\) and \\(\\mathbb{E}_{\\mathbb{P}}(S_2|S_1) = S_1\\). Clearly, the presence of the constraint \\(\\mathbb{E}_{\\mathbb{P}}(S_2|S_1) = S_1\\) in the dual problem is the direct consequence of the extra \\(H\\) variable in the primal problem, which is actually a martingale condition restricted to time points \\(t_1,t_2\\).\nThis problem is called martingale optimal transport due to the extra martingale constraint, which is reasonable to appear due to \\(\\{S_t\\}\\) being a martingale under the martingale measure when \\(r=0\\). In other words, the transport trajectory from \\(\\mathbb{P}^1\\) to \\(\\mathbb{P}^2\\) cannot be arbitrary due to the introduction of two different time points.\nNote that unlike \\(\\mathscr{P}^*(\\mathbb{P}^1,\\mathbb{P}^2)\\) which always contains the trivial coupling, extra conditions are required to ensure that \\(\\mathscr{M}(\\mathbb{P}^1,\\mathbb{P}^2)\\) is not empty. An if and only if condition is given by the convex order between two measures, denoted \\(\\mathbb{P}^1\\leq \\mathbb{P}^2\\), which is defined as \\[\n\\mathbb{E}_{\\mathbb{P}^1}(S_1-K)_+\\leq \\mathbb{E}_{\\mathbb{P}^2}(S_2-K)_+,\\ \\forall K.\n\\] Through the Black-Scholes formula, one can easily check that if \\(\\{S_t\\}\\) denotes the stock price in a Black-Scholes market with a single stock, then \\(\\mathbb{P}^1\\leq \\mathbb{P}^2\\) for \\(\\forall 0\\leq t_1&lt;t_2\\)."
  },
  {
    "objectID": "MOT.html#a-vix-constrained-martingale-optimal-transport",
    "href": "MOT.html#a-vix-constrained-martingale-optimal-transport",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "A VIX-constrained martingale optimal transport",
    "text": "A VIX-constrained martingale optimal transport\nTo capture the broader array of hedging instruments within the model-free pricing framework based on martingale optimal transport, recent literature has studied several constrained MOT problems that extend the basic setting. In this section, we introduce one such extension by incorporating the trading of VIX-index options into the framework.\nThe main literature for this section is (De Marco and Henry-Labordère 2015).\n\nVIX futures and VIX options\nVIX futures and VIX options, traded on the CBOE, have become popular volatility derivatives. The VIX index at a future expiry \\(t_1\\) is by definition the price at \\(t_1\\) of the 30 day log-contract which pays \\(-\\frac{2}{t_2-t_1} \\ln \\frac{S_2}{S_1}\\) at \\(t_2=t_1+30\\) days:\n\\[\\begin{equation}\n    \\mathrm{VIX}_{t_1}^2 \\equiv-\\frac{2}{\\Delta} \\mathbb{E}_{t_1}^{\\mathbb{P}^{\\mathrm{mkt}}}\\left[\\ln \\left(\\frac{S_2}{S_1}\\right)\\right], \\quad \\Delta=t_2-t_1\n\\end{equation}\\]\nThis definition is at first sight strange as \\(\\mathrm{VIX}_{t_1}\\) seems to depend on the probability measure \\(\\mathbb{P}^{m k t}\\) (i.e., pricing model) used to value the log-contract at \\(t_1\\). A choice should therefore be made and the probability measure \\(\\mathbb{P}^{\\text {mkt }}\\) selected should be included in the term sheet which describes the payoff to the client. In fact, this conclusion is not correct and the value VIX \\(_{t_1}\\) is independent of the choice of \\(\\mathbb{P}^{\\text {mkt }}\\) (i.e., model-independence).\nIndeed, by an application of Carr-Madan formula, the arbitrage-free price (model-independent) at \\(t_1\\) can be implied from the \\(t_1\\) market value of \\(t_2\\)-Vanillas:\n\\[\n\\mathrm{VIX}_{t_1}^2 \\equiv \\frac{2}{\\Delta}\\left(\\int_0^{S_1} \\frac{P\\left(t_1, t_2, K\\right)}{K^2} d K+\\int_0^{S_1} \\frac{C\\left(t_1, t_2, K\\right)}{K^2} d K\\right)\n\\]\nwith \\(P\\left(t_1, t_2, K\\right)\\) (resp. \\(\\left.C\\left(t_1, t_2, K\\right)\\right)\\) the undiscounted market price at \\(t_1\\) of a put (resp. call) option with strike \\(K\\) and maturity \\(t_2\\). The payoff of a call option on VIX expiring at \\(t_1\\) with strike \\(K\\) is \\(\\left(\\mathrm{VIX}_{t_1}-K\\right)^{+}\\). Below, the market value (at \\(t=0\\) ) for the VIX future (i.e., \\(K=0\\) ) is denoted VIX.\n\n\nThe constrained MOT duality\nNow we may include the trading of VIX options in our settings.\nFor further reference, we denote by \\(\\mathcal{M}\\left(\\mathbb{P}^1, \\mathbb{P}^2\\right.\\), VIX \\()\\) the set of all martingale measures \\(\\mathbb{P}\\) on \\(I_1 \\times I_2 \\times I_X\\) having marginals \\(\\mathbb{P}^1, \\mathbb{P}^2\\) with mean \\(S_0\\) and such that VIX \\(=\\mathbb{E}^{\\mathbb{P}}\\left[\\mathrm{VIX}_{t_1}\\right],\\) that is:\n\\[\\begin{equation}\n    \\begin{aligned}\n\\mathcal{M}\\left(\\mathbb{P}^1, \\mathbb{P}^2, \\mathrm{VIX}\\right)=\\{\\mathbb{P} & \\in \\mathcal{P}\\left(I_1 \\times I_2 \\times I_X\\right): S_1 \\stackrel{\\mathbb{P}}{\\sim} \\mathbb{P}^1, S_2 \\stackrel{\\mathbb{P}}{\\sim} \\mathbb{P}^2, \\quad\\mathbb{E}^{\\mathbb{P}}\\left[\\mathrm{VIX}_{t_1}\\right]=\\mathrm{VIX}, \\\\\n& \\mathbb{E}^{\\mathbb{P}}\\left[S_2 \\mid S_1, \\mathrm{VIX}_{t_1}\\right]=S_1, \\quad \\left.\\mathbb{E}^{\\mathbb{P}}\\left[\\left.-\\frac{2}{\\Delta} \\log \\frac{S_2}{S_1} \\right\\rvert\\, S_1, \\mathrm{VIX}_{t_1}\\right]=\\mathrm{VIX}_{t_1}^2\\right\\}.\n\\end{aligned}\n\\end{equation}\\]\nIn other words, we request both the stock price, and also the VIX index, as a traded derivative, to be martingale under the pricing measure.\nWe define our constrained MOT for a VIX call option expiring at \\(t_1\\) with strike \\(K\\) as\n\\[\\begin{equation}\n\\mathrm{MK}_{\\mathrm{vix}} \\equiv \\inf _{\\lambda_1 \\in \\mathrm{~L}^1\\left(\\mathbb{P}^1\\right), \\lambda_2 \\in \\mathrm{~L}^1\\left(\\mathbb{P}^2\\right), \\lambda \\in \\mathbb{R}, H_S, H_X} \\mathbb{E}^{\\mathbb{P}^1}\\left[\\lambda_1\\left(S_1\\right)\\right]+\\mathbb{E}^{\\mathbb{P}^2}\\left[\\lambda_2\\left(S_2\\right)\\right]+\\lambda \\mathrm{VIX}\n\\end{equation}\\]\nsuch that for all \\(\\left(s_1, s_2, x\\right) \\in I_1 \\times I_2 \\times I_X\\),\\ \\[\\begin{equation}\\label{eq6}\n\\begin{aligned}\n        \\lambda_1\\left(s_1\\right)+\\lambda_2\\left(s_2\\right)+\\lambda \\sqrt{x} & +H_S\\left(s_1, x\\right)\\left(s_2-s_1\\right) \\\\\n& +H_X\\left(s_1, x\\right)\\left(-\\frac{2}{\\Delta} \\ln \\left(\\frac{s_2}{s_1}\\right)-x\\right) \\geq(\\sqrt{x}-K)^{+}.\n\\end{aligned}\n\\end{equation}\\]\nIn the above definition, the functions \\(H_S, H_X: I_1 \\times I_X \\rightarrow \\mathbb{R}\\) are assumed to be bounded continuous functions on \\(I_1 \\times I_X, \\lambda_1 \\in \\mathrm{~L}^1\\left(\\mathbb{P}^1\\right)\\) and \\(\\lambda_2 \\in \\mathrm{~L}^1\\left(\\mathbb{P}^2\\right)\\). The variable \\(x\\) should be interpreted as the \\(t_1\\)-value of a log-contract \\(-2 / \\Delta \\ln \\frac{s_2}{s_1}\\), i.e., the square of the VIX index VIX\\(_{t_1}^2\\). This semi-static superreplication consists in holding statically \\(t_1\\) and \\(t_2\\)-European payoffs with market prices \\(\\mathbb{E}^{\\mathbb{P}^1}\\left[\\lambda_1\\left(S_1\\right)\\right]\\) and \\(\\mathbb{E}^{\\mathbb{P}^2}\\left[\\lambda_2\\left(S_2\\right)\\right]\\), a VIX future with market price VIX and delta hedging at \\(t_1\\) (with zero-cost) on the spot and on a forward \\(\\log\\) contract with price \\(x\\). The value of this portfolio is greater than or equal to the payoff \\((\\sqrt{x}-K)^{+}\\). If somebody offers this VIX option at a price \\(p\\) above \\(\\mathrm{MK}_{\\mathrm{vix}}\\), the arbitrage can be locked in by selling this option and going long in the above super-replication: \\[\\begin{equation}\n\\begin{aligned}\n& \\lambda_1\\left(s_1\\right)+\\lambda_2\\left(s_2\\right)+\\lambda \\sqrt{x}+H_S\\left(s_1, x\\right)\\left(s_2-s_1\\right)+H_X\\left(s_1, x\\right)\\left(-\\frac{2}{\\Delta} \\ln \\left(\\frac{s_2}{s_1}\\right)-x\\right) \\\\\n&\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-(\\sqrt{x}-K)^{+}+\\left(p-\\mathrm{MK}_{\\mathrm{vix}}\\right) \\geq 0, \\quad \\forall\\left(s_1, s_2, x\\right) \\in I_1 \\times I_2 \\times I_X\n\\end{aligned}\n\\end{equation}\\]\nSimilarly to the above sections, the main duality theorem of this constrained martingale optimal transport problem can be constructed via Fenchel-Rockafeller duality:\nTheorem Assume that \\(\\mathbb{P}^1, \\mathbb{P}^2\\) are probability measures respectively on \\(I_1\\) and \\(I_2\\) such that \\(\\mathcal{M}\\left(\\mathbb{P}^1, \\mathbb{P}^2\\right.\\), VIX \\()\\) is non-empty. Then, \\[\\begin{equation}\n    \\mathrm{MK}_{\\mathrm{vix}}=\\max _{\\mathbb{P} \\in \\mathcal{M}\\left(\\mathbb{P}^1, \\mathbb{P}^2, \\mathrm{VIX}\\right)} \\mathbb{E}\\left[\\left(\\mathrm{VIX}_{t_1}-K\\right)^{+}\\right]\n\\end{equation}\\]\nAgain, the non-empty condition of the set of martingale measures depends on a convex-order-type condition on the known probability measures \\(\\mathbb{P}^1\\) and \\(\\mathbb{P}^2\\). Moreover, the dual side of the above equation, admits concrete optimizer under certain further convex requirement on the known probability measures \\(\\mathbb{P}^1\\) and \\(\\mathbb{P}^2\\). In that case, the price on the dual side can be converted to an expectation of the payoff of the VIX-option with respect to a bi-atomic measure on the VIX index on \\(t_1\\) of the form\n\\[\\begin{equation}\n    \\overline{\\mathbb{P}}(d x)=p \\delta_{x_0}(d x)+(1-p) \\delta_{x_1}(d x)\n\\end{equation}\\]\nfor some \\(x_1\\) and \\(x_2\\). See De Marco and Henry-Labordère (2015) and Henry-Labordère (2017) for more details."
  },
  {
    "objectID": "MOT.html#pricing-via-continuous-time-mot",
    "href": "MOT.html#pricing-via-continuous-time-mot",
    "title": "Applications of Optimal Transport and Martingale Optimal Transport in Mathematical Finance",
    "section": "Pricing via continuous-time MOT",
    "text": "Pricing via continuous-time MOT\nTo resolve the second issue we mentioned above, we give a brief but descriptive introduction to continuous-time martingale optimal transport and the dual pricing formulas.\nRobust superhedging in continuous-time finance is formulated on the canonical path space\n\\[\n\\Omega = \\{\\omega \\in C([0,T],\\mathbb{R}_+): \\omega(0)=0\\},\n\\]\nequipped with the canonical process \\(B_t(\\omega)=\\omega(t)\\) and the Wiener measure \\(\\mathbb{P}^0\\). The asset price process is defined by\n\\[\nS_t = S_0+B_t,\n\\]\nand by incorporating an adapted volatility process \\(\\sigma\\), we obtain a modified price process\n\\[\nS_t^\\sigma = S_0 + \\int_0^t \\sigma_r\\,dB_r.\n\\]\nThe induced probability measure \\(\\mathbb{P}^\\sigma = \\mathbb{P}^0 \\circ (S^\\sigma)^{-1}\\) makes \\(S\\) a local martingale, and the collection of all such measures is denoted by \\(\\mathcal{M}^c\\). Along with the appropriate set of admissible trading strategies, the robust superhedging price for an option with payoff \\(\\xi\\) is defined as the minimum initial capital needed to ensure that the hedged portfolio exceeds \\(\\xi\\) under every \\(\\mathbb{P} \\in \\mathcal{M}^c\\).\nAssuming a minimax argument holds, one can interchange the infimum over hedging strategies and the supremum over martingale measures with prescribed marginals. This leads to the dual formulation:\n\\[\n\\operatorname{MK}_n^c(\\mathbb{P}^1,\\ldots,\\mathbb{P}^n) = \\sup_{\\mathbb{P}\\in\\mathcal{M}^c(\\mathbb{P}^1,\\ldots,\\mathbb{P}^n)} \\mathbb{E}^\\mathbb{P}[\\xi],\n\\]\nwhere \\(\\mathcal{M}^c(\\mathbb{P}^1,\\ldots,\\mathbb{P}^n)\\) is the set of martingale measures whose marginals at the specified times match those observed in the market. This duality bridges the seller’s perspective—obtained via superhedging (primal side)—with the buyer’s perspective—based on risk-neutral valuation (dual side). In essence, it unifies the robust pricing framework by showing that the highest price acceptable by the buyer coincides with the lowest cost required by the seller to hedge against all market scenarios.\nExcept for deriving the duality, the main technical difficulty in the continuous-setting is the construction of a martingale measure on the paths space, or equivalently, the construction of a martingale process matching the known marginal. The most common solution is to employ solutions of Skorokhod embedding problem from classical stochastic analysis and then covert the dual side to a supremum over a set of stopping times of a Brownian motion and then take expectation over that Brownian motion. In other words, we embed every possible martingale price process into a Brownian motion by a stopping time. This provide numerical efficiency in obtaining the dual price. The discussion of these topics lead to the follow-up article of this one, where we set up a robust and dynamic connection to bridge martingale optimal transport (MOT) with continuous-time mathematical finance, providing a model-independent framework for robust hedging without assuming a specific stochastic model for the asset dynamics. See Continuous-Time MOT and Skorokhod Embedding for details."
  }
]